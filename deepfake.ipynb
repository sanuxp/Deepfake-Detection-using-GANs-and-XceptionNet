{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Frames from the training videos to train the model\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_folder, target_fps=1 ):\n",
    "    # Open the video file\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get the original frame rate of the video (fps)\n",
    "    original_fps = 30\n",
    "\n",
    "    # Calculate the interval based on the target fps and the original fps\n",
    "    frame_interval = int(original_fps // target_fps)\n",
    "\n",
    "    # Get the video name (without extension) to include it in the frame filename\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        # If we are at the right interval, save the frame\n",
    "        if count % frame_interval == 0:\n",
    "            # Include the video name in the frame filename\n",
    "            frame_path = os.path.join(output_folder, f\"{video_name}_frame_{count:04d}.jpg\")\n",
    "            cv2.imwrite(frame_path, image)\n",
    "        \n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "\n",
    "# # Process all videos in the real and fake folders\n",
    "real_video_folder = \"Celeb-DF-v2/Celeb-real\"\n",
    "fake_video_folder = \"Celeb-DF-v2/Celeb-synthesis\"\n",
    "real_frames_folder = \"Frames/real_frames\"\n",
    "fake_frames_folder = \"Frames/fake_frames\"\n",
    "# # #Replace these paths with respective paths of the directory\n",
    "\n",
    "if not os.path.isdir(real_frames_folder) and not os.path.isdir(fake_frames_folder):\n",
    "    os.makedirs(real_frames_folder, exist_ok=True)\n",
    "    os.makedirs(fake_frames_folder, exist_ok=True)\n",
    "\n",
    "    for video_name in os.listdir(real_video_folder):\n",
    "        video_path = os.path.join(real_video_folder, video_name)\n",
    "        extract_frames(video_path, real_frames_folder)\n",
    "\n",
    "    for video_name in os.listdir(fake_video_folder):\n",
    "        video_path = os.path.join(fake_video_folder, video_name)\n",
    "        extract_frames(video_path, fake_frames_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class DeepFakeFrameDataset(Dataset):\n",
    "    def __init__(self, real_folder, fake_folder, transform=None):\n",
    "        self.real_frames = [os.path.join(real_folder, f) for f in os.listdir(real_folder)]\n",
    "        self.fake_frames = [os.path.join(fake_folder, f) for f in os.listdir(fake_folder)]\n",
    "        self.all_frames = self.real_frames + self.fake_frames\n",
    "        self.labels = [0] * len(self.real_frames) + [1] * len(self.fake_frames)  # 0 = real, 1 = fake\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.all_frames[idx]).convert(\"RGB\")  # Ensure 3 channels\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1] range\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = DeepFakeFrameDataset(real_frames_folder, fake_frames_folder, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Load XceptionNet as the backbone\n",
    "        self.backbone = timm.create_model('legacy_xception', pretrained=True, features_only=True)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layer for binary classification\n",
    "        self.fc = nn.Linear(self.backbone.feature_info.channels()[-1], num_classes)\n",
    "        \n",
    "        # ReLU activation for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features from XceptionNet\n",
    "        features = self.backbone(x)[-1]  # Use the last feature map\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled = self.global_pool(features)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.fc(pooled)\n",
    "        \n",
    "        # Sigmoid activation\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.load_state_dict(torch.load(\"discriminator_epoch_7.pth\", weights_only=False))\n",
    "torch.save(discriminator.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: latent_dim x 1 x 1\n",
    "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 256 x 4 x 4\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 128 x 8 x 8\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 64 x 16 x 16\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output: 3 x 32 x 32 (normalized to [-1, 1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device to GPU or CPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small Learning Rate helps model converge quickly leading to less training time\n",
    "#If the model overfits, learning rate will be decreased\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0005, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanu8\\AppData\\Local\\Temp\\ipykernel_15096\\2137540455.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  discriminator.load_state_dict(torch.load(\"discriminator_epoch_5.pth\"))\n",
      "C:\\Users\\sanu8\\AppData\\Local\\Temp\\ipykernel_15096\\2137540455.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(\"generator_epoch_5.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.load_state_dict(torch.load(\"discriminator_epoch_7.pth\", weights_only = False))\n",
    "generator.load_state_dict(torch.load(\"generator_epoch_7.pth\", weights_only = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Step [0/2541], D Loss: 3.5307, G Loss: 15.0126\n",
      "Epoch [5/20], Step [1/2541], D Loss: 3.0587, G Loss: 8.6894\n",
      "Epoch [5/20], Step [2/2541], D Loss: 2.4064, G Loss: 5.2654\n",
      "Epoch [5/20], Step [3/2541], D Loss: 1.9223, G Loss: 3.7178\n",
      "Epoch [5/20], Step [4/2541], D Loss: 1.7402, G Loss: 2.9206\n",
      "Epoch [5/20], Step [5/2541], D Loss: 1.5128, G Loss: 2.7533\n",
      "Epoch [5/20], Step [6/2541], D Loss: 1.3541, G Loss: 2.6963\n",
      "Epoch [5/20], Step [7/2541], D Loss: 1.1161, G Loss: 2.2041\n",
      "Epoch [5/20], Step [8/2541], D Loss: 0.8918, G Loss: 2.0681\n",
      "Epoch [5/20], Step [9/2541], D Loss: 0.8279, G Loss: 2.1277\n",
      "Epoch [5/20], Step [10/2541], D Loss: 0.7058, G Loss: 2.3123\n",
      "Epoch [5/20], Step [11/2541], D Loss: 0.6766, G Loss: 2.1689\n",
      "Epoch [5/20], Step [12/2541], D Loss: 0.6889, G Loss: 2.2492\n",
      "Epoch [5/20], Step [13/2541], D Loss: 0.6629, G Loss: 1.7962\n",
      "Epoch [5/20], Step [14/2541], D Loss: 0.6657, G Loss: 1.9657\n",
      "Epoch [5/20], Step [15/2541], D Loss: 0.6718, G Loss: 1.9902\n",
      "Epoch [5/20], Step [16/2541], D Loss: 0.6851, G Loss: 2.3014\n",
      "Epoch [5/20], Step [17/2541], D Loss: 0.6714, G Loss: 2.7048\n",
      "Epoch [5/20], Step [18/2541], D Loss: 0.6715, G Loss: 2.3158\n",
      "Epoch [5/20], Step [19/2541], D Loss: 0.6644, G Loss: 2.0102\n",
      "Epoch [5/20], Step [20/2541], D Loss: 0.6668, G Loss: 2.0674\n",
      "Epoch [5/20], Step [21/2541], D Loss: 0.6673, G Loss: 1.9275\n",
      "Epoch [5/20], Step [22/2541], D Loss: 0.6538, G Loss: 2.0303\n",
      "Epoch [5/20], Step [23/2541], D Loss: 0.6545, G Loss: 2.0825\n",
      "Epoch [5/20], Step [24/2541], D Loss: 0.6511, G Loss: 2.4473\n",
      "Epoch [5/20], Step [25/2541], D Loss: 0.6519, G Loss: 2.2757\n",
      "Epoch [5/20], Step [26/2541], D Loss: 0.6522, G Loss: 1.9846\n",
      "Epoch [5/20], Step [27/2541], D Loss: 0.6560, G Loss: 1.9052\n",
      "Epoch [5/20], Step [28/2541], D Loss: 0.6551, G Loss: 2.0343\n",
      "Epoch [5/20], Step [29/2541], D Loss: 0.6515, G Loss: 1.9651\n",
      "Epoch [5/20], Step [30/2541], D Loss: 0.6521, G Loss: 1.9932\n",
      "Epoch [5/20], Step [31/2541], D Loss: 0.6516, G Loss: 2.1773\n",
      "Epoch [5/20], Step [32/2541], D Loss: 0.6506, G Loss: 2.1460\n",
      "Epoch [5/20], Step [33/2541], D Loss: 0.6514, G Loss: 2.0050\n",
      "Epoch [5/20], Step [34/2541], D Loss: 0.6515, G Loss: 2.0875\n",
      "Epoch [5/20], Step [35/2541], D Loss: 0.6539, G Loss: 2.2461\n",
      "Epoch [5/20], Step [36/2541], D Loss: 0.6553, G Loss: 1.9585\n",
      "Epoch [5/20], Step [37/2541], D Loss: 0.6718, G Loss: 1.8616\n",
      "Epoch [5/20], Step [38/2541], D Loss: 0.6509, G Loss: 1.7503\n",
      "Epoch [5/20], Step [39/2541], D Loss: 0.6762, G Loss: 2.0339\n",
      "Epoch [5/20], Step [40/2541], D Loss: 0.6638, G Loss: 2.9830\n",
      "Epoch [5/20], Step [41/2541], D Loss: 0.6638, G Loss: 2.1462\n",
      "Epoch [5/20], Step [42/2541], D Loss: 0.6697, G Loss: 2.5335\n",
      "Epoch [5/20], Step [43/2541], D Loss: 0.6642, G Loss: 2.0859\n",
      "Epoch [5/20], Step [44/2541], D Loss: 0.6548, G Loss: 2.0735\n",
      "Epoch [5/20], Step [45/2541], D Loss: 0.6582, G Loss: 2.1392\n",
      "Epoch [5/20], Step [46/2541], D Loss: 0.6531, G Loss: 1.9988\n",
      "Epoch [5/20], Step [47/2541], D Loss: 0.6519, G Loss: 2.2218\n",
      "Epoch [5/20], Step [48/2541], D Loss: 0.6509, G Loss: 2.0532\n",
      "Epoch [5/20], Step [49/2541], D Loss: 0.6766, G Loss: 2.1133\n",
      "Epoch [5/20], Step [50/2541], D Loss: 0.6543, G Loss: 2.2023\n",
      "Epoch [5/20], Step [51/2541], D Loss: 0.6570, G Loss: 2.1489\n",
      "Epoch [5/20], Step [52/2541], D Loss: 0.6658, G Loss: 2.2013\n",
      "Epoch [5/20], Step [53/2541], D Loss: 0.6613, G Loss: 2.1893\n",
      "Epoch [5/20], Step [54/2541], D Loss: 0.6514, G Loss: 2.0909\n",
      "Epoch [5/20], Step [55/2541], D Loss: 0.6525, G Loss: 2.1861\n",
      "Epoch [5/20], Step [56/2541], D Loss: 0.6516, G Loss: 2.1681\n",
      "Epoch [5/20], Step [57/2541], D Loss: 0.6732, G Loss: 2.1447\n",
      "Epoch [5/20], Step [58/2541], D Loss: 0.6549, G Loss: 2.0943\n",
      "Epoch [5/20], Step [59/2541], D Loss: 0.6530, G Loss: 2.0608\n",
      "Epoch [5/20], Step [60/2541], D Loss: 0.6513, G Loss: 2.0586\n",
      "Epoch [5/20], Step [61/2541], D Loss: 0.6511, G Loss: 2.1673\n",
      "Epoch [5/20], Step [62/2541], D Loss: 0.6560, G Loss: 2.1959\n",
      "Epoch [5/20], Step [63/2541], D Loss: 0.6540, G Loss: 2.2157\n",
      "Epoch [5/20], Step [64/2541], D Loss: 0.6532, G Loss: 2.1355\n",
      "Epoch [5/20], Step [65/2541], D Loss: 0.6557, G Loss: 2.1201\n",
      "Epoch [5/20], Step [66/2541], D Loss: 0.6522, G Loss: 2.2757\n",
      "Epoch [5/20], Step [67/2541], D Loss: 0.6528, G Loss: 2.0801\n",
      "Epoch [5/20], Step [68/2541], D Loss: 0.6518, G Loss: 2.2872\n",
      "Epoch [5/20], Step [69/2541], D Loss: 0.6541, G Loss: 2.1524\n",
      "Epoch [5/20], Step [70/2541], D Loss: 0.6534, G Loss: 2.1794\n",
      "Epoch [5/20], Step [71/2541], D Loss: 0.6520, G Loss: 2.1452\n",
      "Epoch [5/20], Step [72/2541], D Loss: 0.6512, G Loss: 2.0350\n",
      "Epoch [5/20], Step [73/2541], D Loss: 0.6522, G Loss: 2.0089\n",
      "Epoch [5/20], Step [74/2541], D Loss: 0.6504, G Loss: 2.0501\n",
      "Epoch [5/20], Step [75/2541], D Loss: 0.6516, G Loss: 2.0505\n",
      "Epoch [5/20], Step [76/2541], D Loss: 0.6508, G Loss: 2.0620\n",
      "Epoch [5/20], Step [77/2541], D Loss: 0.6523, G Loss: 2.0577\n",
      "Epoch [5/20], Step [78/2541], D Loss: 0.6527, G Loss: 2.0936\n",
      "Epoch [5/20], Step [79/2541], D Loss: 0.6505, G Loss: 2.0617\n",
      "Epoch [5/20], Step [80/2541], D Loss: 0.6524, G Loss: 2.1922\n",
      "Epoch [5/20], Step [81/2541], D Loss: 0.6512, G Loss: 2.2197\n",
      "Epoch [5/20], Step [82/2541], D Loss: 0.6515, G Loss: 2.1615\n",
      "Epoch [5/20], Step [83/2541], D Loss: 0.6516, G Loss: 2.0563\n",
      "Epoch [5/20], Step [84/2541], D Loss: 0.6505, G Loss: 1.9940\n",
      "Epoch [5/20], Step [85/2541], D Loss: 0.6519, G Loss: 2.0504\n",
      "Epoch [5/20], Step [86/2541], D Loss: 0.6509, G Loss: 2.1215\n",
      "Epoch [5/20], Step [87/2541], D Loss: 0.6505, G Loss: 2.1109\n",
      "Epoch [5/20], Step [88/2541], D Loss: 0.6651, G Loss: 2.1201\n",
      "Epoch [5/20], Step [89/2541], D Loss: 0.6535, G Loss: 2.0540\n",
      "Epoch [5/20], Step [90/2541], D Loss: 0.6669, G Loss: 2.2849\n",
      "Epoch [5/20], Step [91/2541], D Loss: 0.6591, G Loss: 2.2481\n",
      "Epoch [5/20], Step [92/2541], D Loss: 0.6640, G Loss: 2.3574\n",
      "Epoch [5/20], Step [93/2541], D Loss: 0.6644, G Loss: 2.2832\n",
      "Epoch [5/20], Step [94/2541], D Loss: 0.6601, G Loss: 2.1205\n",
      "Epoch [5/20], Step [95/2541], D Loss: 0.6559, G Loss: 2.0972\n",
      "Epoch [5/20], Step [96/2541], D Loss: 0.6528, G Loss: 2.0226\n",
      "Epoch [5/20], Step [97/2541], D Loss: 0.6605, G Loss: 1.9876\n",
      "Epoch [5/20], Step [98/2541], D Loss: 0.6581, G Loss: 2.0566\n",
      "Epoch [5/20], Step [99/2541], D Loss: 0.6542, G Loss: 2.0889\n",
      "Epoch [5/20], Step [100/2541], D Loss: 0.6533, G Loss: 2.1481\n",
      "Epoch [5/20], Step [101/2541], D Loss: 0.6516, G Loss: 2.1196\n",
      "Epoch [5/20], Step [102/2541], D Loss: 0.6514, G Loss: 2.0595\n",
      "Epoch [5/20], Step [103/2541], D Loss: 0.6590, G Loss: 2.1259\n",
      "Epoch [5/20], Step [104/2541], D Loss: 0.6525, G Loss: 2.1477\n",
      "Epoch [5/20], Step [105/2541], D Loss: 0.6549, G Loss: 2.1277\n",
      "Epoch [5/20], Step [106/2541], D Loss: 0.6513, G Loss: 1.9981\n",
      "Epoch [5/20], Step [107/2541], D Loss: 0.6510, G Loss: 2.1036\n",
      "Epoch [5/20], Step [108/2541], D Loss: 0.6509, G Loss: 2.0519\n",
      "Epoch [5/20], Step [109/2541], D Loss: 0.6547, G Loss: 1.9523\n",
      "Epoch [5/20], Step [110/2541], D Loss: 0.6527, G Loss: 2.0524\n",
      "Epoch [5/20], Step [111/2541], D Loss: 0.6510, G Loss: 2.1025\n",
      "Epoch [5/20], Step [112/2541], D Loss: 0.6516, G Loss: 2.0861\n",
      "Epoch [5/20], Step [113/2541], D Loss: 0.6514, G Loss: 2.0720\n",
      "Epoch [5/20], Step [114/2541], D Loss: 0.6507, G Loss: 2.1709\n",
      "Epoch [5/20], Step [115/2541], D Loss: 0.6515, G Loss: 2.0582\n",
      "Epoch [5/20], Step [116/2541], D Loss: 0.6512, G Loss: 2.0284\n",
      "Epoch [5/20], Step [117/2541], D Loss: 0.6504, G Loss: 2.0331\n",
      "Epoch [5/20], Step [118/2541], D Loss: 0.6505, G Loss: 2.0517\n",
      "Epoch [5/20], Step [119/2541], D Loss: 0.6512, G Loss: 2.0836\n",
      "Epoch [5/20], Step [120/2541], D Loss: 0.6532, G Loss: 2.0830\n",
      "Epoch [5/20], Step [121/2541], D Loss: 0.6507, G Loss: 2.0776\n",
      "Epoch [5/20], Step [122/2541], D Loss: 0.6512, G Loss: 2.0660\n",
      "Epoch [5/20], Step [123/2541], D Loss: 0.6505, G Loss: 2.1986\n",
      "Epoch [5/20], Step [124/2541], D Loss: 0.6509, G Loss: 2.0514\n",
      "Epoch [5/20], Step [125/2541], D Loss: 0.6503, G Loss: 2.0736\n",
      "Epoch [5/20], Step [126/2541], D Loss: 0.6512, G Loss: 2.0690\n",
      "Epoch [5/20], Step [127/2541], D Loss: 0.6503, G Loss: 2.1103\n",
      "Epoch [5/20], Step [128/2541], D Loss: 0.6507, G Loss: 2.2494\n",
      "Epoch [5/20], Step [129/2541], D Loss: 0.6512, G Loss: 2.1394\n",
      "Epoch [5/20], Step [130/2541], D Loss: 0.6505, G Loss: 2.0144\n",
      "Epoch [5/20], Step [131/2541], D Loss: 0.6512, G Loss: 2.1259\n",
      "Epoch [5/20], Step [132/2541], D Loss: 0.6507, G Loss: 2.1379\n",
      "Epoch [5/20], Step [133/2541], D Loss: 0.6506, G Loss: 2.1059\n",
      "Epoch [5/20], Step [134/2541], D Loss: 0.6503, G Loss: 2.0796\n",
      "Epoch [5/20], Step [135/2541], D Loss: 0.6503, G Loss: 2.0577\n",
      "Epoch [5/20], Step [136/2541], D Loss: 0.6504, G Loss: 2.1306\n",
      "Epoch [5/20], Step [137/2541], D Loss: 0.6513, G Loss: 2.1652\n",
      "Epoch [5/20], Step [138/2541], D Loss: 0.6518, G Loss: 2.2574\n",
      "Epoch [5/20], Step [139/2541], D Loss: 0.6518, G Loss: 2.1368\n",
      "Epoch [5/20], Step [140/2541], D Loss: 0.6510, G Loss: 2.1299\n",
      "Epoch [5/20], Step [141/2541], D Loss: 0.6515, G Loss: 2.0782\n",
      "Epoch [5/20], Step [142/2541], D Loss: 0.6512, G Loss: 2.0500\n",
      "Epoch [5/20], Step [143/2541], D Loss: 0.6518, G Loss: 2.1243\n",
      "Epoch [5/20], Step [144/2541], D Loss: 0.6510, G Loss: 2.0464\n",
      "Epoch [5/20], Step [145/2541], D Loss: 0.6504, G Loss: 2.0583\n",
      "Epoch [5/20], Step [146/2541], D Loss: 0.6507, G Loss: 2.1082\n",
      "Epoch [5/20], Step [147/2541], D Loss: 0.6504, G Loss: 2.1028\n",
      "Epoch [5/20], Step [148/2541], D Loss: 0.6506, G Loss: 2.1182\n",
      "Epoch [5/20], Step [149/2541], D Loss: 0.6508, G Loss: 2.1335\n",
      "Epoch [5/20], Step [150/2541], D Loss: 0.6508, G Loss: 2.1259\n",
      "Epoch [5/20], Step [151/2541], D Loss: 0.6504, G Loss: 2.0487\n",
      "Epoch [5/20], Step [152/2541], D Loss: 0.6524, G Loss: 2.1005\n",
      "Epoch [5/20], Step [153/2541], D Loss: 0.6513, G Loss: 2.1230\n",
      "Epoch [5/20], Step [154/2541], D Loss: 0.6516, G Loss: 2.1178\n",
      "Epoch [5/20], Step [155/2541], D Loss: 0.6507, G Loss: 2.1089\n",
      "Epoch [5/20], Step [156/2541], D Loss: 0.6507, G Loss: 2.0888\n",
      "Epoch [5/20], Step [157/2541], D Loss: 0.6515, G Loss: 1.9813\n",
      "Epoch [5/20], Step [158/2541], D Loss: 0.6506, G Loss: 1.9239\n",
      "Epoch [5/20], Step [159/2541], D Loss: 0.6510, G Loss: 2.0215\n",
      "Epoch [5/20], Step [160/2541], D Loss: 0.6512, G Loss: 2.1691\n",
      "Epoch [5/20], Step [161/2541], D Loss: 0.6507, G Loss: 2.1310\n",
      "Epoch [5/20], Step [162/2541], D Loss: 0.6505, G Loss: 2.1093\n",
      "Epoch [5/20], Step [163/2541], D Loss: 0.6505, G Loss: 2.1171\n",
      "Epoch [5/20], Step [164/2541], D Loss: 0.6508, G Loss: 2.1395\n",
      "Epoch [5/20], Step [165/2541], D Loss: 0.6510, G Loss: 2.0559\n",
      "Epoch [5/20], Step [166/2541], D Loss: 0.6505, G Loss: 2.0123\n",
      "Epoch [5/20], Step [167/2541], D Loss: 0.6505, G Loss: 2.1178\n",
      "Epoch [5/20], Step [168/2541], D Loss: 0.6508, G Loss: 2.1369\n",
      "Epoch [5/20], Step [169/2541], D Loss: 0.6506, G Loss: 2.1075\n",
      "Epoch [5/20], Step [170/2541], D Loss: 0.6504, G Loss: 2.0631\n",
      "Epoch [5/20], Step [171/2541], D Loss: 0.6517, G Loss: 2.0205\n",
      "Epoch [5/20], Step [172/2541], D Loss: 0.6510, G Loss: 2.0012\n",
      "Epoch [5/20], Step [173/2541], D Loss: 0.6506, G Loss: 2.1307\n",
      "Epoch [5/20], Step [174/2541], D Loss: 0.6511, G Loss: 2.0506\n",
      "Epoch [5/20], Step [175/2541], D Loss: 0.6504, G Loss: 2.1168\n",
      "Epoch [5/20], Step [176/2541], D Loss: 0.6505, G Loss: 2.0814\n",
      "Epoch [5/20], Step [177/2541], D Loss: 0.6513, G Loss: 2.0795\n",
      "Epoch [5/20], Step [178/2541], D Loss: 0.6505, G Loss: 2.2359\n",
      "Epoch [5/20], Step [179/2541], D Loss: 0.6510, G Loss: 2.0959\n",
      "Epoch [5/20], Step [180/2541], D Loss: 0.6505, G Loss: 2.0763\n",
      "Epoch [5/20], Step [181/2541], D Loss: 0.6507, G Loss: 2.0981\n",
      "Epoch [5/20], Step [182/2541], D Loss: 0.6505, G Loss: 2.1448\n",
      "Epoch [5/20], Step [183/2541], D Loss: 0.6511, G Loss: 2.0438\n",
      "Epoch [5/20], Step [184/2541], D Loss: 0.6554, G Loss: 2.1274\n",
      "Epoch [5/20], Step [185/2541], D Loss: 0.6531, G Loss: 2.1829\n",
      "Epoch [5/20], Step [186/2541], D Loss: 0.6506, G Loss: 2.1407\n",
      "Epoch [5/20], Step [187/2541], D Loss: 0.6516, G Loss: 2.1076\n",
      "Epoch [5/20], Step [188/2541], D Loss: 0.6506, G Loss: 2.0522\n",
      "Epoch [5/20], Step [189/2541], D Loss: 0.6504, G Loss: 2.0970\n",
      "Epoch [5/20], Step [190/2541], D Loss: 0.6507, G Loss: 2.1089\n",
      "Epoch [5/20], Step [191/2541], D Loss: 0.6504, G Loss: 2.1021\n",
      "Epoch [5/20], Step [192/2541], D Loss: 0.6514, G Loss: 2.1025\n",
      "Epoch [5/20], Step [193/2541], D Loss: 0.6508, G Loss: 2.1589\n",
      "Epoch [5/20], Step [194/2541], D Loss: 0.6506, G Loss: 2.0244\n",
      "Epoch [5/20], Step [195/2541], D Loss: 0.6507, G Loss: 2.0731\n",
      "Epoch [5/20], Step [196/2541], D Loss: 0.6505, G Loss: 2.1890\n",
      "Epoch [5/20], Step [197/2541], D Loss: 0.6505, G Loss: 2.0600\n",
      "Epoch [5/20], Step [198/2541], D Loss: 0.6504, G Loss: 2.0500\n",
      "Epoch [5/20], Step [199/2541], D Loss: 0.6505, G Loss: 2.0831\n",
      "Epoch [5/20], Step [200/2541], D Loss: 0.6508, G Loss: 2.0569\n",
      "Epoch [5/20], Step [201/2541], D Loss: 0.6526, G Loss: 2.1747\n",
      "Epoch [5/20], Step [202/2541], D Loss: 0.6517, G Loss: 2.1799\n",
      "Epoch [5/20], Step [203/2541], D Loss: 0.6510, G Loss: 2.1038\n",
      "Epoch [5/20], Step [204/2541], D Loss: 0.6505, G Loss: 2.0660\n",
      "Epoch [5/20], Step [205/2541], D Loss: 0.6504, G Loss: 2.0042\n",
      "Epoch [5/20], Step [206/2541], D Loss: 0.6506, G Loss: 2.0826\n",
      "Epoch [5/20], Step [207/2541], D Loss: 0.6503, G Loss: 2.1532\n",
      "Epoch [5/20], Step [208/2541], D Loss: 0.6518, G Loss: 2.1676\n",
      "Epoch [5/20], Step [209/2541], D Loss: 0.6521, G Loss: 2.0898\n",
      "Epoch [5/20], Step [210/2541], D Loss: 0.6506, G Loss: 2.0887\n",
      "Epoch [5/20], Step [211/2541], D Loss: 0.6519, G Loss: 2.0855\n",
      "Epoch [5/20], Step [212/2541], D Loss: 0.6508, G Loss: 2.1051\n",
      "Epoch [5/20], Step [213/2541], D Loss: 0.6506, G Loss: 2.0719\n",
      "Epoch [5/20], Step [214/2541], D Loss: 0.6503, G Loss: 2.1451\n",
      "Epoch [5/20], Step [215/2541], D Loss: 0.6504, G Loss: 2.0201\n",
      "Epoch [5/20], Step [216/2541], D Loss: 0.6510, G Loss: 2.1139\n",
      "Epoch [5/20], Step [217/2541], D Loss: 0.6504, G Loss: 2.0911\n",
      "Epoch [5/20], Step [218/2541], D Loss: 0.6506, G Loss: 1.9951\n",
      "Epoch [5/20], Step [219/2541], D Loss: 0.6512, G Loss: 2.0680\n",
      "Epoch [5/20], Step [220/2541], D Loss: 0.6516, G Loss: 2.1808\n",
      "Epoch [5/20], Step [221/2541], D Loss: 0.6517, G Loss: 2.0277\n",
      "Epoch [5/20], Step [222/2541], D Loss: 0.6511, G Loss: 2.0411\n",
      "Epoch [5/20], Step [223/2541], D Loss: 0.6507, G Loss: 2.0639\n",
      "Epoch [5/20], Step [224/2541], D Loss: 0.6504, G Loss: 2.0494\n",
      "Epoch [5/20], Step [225/2541], D Loss: 0.6503, G Loss: 2.0668\n",
      "Epoch [5/20], Step [226/2541], D Loss: 0.6520, G Loss: 2.1195\n",
      "Epoch [5/20], Step [227/2541], D Loss: 0.6518, G Loss: 2.2089\n",
      "Epoch [5/20], Step [228/2541], D Loss: 0.6521, G Loss: 2.1677\n",
      "Epoch [5/20], Step [229/2541], D Loss: 0.6858, G Loss: 1.6599\n",
      "Epoch [5/20], Step [230/2541], D Loss: 0.6714, G Loss: 1.7877\n",
      "Epoch [5/20], Step [231/2541], D Loss: 0.6616, G Loss: 2.0365\n",
      "Epoch [5/20], Step [232/2541], D Loss: 0.6529, G Loss: 2.5750\n",
      "Epoch [5/20], Step [233/2541], D Loss: 0.6624, G Loss: 2.0293\n",
      "Epoch [5/20], Step [234/2541], D Loss: 0.6648, G Loss: 2.2326\n",
      "Epoch [5/20], Step [235/2541], D Loss: 0.6628, G Loss: 2.1799\n",
      "Epoch [5/20], Step [236/2541], D Loss: 0.6612, G Loss: 2.2577\n",
      "Epoch [5/20], Step [237/2541], D Loss: 0.6614, G Loss: 2.2292\n",
      "Epoch [5/20], Step [238/2541], D Loss: 0.6555, G Loss: 2.1952\n",
      "Epoch [5/20], Step [239/2541], D Loss: 0.6527, G Loss: 1.9822\n",
      "Epoch [5/20], Step [240/2541], D Loss: 0.6555, G Loss: 2.0583\n",
      "Epoch [5/20], Step [241/2541], D Loss: 0.6527, G Loss: 2.1086\n",
      "Epoch [5/20], Step [242/2541], D Loss: 0.6524, G Loss: 2.0159\n",
      "Epoch [5/20], Step [243/2541], D Loss: 0.6532, G Loss: 2.0457\n",
      "Epoch [5/20], Step [244/2541], D Loss: 0.6513, G Loss: 2.0387\n",
      "Epoch [5/20], Step [245/2541], D Loss: 0.6521, G Loss: 2.0440\n",
      "Epoch [5/20], Step [246/2541], D Loss: 0.6533, G Loss: 2.1681\n",
      "Epoch [5/20], Step [247/2541], D Loss: 0.6539, G Loss: 2.0959\n",
      "Epoch [5/20], Step [248/2541], D Loss: 0.6520, G Loss: 2.2063\n",
      "Epoch [5/20], Step [249/2541], D Loss: 0.6519, G Loss: 2.1371\n",
      "Epoch [5/20], Step [250/2541], D Loss: 0.6514, G Loss: 2.1252\n",
      "Epoch [5/20], Step [251/2541], D Loss: 0.6513, G Loss: 1.9784\n",
      "Epoch [5/20], Step [252/2541], D Loss: 0.6517, G Loss: 2.0866\n",
      "Epoch [5/20], Step [253/2541], D Loss: 0.6517, G Loss: 2.0634\n",
      "Epoch [5/20], Step [254/2541], D Loss: 0.6506, G Loss: 2.0876\n",
      "Epoch [5/20], Step [255/2541], D Loss: 0.6519, G Loss: 2.0111\n",
      "Epoch [5/20], Step [256/2541], D Loss: 0.6506, G Loss: 2.1051\n",
      "Epoch [5/20], Step [257/2541], D Loss: 0.6519, G Loss: 2.0635\n",
      "Epoch [5/20], Step [258/2541], D Loss: 0.6511, G Loss: 2.0913\n",
      "Epoch [5/20], Step [259/2541], D Loss: 0.6524, G Loss: 1.9940\n",
      "Epoch [5/20], Step [260/2541], D Loss: 0.6514, G Loss: 2.0054\n",
      "Epoch [5/20], Step [261/2541], D Loss: 0.6515, G Loss: 2.0166\n",
      "Epoch [5/20], Step [262/2541], D Loss: 0.6512, G Loss: 2.0698\n",
      "Epoch [5/20], Step [263/2541], D Loss: 0.6506, G Loss: 2.1374\n",
      "Epoch [5/20], Step [264/2541], D Loss: 0.6513, G Loss: 1.9994\n",
      "Epoch [5/20], Step [265/2541], D Loss: 0.6504, G Loss: 2.0839\n",
      "Epoch [5/20], Step [266/2541], D Loss: 0.6513, G Loss: 2.0263\n",
      "Epoch [5/20], Step [267/2541], D Loss: 0.6687, G Loss: 2.1298\n",
      "Epoch [5/20], Step [268/2541], D Loss: 0.6582, G Loss: 2.0618\n",
      "Epoch [5/20], Step [269/2541], D Loss: 0.6532, G Loss: 2.0852\n",
      "Epoch [5/20], Step [270/2541], D Loss: 0.6535, G Loss: 2.1634\n",
      "Epoch [5/20], Step [271/2541], D Loss: 0.6522, G Loss: 2.1346\n",
      "Epoch [5/20], Step [272/2541], D Loss: 0.6521, G Loss: 2.1639\n",
      "Epoch [5/20], Step [273/2541], D Loss: 0.6518, G Loss: 1.9980\n",
      "Epoch [5/20], Step [274/2541], D Loss: 0.6510, G Loss: 1.8960\n",
      "Epoch [5/20], Step [275/2541], D Loss: 0.6509, G Loss: 2.0374\n",
      "Epoch [5/20], Step [276/2541], D Loss: 0.6505, G Loss: 2.1716\n",
      "Epoch [5/20], Step [277/2541], D Loss: 0.6509, G Loss: 2.1280\n",
      "Epoch [5/20], Step [278/2541], D Loss: 0.6504, G Loss: 2.1204\n",
      "Epoch [5/20], Step [279/2541], D Loss: 0.6505, G Loss: 2.0427\n",
      "Epoch [5/20], Step [280/2541], D Loss: 0.6509, G Loss: 2.0284\n",
      "Epoch [5/20], Step [281/2541], D Loss: 0.6512, G Loss: 2.0936\n",
      "Epoch [5/20], Step [282/2541], D Loss: 0.6504, G Loss: 2.0919\n",
      "Epoch [5/20], Step [283/2541], D Loss: 0.6505, G Loss: 2.1119\n",
      "Epoch [5/20], Step [284/2541], D Loss: 0.6516, G Loss: 2.0270\n",
      "Epoch [5/20], Step [285/2541], D Loss: 0.6511, G Loss: 2.0382\n",
      "Epoch [5/20], Step [286/2541], D Loss: 0.6518, G Loss: 2.0373\n",
      "Epoch [5/20], Step [287/2541], D Loss: 0.6509, G Loss: 2.1166\n",
      "Epoch [5/20], Step [288/2541], D Loss: 0.6514, G Loss: 2.1116\n",
      "Epoch [5/20], Step [289/2541], D Loss: 0.6510, G Loss: 1.9831\n",
      "Epoch [5/20], Step [290/2541], D Loss: 0.6504, G Loss: 2.0638\n",
      "Epoch [5/20], Step [291/2541], D Loss: 0.6506, G Loss: 2.1197\n",
      "Epoch [5/20], Step [292/2541], D Loss: 0.6514, G Loss: 2.0864\n",
      "Epoch [5/20], Step [293/2541], D Loss: 0.6509, G Loss: 2.1082\n",
      "Epoch [5/20], Step [294/2541], D Loss: 0.6506, G Loss: 2.0876\n",
      "Epoch [5/20], Step [295/2541], D Loss: 0.6509, G Loss: 2.0626\n",
      "Epoch [5/20], Step [296/2541], D Loss: 0.6526, G Loss: 2.1599\n",
      "Epoch [5/20], Step [297/2541], D Loss: 0.6516, G Loss: 2.1781\n",
      "Epoch [5/20], Step [298/2541], D Loss: 0.6511, G Loss: 2.0710\n",
      "Epoch [5/20], Step [299/2541], D Loss: 0.6507, G Loss: 2.0125\n",
      "Epoch [5/20], Step [300/2541], D Loss: 0.6509, G Loss: 2.0781\n",
      "Epoch [5/20], Step [301/2541], D Loss: 0.6519, G Loss: 2.0915\n",
      "Epoch [5/20], Step [302/2541], D Loss: 0.6512, G Loss: 2.1551\n",
      "Epoch [5/20], Step [303/2541], D Loss: 0.6506, G Loss: 2.0654\n",
      "Epoch [5/20], Step [304/2541], D Loss: 0.6507, G Loss: 2.0249\n",
      "Epoch [5/20], Step [305/2541], D Loss: 0.6520, G Loss: 2.0646\n",
      "Epoch [5/20], Step [306/2541], D Loss: 0.6514, G Loss: 2.0924\n",
      "Epoch [5/20], Step [307/2541], D Loss: 0.6506, G Loss: 1.9806\n",
      "Epoch [5/20], Step [308/2541], D Loss: 0.6526, G Loss: 2.0217\n",
      "Epoch [5/20], Step [309/2541], D Loss: 0.6530, G Loss: 2.1075\n",
      "Epoch [5/20], Step [310/2541], D Loss: 0.6512, G Loss: 2.0924\n",
      "Epoch [5/20], Step [311/2541], D Loss: 0.6511, G Loss: 2.0192\n",
      "Epoch [5/20], Step [312/2541], D Loss: 0.6508, G Loss: 2.0984\n",
      "Epoch [5/20], Step [313/2541], D Loss: 0.6513, G Loss: 2.1225\n",
      "Epoch [5/20], Step [314/2541], D Loss: 0.6508, G Loss: 2.1244\n",
      "Epoch [5/20], Step [315/2541], D Loss: 0.6519, G Loss: 2.1616\n",
      "Epoch [5/20], Step [316/2541], D Loss: 0.6508, G Loss: 2.0658\n",
      "Epoch [5/20], Step [317/2541], D Loss: 0.6507, G Loss: 2.1466\n",
      "Epoch [5/20], Step [318/2541], D Loss: 0.6513, G Loss: 2.0485\n",
      "Epoch [5/20], Step [319/2541], D Loss: 0.6507, G Loss: 2.0496\n",
      "Epoch [5/20], Step [320/2541], D Loss: 0.6508, G Loss: 2.1154\n",
      "Epoch [5/20], Step [321/2541], D Loss: 0.6514, G Loss: 2.1275\n",
      "Epoch [5/20], Step [322/2541], D Loss: 0.6511, G Loss: 2.1331\n",
      "Epoch [5/20], Step [323/2541], D Loss: 0.6508, G Loss: 2.0960\n",
      "Epoch [5/20], Step [324/2541], D Loss: 0.6507, G Loss: 1.9911\n",
      "Epoch [5/20], Step [325/2541], D Loss: 0.6505, G Loss: 2.0457\n",
      "Epoch [5/20], Step [326/2541], D Loss: 0.6504, G Loss: 2.0442\n",
      "Epoch [5/20], Step [327/2541], D Loss: 0.6509, G Loss: 2.0661\n",
      "Epoch [5/20], Step [328/2541], D Loss: 0.6504, G Loss: 2.1043\n",
      "Epoch [5/20], Step [329/2541], D Loss: 0.6723, G Loss: 2.0503\n",
      "Epoch [5/20], Step [330/2541], D Loss: 0.6539, G Loss: 2.0704\n",
      "Epoch [5/20], Step [331/2541], D Loss: 0.6612, G Loss: 2.1323\n",
      "Epoch [5/20], Step [332/2541], D Loss: 0.6505, G Loss: 2.1994\n",
      "Epoch [5/20], Step [333/2541], D Loss: 0.6524, G Loss: 2.1801\n",
      "Epoch [5/20], Step [334/2541], D Loss: 0.6521, G Loss: 2.0652\n",
      "Epoch [5/20], Step [335/2541], D Loss: 0.6508, G Loss: 1.9586\n",
      "Epoch [5/20], Step [336/2541], D Loss: 0.6627, G Loss: 1.9092\n",
      "Epoch [5/20], Step [337/2541], D Loss: 0.6566, G Loss: 1.9682\n",
      "Epoch [5/20], Step [338/2541], D Loss: 0.6529, G Loss: 2.2005\n",
      "Epoch [5/20], Step [339/2541], D Loss: 0.6563, G Loss: 2.2419\n",
      "Epoch [5/20], Step [340/2541], D Loss: 0.6520, G Loss: 2.0827\n",
      "Epoch [5/20], Step [341/2541], D Loss: 0.6518, G Loss: 2.1733\n",
      "Epoch [5/20], Step [342/2541], D Loss: 0.6522, G Loss: 2.1163\n",
      "Epoch [5/20], Step [343/2541], D Loss: 0.6513, G Loss: 1.9866\n",
      "Epoch [5/20], Step [344/2541], D Loss: 0.6508, G Loss: 2.0486\n",
      "Epoch [5/20], Step [345/2541], D Loss: 0.6519, G Loss: 2.2063\n",
      "Epoch [5/20], Step [346/2541], D Loss: 0.6504, G Loss: 3.5819\n",
      "Epoch [5/20], Step [347/2541], D Loss: 0.6511, G Loss: 2.0642\n",
      "Epoch [5/20], Step [348/2541], D Loss: 0.6507, G Loss: 2.0740\n",
      "Epoch [5/20], Step [349/2541], D Loss: 0.6523, G Loss: 1.9207\n",
      "Epoch [5/20], Step [350/2541], D Loss: 0.6515, G Loss: 2.0878\n",
      "Epoch [5/20], Step [351/2541], D Loss: 0.6577, G Loss: 1.9888\n",
      "Epoch [5/20], Step [352/2541], D Loss: 0.6515, G Loss: 2.0735\n",
      "Epoch [5/20], Step [353/2541], D Loss: 0.6532, G Loss: 2.1288\n",
      "Epoch [5/20], Step [354/2541], D Loss: 0.6516, G Loss: 2.0610\n",
      "Epoch [5/20], Step [355/2541], D Loss: 0.6507, G Loss: 2.1453\n",
      "Epoch [5/20], Step [356/2541], D Loss: 0.6508, G Loss: 2.1929\n",
      "Epoch [5/20], Step [357/2541], D Loss: 0.6508, G Loss: 2.0668\n",
      "Epoch [5/20], Step [358/2541], D Loss: 0.6519, G Loss: 2.1475\n",
      "Epoch [5/20], Step [359/2541], D Loss: 0.6509, G Loss: 2.1477\n",
      "Epoch [5/20], Step [360/2541], D Loss: 0.6505, G Loss: 2.1773\n",
      "Epoch [5/20], Step [361/2541], D Loss: 0.6515, G Loss: 2.0363\n",
      "Epoch [5/20], Step [362/2541], D Loss: 0.6507, G Loss: 2.0794\n",
      "Epoch [5/20], Step [363/2541], D Loss: 0.6779, G Loss: 1.6299\n",
      "Epoch [5/20], Step [364/2541], D Loss: 0.6762, G Loss: 1.9337\n",
      "Epoch [5/20], Step [365/2541], D Loss: 0.6609, G Loss: 2.1162\n",
      "Epoch [5/20], Step [366/2541], D Loss: 0.6563, G Loss: 2.3938\n",
      "Epoch [5/20], Step [367/2541], D Loss: 0.6533, G Loss: 2.0912\n",
      "Epoch [5/20], Step [368/2541], D Loss: 0.6535, G Loss: 2.1012\n",
      "Epoch [5/20], Step [369/2541], D Loss: 0.6526, G Loss: 2.3428\n",
      "Epoch [5/20], Step [370/2541], D Loss: 0.6646, G Loss: 2.0107\n",
      "Epoch [5/20], Step [371/2541], D Loss: 0.6569, G Loss: 2.0250\n",
      "Epoch [5/20], Step [372/2541], D Loss: 0.6740, G Loss: 2.0880\n",
      "Epoch [5/20], Step [373/2541], D Loss: 0.6683, G Loss: 2.1130\n",
      "Epoch [5/20], Step [374/2541], D Loss: 0.6539, G Loss: 2.1289\n",
      "Epoch [5/20], Step [375/2541], D Loss: 0.6623, G Loss: 1.9164\n",
      "Epoch [5/20], Step [376/2541], D Loss: 0.6558, G Loss: 1.9285\n",
      "Epoch [5/20], Step [377/2541], D Loss: 0.6535, G Loss: 1.9697\n",
      "Epoch [5/20], Step [378/2541], D Loss: 0.6572, G Loss: 1.9318\n",
      "Epoch [5/20], Step [379/2541], D Loss: 0.6520, G Loss: 2.0104\n",
      "Epoch [5/20], Step [380/2541], D Loss: 0.6518, G Loss: 2.2042\n",
      "Epoch [5/20], Step [381/2541], D Loss: 0.6506, G Loss: 2.2602\n",
      "Epoch [5/20], Step [382/2541], D Loss: 0.6517, G Loss: 2.2112\n",
      "Epoch [5/20], Step [383/2541], D Loss: 0.6508, G Loss: 2.1454\n",
      "Epoch [5/20], Step [384/2541], D Loss: 0.6542, G Loss: 2.0310\n",
      "Epoch [5/20], Step [385/2541], D Loss: 0.6504, G Loss: 2.0270\n",
      "Epoch [5/20], Step [386/2541], D Loss: 0.6507, G Loss: 1.9767\n",
      "Epoch [5/20], Step [387/2541], D Loss: 0.6509, G Loss: 2.0693\n",
      "Epoch [5/20], Step [388/2541], D Loss: 0.6506, G Loss: 2.0805\n",
      "Epoch [5/20], Step [389/2541], D Loss: 0.6515, G Loss: 2.0593\n",
      "Epoch [5/20], Step [390/2541], D Loss: 0.6591, G Loss: 2.0980\n",
      "Epoch [5/20], Step [391/2541], D Loss: 0.6519, G Loss: 2.0868\n",
      "Epoch [5/20], Step [392/2541], D Loss: 0.6516, G Loss: 1.9793\n",
      "Epoch [5/20], Step [393/2541], D Loss: 0.6507, G Loss: 1.9725\n",
      "Epoch [5/20], Step [394/2541], D Loss: 0.6514, G Loss: 2.1770\n",
      "Epoch [5/20], Step [395/2541], D Loss: 0.6504, G Loss: 2.1604\n",
      "Epoch [5/20], Step [396/2541], D Loss: 0.6513, G Loss: 2.2916\n",
      "Epoch [5/20], Step [397/2541], D Loss: 0.6508, G Loss: 2.0373\n",
      "Epoch [5/20], Step [398/2541], D Loss: 0.6505, G Loss: 1.9597\n",
      "Epoch [5/20], Step [399/2541], D Loss: 0.6506, G Loss: 2.0828\n",
      "Epoch [5/20], Step [400/2541], D Loss: 0.6508, G Loss: 2.0899\n",
      "Epoch [5/20], Step [401/2541], D Loss: 0.6511, G Loss: 2.1040\n",
      "Epoch [5/20], Step [402/2541], D Loss: 0.6512, G Loss: 2.2454\n",
      "Epoch [5/20], Step [403/2541], D Loss: 0.6504, G Loss: 1.9975\n",
      "Epoch [5/20], Step [404/2541], D Loss: 0.6563, G Loss: 2.0777\n",
      "Epoch [5/20], Step [405/2541], D Loss: 0.6517, G Loss: 2.1210\n",
      "Epoch [5/20], Step [406/2541], D Loss: 0.6515, G Loss: 2.0840\n",
      "Epoch [5/20], Step [407/2541], D Loss: 0.6512, G Loss: 2.1077\n",
      "Epoch [5/20], Step [408/2541], D Loss: 0.6522, G Loss: 2.1377\n",
      "Epoch [5/20], Step [409/2541], D Loss: 0.6527, G Loss: 2.0398\n",
      "Epoch [5/20], Step [410/2541], D Loss: 0.6531, G Loss: 2.0548\n",
      "Epoch [5/20], Step [411/2541], D Loss: 0.6514, G Loss: 1.9697\n",
      "Epoch [5/20], Step [412/2541], D Loss: 0.6512, G Loss: 2.0371\n",
      "Epoch [5/20], Step [413/2541], D Loss: 0.6506, G Loss: 2.0625\n",
      "Epoch [5/20], Step [414/2541], D Loss: 0.6505, G Loss: 2.0813\n",
      "Epoch [5/20], Step [415/2541], D Loss: 0.6506, G Loss: 2.3993\n",
      "Epoch [5/20], Step [416/2541], D Loss: 0.6537, G Loss: 2.0248\n",
      "Epoch [5/20], Step [417/2541], D Loss: 0.6512, G Loss: 1.9860\n",
      "Epoch [5/20], Step [418/2541], D Loss: 0.6533, G Loss: 2.0501\n",
      "Epoch [5/20], Step [419/2541], D Loss: 0.6509, G Loss: 2.1596\n",
      "Epoch [5/20], Step [420/2541], D Loss: 0.6510, G Loss: 2.0982\n",
      "Epoch [5/20], Step [421/2541], D Loss: 0.6536, G Loss: 2.0631\n",
      "Epoch [5/20], Step [422/2541], D Loss: 0.6506, G Loss: 2.0447\n",
      "Epoch [5/20], Step [423/2541], D Loss: 0.6534, G Loss: 1.9980\n",
      "Epoch [5/20], Step [424/2541], D Loss: 0.6511, G Loss: 2.0481\n",
      "Epoch [5/20], Step [425/2541], D Loss: 0.6528, G Loss: 2.0706\n",
      "Epoch [5/20], Step [426/2541], D Loss: 0.6525, G Loss: 2.0657\n",
      "Epoch [5/20], Step [427/2541], D Loss: 0.6525, G Loss: 1.9798\n",
      "Epoch [5/20], Step [428/2541], D Loss: 0.6514, G Loss: 2.0065\n",
      "Epoch [5/20], Step [429/2541], D Loss: 0.6568, G Loss: 2.0936\n",
      "Epoch [5/20], Step [430/2541], D Loss: 0.6526, G Loss: 1.9002\n",
      "Epoch [5/20], Step [431/2541], D Loss: 0.6537, G Loss: 2.0475\n",
      "Epoch [5/20], Step [432/2541], D Loss: 0.6529, G Loss: 2.0368\n",
      "Epoch [5/20], Step [433/2541], D Loss: 0.6560, G Loss: 2.1170\n",
      "Epoch [5/20], Step [434/2541], D Loss: 0.6526, G Loss: 2.1323\n",
      "Epoch [5/20], Step [435/2541], D Loss: 0.6521, G Loss: 2.0796\n",
      "Epoch [5/20], Step [436/2541], D Loss: 0.6508, G Loss: 2.0514\n",
      "Epoch [5/20], Step [437/2541], D Loss: 0.6545, G Loss: 2.1752\n",
      "Epoch [5/20], Step [438/2541], D Loss: 0.6551, G Loss: 2.0678\n",
      "Epoch [5/20], Step [439/2541], D Loss: 0.6518, G Loss: 1.9027\n",
      "Epoch [5/20], Step [440/2541], D Loss: 0.6520, G Loss: 2.0106\n",
      "Epoch [5/20], Step [441/2541], D Loss: 0.6513, G Loss: 2.0838\n",
      "Epoch [5/20], Step [442/2541], D Loss: 0.6505, G Loss: 2.1191\n",
      "Epoch [5/20], Step [443/2541], D Loss: 0.6504, G Loss: 2.1496\n",
      "Epoch [5/20], Step [444/2541], D Loss: 0.6505, G Loss: 2.1422\n",
      "Epoch [5/20], Step [445/2541], D Loss: 0.6504, G Loss: 2.1152\n",
      "Epoch [5/20], Step [446/2541], D Loss: 0.6507, G Loss: 2.1036\n",
      "Epoch [5/20], Step [447/2541], D Loss: 0.6503, G Loss: 2.1039\n",
      "Epoch [5/20], Step [448/2541], D Loss: 0.6503, G Loss: 2.1035\n",
      "Epoch [5/20], Step [449/2541], D Loss: 0.6503, G Loss: 2.0534\n",
      "Epoch [5/20], Step [450/2541], D Loss: 0.6503, G Loss: 2.0892\n",
      "Epoch [5/20], Step [451/2541], D Loss: 0.6503, G Loss: 2.0308\n",
      "Epoch [5/20], Step [452/2541], D Loss: 0.6505, G Loss: 1.9124\n",
      "Epoch [5/20], Step [453/2541], D Loss: 0.6505, G Loss: 2.0429\n",
      "Epoch [5/20], Step [454/2541], D Loss: 0.6506, G Loss: 1.9894\n",
      "Epoch [5/20], Step [455/2541], D Loss: 0.6510, G Loss: 2.0223\n",
      "Epoch [5/20], Step [456/2541], D Loss: 0.6507, G Loss: 2.0996\n",
      "Epoch [5/20], Step [457/2541], D Loss: 0.6506, G Loss: 2.1137\n",
      "Epoch [5/20], Step [458/2541], D Loss: 0.6507, G Loss: 2.0977\n",
      "Epoch [5/20], Step [459/2541], D Loss: 0.6506, G Loss: 2.0721\n",
      "Epoch [5/20], Step [460/2541], D Loss: 0.6507, G Loss: 2.1611\n",
      "Epoch [5/20], Step [461/2541], D Loss: 0.6508, G Loss: 2.1360\n",
      "Epoch [5/20], Step [462/2541], D Loss: 0.6508, G Loss: 2.0450\n",
      "Epoch [5/20], Step [463/2541], D Loss: 0.6506, G Loss: 2.0101\n",
      "Epoch [5/20], Step [464/2541], D Loss: 0.6505, G Loss: 2.0492\n",
      "Epoch [5/20], Step [465/2541], D Loss: 0.6503, G Loss: 2.1334\n",
      "Epoch [5/20], Step [466/2541], D Loss: 0.6505, G Loss: 2.0857\n",
      "Epoch [5/20], Step [467/2541], D Loss: 0.6503, G Loss: 2.0482\n",
      "Epoch [5/20], Step [468/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [5/20], Step [469/2541], D Loss: 0.6504, G Loss: 2.1172\n",
      "Epoch [5/20], Step [470/2541], D Loss: 0.6509, G Loss: 2.1019\n",
      "Epoch [5/20], Step [471/2541], D Loss: 0.6506, G Loss: 2.1420\n",
      "Epoch [5/20], Step [472/2541], D Loss: 0.6506, G Loss: 2.1009\n",
      "Epoch [5/20], Step [473/2541], D Loss: 0.6503, G Loss: 2.1661\n",
      "Epoch [5/20], Step [474/2541], D Loss: 0.6507, G Loss: 2.0080\n",
      "Epoch [5/20], Step [475/2541], D Loss: 0.6503, G Loss: 2.0406\n",
      "Epoch [5/20], Step [476/2541], D Loss: 0.6506, G Loss: 2.0179\n",
      "Epoch [5/20], Step [477/2541], D Loss: 0.6503, G Loss: 2.0431\n",
      "Epoch [5/20], Step [478/2541], D Loss: 0.6504, G Loss: 2.0312\n",
      "Epoch [5/20], Step [479/2541], D Loss: 0.6504, G Loss: 2.1339\n",
      "Epoch [5/20], Step [480/2541], D Loss: 0.6513, G Loss: 2.1230\n",
      "Epoch [5/20], Step [481/2541], D Loss: 0.6513, G Loss: 2.1798\n",
      "Epoch [5/20], Step [482/2541], D Loss: 0.6507, G Loss: 2.1311\n",
      "Epoch [5/20], Step [483/2541], D Loss: 0.6508, G Loss: 2.1576\n",
      "Epoch [5/20], Step [484/2541], D Loss: 0.6505, G Loss: 2.0455\n",
      "Epoch [5/20], Step [485/2541], D Loss: 0.6507, G Loss: 2.0202\n",
      "Epoch [5/20], Step [486/2541], D Loss: 0.6508, G Loss: 2.0825\n",
      "Epoch [5/20], Step [487/2541], D Loss: 0.6508, G Loss: 2.0176\n",
      "Epoch [5/20], Step [488/2541], D Loss: 0.6503, G Loss: 2.0473\n",
      "Epoch [5/20], Step [489/2541], D Loss: 0.6504, G Loss: 2.0182\n",
      "Epoch [5/20], Step [490/2541], D Loss: 0.6505, G Loss: 2.0779\n",
      "Epoch [5/20], Step [491/2541], D Loss: 0.6505, G Loss: 2.0797\n",
      "Epoch [5/20], Step [492/2541], D Loss: 0.6503, G Loss: 2.1027\n",
      "Epoch [5/20], Step [493/2541], D Loss: 0.6505, G Loss: 2.0544\n",
      "Epoch [5/20], Step [494/2541], D Loss: 0.6504, G Loss: 2.1359\n",
      "Epoch [5/20], Step [495/2541], D Loss: 0.6507, G Loss: 2.0994\n",
      "Epoch [5/20], Step [496/2541], D Loss: 0.6503, G Loss: 2.0671\n",
      "Epoch [5/20], Step [497/2541], D Loss: 0.6505, G Loss: 1.9734\n",
      "Epoch [5/20], Step [498/2541], D Loss: 0.6505, G Loss: 2.1140\n",
      "Epoch [5/20], Step [499/2541], D Loss: 0.6503, G Loss: 2.0622\n",
      "Epoch [5/20], Step [500/2541], D Loss: 0.6508, G Loss: 2.0954\n",
      "Epoch [5/20], Step [501/2541], D Loss: 0.6505, G Loss: 2.1528\n",
      "Epoch [5/20], Step [502/2541], D Loss: 0.6508, G Loss: 2.0626\n",
      "Epoch [5/20], Step [503/2541], D Loss: 0.6514, G Loss: 2.1629\n",
      "Epoch [5/20], Step [504/2541], D Loss: 0.6520, G Loss: 2.1407\n",
      "Epoch [5/20], Step [505/2541], D Loss: 0.6507, G Loss: 2.0909\n",
      "Epoch [5/20], Step [506/2541], D Loss: 0.6513, G Loss: 2.0354\n",
      "Epoch [5/20], Step [507/2541], D Loss: 0.6507, G Loss: 2.1053\n",
      "Epoch [5/20], Step [508/2541], D Loss: 0.6503, G Loss: 2.0722\n",
      "Epoch [5/20], Step [509/2541], D Loss: 0.6503, G Loss: 2.1103\n",
      "Epoch [5/20], Step [510/2541], D Loss: 0.6505, G Loss: 2.1053\n",
      "Epoch [5/20], Step [511/2541], D Loss: 0.6515, G Loss: 2.0919\n",
      "Epoch [5/20], Step [512/2541], D Loss: 0.6505, G Loss: 2.0911\n",
      "Epoch [5/20], Step [513/2541], D Loss: 0.6505, G Loss: 2.0727\n",
      "Epoch [5/20], Step [514/2541], D Loss: 0.6503, G Loss: 2.0843\n",
      "Epoch [5/20], Step [515/2541], D Loss: 0.6503, G Loss: 2.0709\n",
      "Epoch [5/20], Step [516/2541], D Loss: 0.6503, G Loss: 2.0506\n",
      "Epoch [5/20], Step [517/2541], D Loss: 0.6503, G Loss: 2.1439\n",
      "Epoch [5/20], Step [518/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [5/20], Step [519/2541], D Loss: 0.6503, G Loss: 2.0716\n",
      "Epoch [5/20], Step [520/2541], D Loss: 0.6503, G Loss: 2.2397\n",
      "Epoch [5/20], Step [521/2541], D Loss: 0.6505, G Loss: 2.0884\n",
      "Epoch [5/20], Step [522/2541], D Loss: 0.6515, G Loss: 2.0801\n",
      "Epoch [5/20], Step [523/2541], D Loss: 0.6510, G Loss: 2.0997\n",
      "Epoch [5/20], Step [524/2541], D Loss: 0.6504, G Loss: 2.0895\n",
      "Epoch [5/20], Step [525/2541], D Loss: 0.6505, G Loss: 2.0554\n",
      "Epoch [5/20], Step [526/2541], D Loss: 0.6535, G Loss: 2.0688\n",
      "Epoch [5/20], Step [527/2541], D Loss: 0.6515, G Loss: 2.0636\n",
      "Epoch [5/20], Step [528/2541], D Loss: 0.6511, G Loss: 1.9657\n",
      "Epoch [5/20], Step [529/2541], D Loss: 0.6509, G Loss: 2.0504\n",
      "Epoch [5/20], Step [530/2541], D Loss: 0.6507, G Loss: 2.0889\n",
      "Epoch [5/20], Step [531/2541], D Loss: 0.6510, G Loss: 2.0467\n",
      "Epoch [5/20], Step [532/2541], D Loss: 0.6503, G Loss: 2.0442\n",
      "Epoch [5/20], Step [533/2541], D Loss: 0.6503, G Loss: 2.0917\n",
      "Epoch [5/20], Step [534/2541], D Loss: 0.6505, G Loss: 2.0626\n",
      "Epoch [5/20], Step [535/2541], D Loss: 0.6504, G Loss: 2.1064\n",
      "Epoch [5/20], Step [536/2541], D Loss: 0.6505, G Loss: 2.0651\n",
      "Epoch [5/20], Step [537/2541], D Loss: 0.6505, G Loss: 2.0134\n",
      "Epoch [5/20], Step [538/2541], D Loss: 0.6504, G Loss: 2.0730\n",
      "Epoch [5/20], Step [539/2541], D Loss: 0.6502, G Loss: 2.0459\n",
      "Epoch [5/20], Step [540/2541], D Loss: 0.6504, G Loss: 2.1110\n",
      "Epoch [5/20], Step [541/2541], D Loss: 0.6503, G Loss: 2.0809\n",
      "Epoch [5/20], Step [542/2541], D Loss: 0.6503, G Loss: 2.0736\n",
      "Epoch [5/20], Step [543/2541], D Loss: 0.6514, G Loss: 2.1336\n",
      "Epoch [5/20], Step [544/2541], D Loss: 0.6505, G Loss: 2.1304\n",
      "Epoch [5/20], Step [545/2541], D Loss: 0.6504, G Loss: 2.1202\n",
      "Epoch [5/20], Step [546/2541], D Loss: 0.6503, G Loss: 2.1180\n",
      "Epoch [5/20], Step [547/2541], D Loss: 0.6529, G Loss: 2.0028\n",
      "Epoch [5/20], Step [548/2541], D Loss: 0.6507, G Loss: 1.9951\n",
      "Epoch [5/20], Step [549/2541], D Loss: 0.6512, G Loss: 1.9383\n",
      "Epoch [5/20], Step [550/2541], D Loss: 0.6504, G Loss: 2.0584\n",
      "Epoch [5/20], Step [551/2541], D Loss: 0.6504, G Loss: 2.3259\n",
      "Epoch [5/20], Step [552/2541], D Loss: 0.6504, G Loss: 2.0963\n",
      "Epoch [5/20], Step [553/2541], D Loss: 0.6523, G Loss: 2.0775\n",
      "Epoch [5/20], Step [554/2541], D Loss: 0.6508, G Loss: 2.0133\n",
      "Epoch [5/20], Step [555/2541], D Loss: 0.6508, G Loss: 2.1280\n",
      "Epoch [5/20], Step [556/2541], D Loss: 0.6505, G Loss: 2.0858\n",
      "Epoch [5/20], Step [557/2541], D Loss: 0.6505, G Loss: 2.0905\n",
      "Epoch [5/20], Step [558/2541], D Loss: 0.6503, G Loss: 2.1029\n",
      "Epoch [5/20], Step [559/2541], D Loss: 0.6533, G Loss: 2.0926\n",
      "Epoch [5/20], Step [560/2541], D Loss: 0.6507, G Loss: 2.0823\n",
      "Epoch [5/20], Step [561/2541], D Loss: 0.6507, G Loss: 2.0717\n",
      "Epoch [5/20], Step [562/2541], D Loss: 0.6508, G Loss: 2.1188\n",
      "Epoch [5/20], Step [563/2541], D Loss: 0.6508, G Loss: 2.1498\n",
      "Epoch [5/20], Step [564/2541], D Loss: 0.6504, G Loss: 2.0156\n",
      "Epoch [5/20], Step [565/2541], D Loss: 0.6504, G Loss: 2.0216\n",
      "Epoch [5/20], Step [566/2541], D Loss: 0.6503, G Loss: 2.1150\n",
      "Epoch [5/20], Step [567/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [5/20], Step [568/2541], D Loss: 0.6504, G Loss: 2.0840\n",
      "Epoch [5/20], Step [569/2541], D Loss: 0.6503, G Loss: 2.0977\n",
      "Epoch [5/20], Step [570/2541], D Loss: 0.6503, G Loss: 2.0907\n",
      "Epoch [5/20], Step [571/2541], D Loss: 0.6505, G Loss: 2.0439\n",
      "Epoch [5/20], Step [572/2541], D Loss: 0.6503, G Loss: 2.0341\n",
      "Epoch [5/20], Step [573/2541], D Loss: 0.6504, G Loss: 2.0597\n",
      "Epoch [5/20], Step [574/2541], D Loss: 0.6504, G Loss: 2.0718\n",
      "Epoch [5/20], Step [575/2541], D Loss: 0.6504, G Loss: 2.0438\n",
      "Epoch [5/20], Step [576/2541], D Loss: 0.6504, G Loss: 2.0721\n",
      "Epoch [5/20], Step [577/2541], D Loss: 0.6503, G Loss: 2.0830\n",
      "Epoch [5/20], Step [578/2541], D Loss: 0.6502, G Loss: 2.1257\n",
      "Epoch [5/20], Step [579/2541], D Loss: 0.6593, G Loss: 1.9723\n",
      "Epoch [5/20], Step [580/2541], D Loss: 0.6511, G Loss: 2.0228\n",
      "Epoch [5/20], Step [581/2541], D Loss: 0.6511, G Loss: 2.1036\n",
      "Epoch [5/20], Step [582/2541], D Loss: 0.6525, G Loss: 2.0826\n",
      "Epoch [5/20], Step [583/2541], D Loss: 0.6508, G Loss: 2.1105\n",
      "Epoch [5/20], Step [584/2541], D Loss: 0.6505, G Loss: 2.1125\n",
      "Epoch [5/20], Step [585/2541], D Loss: 0.6504, G Loss: 2.1201\n",
      "Epoch [5/20], Step [586/2541], D Loss: 0.6512, G Loss: 2.0382\n",
      "Epoch [5/20], Step [587/2541], D Loss: 0.6508, G Loss: 2.0919\n",
      "Epoch [5/20], Step [588/2541], D Loss: 0.6509, G Loss: 2.0429\n",
      "Epoch [5/20], Step [589/2541], D Loss: 0.6511, G Loss: 2.1149\n",
      "Epoch [5/20], Step [590/2541], D Loss: 0.6513, G Loss: 2.0585\n",
      "Epoch [5/20], Step [591/2541], D Loss: 0.6503, G Loss: 2.0994\n",
      "Epoch [5/20], Step [592/2541], D Loss: 0.6504, G Loss: 2.1298\n",
      "Epoch [5/20], Step [593/2541], D Loss: 0.6504, G Loss: 2.1077\n",
      "Epoch [5/20], Step [594/2541], D Loss: 0.6505, G Loss: 2.0741\n",
      "Epoch [5/20], Step [595/2541], D Loss: 0.6503, G Loss: 2.0553\n",
      "Epoch [5/20], Step [596/2541], D Loss: 0.6503, G Loss: 2.0411\n",
      "Epoch [5/20], Step [597/2541], D Loss: 0.6505, G Loss: 2.0369\n",
      "Epoch [5/20], Step [598/2541], D Loss: 0.6505, G Loss: 2.0972\n",
      "Epoch [5/20], Step [599/2541], D Loss: 0.6503, G Loss: 2.1106\n",
      "Epoch [5/20], Step [600/2541], D Loss: 0.6503, G Loss: 2.0504\n",
      "Epoch [5/20], Step [601/2541], D Loss: 0.6503, G Loss: 2.0941\n",
      "Epoch [5/20], Step [602/2541], D Loss: 0.6503, G Loss: 2.0454\n",
      "Epoch [5/20], Step [603/2541], D Loss: 0.6508, G Loss: 2.0467\n",
      "Epoch [5/20], Step [604/2541], D Loss: 0.6504, G Loss: 2.1262\n",
      "Epoch [5/20], Step [605/2541], D Loss: 0.6505, G Loss: 2.1057\n",
      "Epoch [5/20], Step [606/2541], D Loss: 0.6503, G Loss: 2.1080\n",
      "Epoch [5/20], Step [607/2541], D Loss: 0.6504, G Loss: 2.0727\n",
      "Epoch [5/20], Step [608/2541], D Loss: 0.6503, G Loss: 2.0647\n",
      "Epoch [5/20], Step [609/2541], D Loss: 0.6506, G Loss: 2.1018\n",
      "Epoch [5/20], Step [610/2541], D Loss: 0.6506, G Loss: 2.0934\n",
      "Epoch [5/20], Step [611/2541], D Loss: 0.6503, G Loss: 2.0997\n",
      "Epoch [5/20], Step [612/2541], D Loss: 0.6502, G Loss: 2.0453\n",
      "Epoch [5/20], Step [613/2541], D Loss: 0.6504, G Loss: 2.0573\n",
      "Epoch [5/20], Step [614/2541], D Loss: 0.6503, G Loss: 2.1058\n",
      "Epoch [5/20], Step [615/2541], D Loss: 0.6503, G Loss: 2.0301\n",
      "Epoch [5/20], Step [616/2541], D Loss: 0.6503, G Loss: 2.0297\n",
      "Epoch [5/20], Step [617/2541], D Loss: 0.6505, G Loss: 2.0718\n",
      "Epoch [5/20], Step [618/2541], D Loss: 0.6503, G Loss: 2.1068\n",
      "Epoch [5/20], Step [619/2541], D Loss: 0.6503, G Loss: 2.1201\n",
      "Epoch [5/20], Step [620/2541], D Loss: 0.6503, G Loss: 2.0867\n",
      "Epoch [5/20], Step [621/2541], D Loss: 0.6503, G Loss: 2.0630\n",
      "Epoch [5/20], Step [622/2541], D Loss: 0.6505, G Loss: 2.1076\n",
      "Epoch [5/20], Step [623/2541], D Loss: 0.6503, G Loss: 2.0612\n",
      "Epoch [5/20], Step [624/2541], D Loss: 0.6502, G Loss: 2.1150\n",
      "Epoch [5/20], Step [625/2541], D Loss: 0.6503, G Loss: 2.0947\n",
      "Epoch [5/20], Step [626/2541], D Loss: 0.6503, G Loss: 2.1311\n",
      "Epoch [5/20], Step [627/2541], D Loss: 0.6503, G Loss: 2.0891\n",
      "Epoch [5/20], Step [628/2541], D Loss: 0.6503, G Loss: 2.0757\n",
      "Epoch [5/20], Step [629/2541], D Loss: 0.6506, G Loss: 2.1303\n",
      "Epoch [5/20], Step [630/2541], D Loss: 0.6503, G Loss: 2.1482\n",
      "Epoch [5/20], Step [631/2541], D Loss: 0.6502, G Loss: 2.1365\n",
      "Epoch [5/20], Step [632/2541], D Loss: 0.6508, G Loss: 2.1090\n",
      "Epoch [5/20], Step [633/2541], D Loss: 0.6503, G Loss: 2.0536\n",
      "Epoch [5/20], Step [634/2541], D Loss: 0.6504, G Loss: 2.0624\n",
      "Epoch [5/20], Step [635/2541], D Loss: 0.6504, G Loss: 2.0934\n",
      "Epoch [5/20], Step [636/2541], D Loss: 0.6505, G Loss: 2.0620\n",
      "Epoch [5/20], Step [637/2541], D Loss: 0.6503, G Loss: 2.0600\n",
      "Epoch [5/20], Step [638/2541], D Loss: 0.6503, G Loss: 2.0706\n",
      "Epoch [5/20], Step [639/2541], D Loss: 0.6503, G Loss: 2.0177\n",
      "Epoch [5/20], Step [640/2541], D Loss: 0.6502, G Loss: 2.0567\n",
      "Epoch [5/20], Step [641/2541], D Loss: 0.6502, G Loss: 2.0231\n",
      "Epoch [5/20], Step [642/2541], D Loss: 0.6506, G Loss: 2.0562\n",
      "Epoch [5/20], Step [643/2541], D Loss: 0.6504, G Loss: 2.0889\n",
      "Epoch [5/20], Step [644/2541], D Loss: 0.6508, G Loss: 2.1021\n",
      "Epoch [5/20], Step [645/2541], D Loss: 0.6504, G Loss: 2.1145\n",
      "Epoch [5/20], Step [646/2541], D Loss: 0.6503, G Loss: 2.0996\n",
      "Epoch [5/20], Step [647/2541], D Loss: 0.6503, G Loss: 2.0883\n",
      "Epoch [5/20], Step [648/2541], D Loss: 0.6505, G Loss: 2.1102\n",
      "Epoch [5/20], Step [649/2541], D Loss: 0.6502, G Loss: 2.0971\n",
      "Epoch [5/20], Step [650/2541], D Loss: 0.6504, G Loss: 2.0601\n",
      "Epoch [5/20], Step [651/2541], D Loss: 0.6503, G Loss: 2.0818\n",
      "Epoch [5/20], Step [652/2541], D Loss: 0.6504, G Loss: 2.0383\n",
      "Epoch [5/20], Step [653/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [5/20], Step [654/2541], D Loss: 0.6502, G Loss: 2.0219\n",
      "Epoch [5/20], Step [655/2541], D Loss: 0.6505, G Loss: 2.0409\n",
      "Epoch [5/20], Step [656/2541], D Loss: 0.6533, G Loss: 2.1023\n",
      "Epoch [5/20], Step [657/2541], D Loss: 0.6509, G Loss: 2.1012\n",
      "Epoch [5/20], Step [658/2541], D Loss: 0.6515, G Loss: 2.1076\n",
      "Epoch [5/20], Step [659/2541], D Loss: 0.6506, G Loss: 2.0118\n",
      "Epoch [5/20], Step [660/2541], D Loss: 0.6503, G Loss: 2.0158\n",
      "Epoch [5/20], Step [661/2541], D Loss: 0.6507, G Loss: 2.0441\n",
      "Epoch [5/20], Step [662/2541], D Loss: 0.6505, G Loss: 2.1281\n",
      "Epoch [5/20], Step [663/2541], D Loss: 0.6504, G Loss: 2.0920\n",
      "Epoch [5/20], Step [664/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [5/20], Step [665/2541], D Loss: 0.6504, G Loss: 2.0691\n",
      "Epoch [5/20], Step [666/2541], D Loss: 0.6504, G Loss: 2.0753\n",
      "Epoch [5/20], Step [667/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [5/20], Step [668/2541], D Loss: 0.6503, G Loss: 2.0649\n",
      "Epoch [5/20], Step [669/2541], D Loss: 0.6504, G Loss: 2.0654\n",
      "Epoch [5/20], Step [670/2541], D Loss: 0.6505, G Loss: 2.0943\n",
      "Epoch [5/20], Step [671/2541], D Loss: 0.6505, G Loss: 2.1102\n",
      "Epoch [5/20], Step [672/2541], D Loss: 0.6504, G Loss: 2.1062\n",
      "Epoch [5/20], Step [673/2541], D Loss: 0.6509, G Loss: 2.0928\n",
      "Epoch [5/20], Step [674/2541], D Loss: 0.6510, G Loss: 2.1053\n",
      "Epoch [5/20], Step [675/2541], D Loss: 0.6514, G Loss: 2.1143\n",
      "Epoch [5/20], Step [676/2541], D Loss: 0.6503, G Loss: 2.1275\n",
      "Epoch [5/20], Step [677/2541], D Loss: 0.6505, G Loss: 2.0991\n",
      "Epoch [5/20], Step [678/2541], D Loss: 0.6504, G Loss: 1.9952\n",
      "Epoch [5/20], Step [679/2541], D Loss: 0.6503, G Loss: 2.0603\n",
      "Epoch [5/20], Step [680/2541], D Loss: 0.6506, G Loss: 2.0380\n",
      "Epoch [5/20], Step [681/2541], D Loss: 0.6504, G Loss: 2.0773\n",
      "Epoch [5/20], Step [682/2541], D Loss: 0.6505, G Loss: 2.0509\n",
      "Epoch [5/20], Step [683/2541], D Loss: 0.6505, G Loss: 2.0914\n",
      "Epoch [5/20], Step [684/2541], D Loss: 0.6505, G Loss: 2.0912\n",
      "Epoch [5/20], Step [685/2541], D Loss: 0.6503, G Loss: 2.0644\n",
      "Epoch [5/20], Step [686/2541], D Loss: 0.6503, G Loss: 2.0967\n",
      "Epoch [5/20], Step [687/2541], D Loss: 0.6504, G Loss: 2.0691\n",
      "Epoch [5/20], Step [688/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [5/20], Step [689/2541], D Loss: 0.6502, G Loss: 2.0568\n",
      "Epoch [5/20], Step [690/2541], D Loss: 0.6503, G Loss: 2.0306\n",
      "Epoch [5/20], Step [691/2541], D Loss: 0.6503, G Loss: 2.0626\n",
      "Epoch [5/20], Step [692/2541], D Loss: 0.6503, G Loss: 2.1323\n",
      "Epoch [5/20], Step [693/2541], D Loss: 0.6503, G Loss: 2.1089\n",
      "Epoch [5/20], Step [694/2541], D Loss: 0.6503, G Loss: 2.1253\n",
      "Epoch [5/20], Step [695/2541], D Loss: 0.6503, G Loss: 2.1226\n",
      "Epoch [5/20], Step [696/2541], D Loss: 0.6504, G Loss: 2.1592\n",
      "Epoch [5/20], Step [697/2541], D Loss: 0.6505, G Loss: 2.0565\n",
      "Epoch [5/20], Step [698/2541], D Loss: 0.6503, G Loss: 2.0619\n",
      "Epoch [5/20], Step [699/2541], D Loss: 0.6503, G Loss: 2.1514\n",
      "Epoch [5/20], Step [700/2541], D Loss: 0.6504, G Loss: 2.0659\n",
      "Epoch [5/20], Step [701/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [5/20], Step [702/2541], D Loss: 0.6503, G Loss: 2.0362\n",
      "Epoch [5/20], Step [703/2541], D Loss: 0.6504, G Loss: 2.0377\n",
      "Epoch [5/20], Step [704/2541], D Loss: 0.6502, G Loss: 2.0625\n",
      "Epoch [5/20], Step [705/2541], D Loss: 0.6504, G Loss: 2.0594\n",
      "Epoch [5/20], Step [706/2541], D Loss: 0.6505, G Loss: 2.0594\n",
      "Epoch [5/20], Step [707/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [5/20], Step [708/2541], D Loss: 0.6503, G Loss: 2.0813\n",
      "Epoch [5/20], Step [709/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [5/20], Step [710/2541], D Loss: 0.6504, G Loss: 2.1112\n",
      "Epoch [5/20], Step [711/2541], D Loss: 0.6503, G Loss: 2.1156\n",
      "Epoch [5/20], Step [712/2541], D Loss: 0.6503, G Loss: 2.1082\n",
      "Epoch [5/20], Step [713/2541], D Loss: 0.6502, G Loss: 2.1170\n",
      "Epoch [5/20], Step [714/2541], D Loss: 0.6502, G Loss: 2.0960\n",
      "Epoch [5/20], Step [715/2541], D Loss: 0.6502, G Loss: 2.0527\n",
      "Epoch [5/20], Step [716/2541], D Loss: 0.6505, G Loss: 2.0493\n",
      "Epoch [5/20], Step [717/2541], D Loss: 0.6504, G Loss: 2.0240\n",
      "Epoch [5/20], Step [718/2541], D Loss: 0.6507, G Loss: 2.1019\n",
      "Epoch [5/20], Step [719/2541], D Loss: 0.6503, G Loss: 2.0965\n",
      "Epoch [5/20], Step [720/2541], D Loss: 0.6503, G Loss: 2.0651\n",
      "Epoch [5/20], Step [721/2541], D Loss: 0.6504, G Loss: 2.0845\n",
      "Epoch [5/20], Step [722/2541], D Loss: 0.6504, G Loss: 2.0776\n",
      "Epoch [5/20], Step [723/2541], D Loss: 0.6504, G Loss: 2.0594\n",
      "Epoch [5/20], Step [724/2541], D Loss: 0.6504, G Loss: 2.1057\n",
      "Epoch [5/20], Step [725/2541], D Loss: 0.6508, G Loss: 2.0939\n",
      "Epoch [5/20], Step [726/2541], D Loss: 0.6508, G Loss: 2.0884\n",
      "Epoch [5/20], Step [727/2541], D Loss: 0.6509, G Loss: 2.0864\n",
      "Epoch [5/20], Step [728/2541], D Loss: 0.6503, G Loss: 2.0571\n",
      "Epoch [5/20], Step [729/2541], D Loss: 0.6504, G Loss: 2.0832\n",
      "Epoch [5/20], Step [730/2541], D Loss: 0.6503, G Loss: 2.0948\n",
      "Epoch [5/20], Step [731/2541], D Loss: 0.6502, G Loss: 2.1258\n",
      "Epoch [5/20], Step [732/2541], D Loss: 0.6502, G Loss: 2.0089\n",
      "Epoch [5/20], Step [733/2541], D Loss: 0.6502, G Loss: 2.1113\n",
      "Epoch [5/20], Step [734/2541], D Loss: 0.6502, G Loss: 2.1335\n",
      "Epoch [5/20], Step [735/2541], D Loss: 0.6504, G Loss: 2.1031\n",
      "Epoch [5/20], Step [736/2541], D Loss: 0.6504, G Loss: 2.1335\n",
      "Epoch [5/20], Step [737/2541], D Loss: 0.6503, G Loss: 2.1196\n",
      "Epoch [5/20], Step [738/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [5/20], Step [739/2541], D Loss: 0.6503, G Loss: 2.0833\n",
      "Epoch [5/20], Step [740/2541], D Loss: 0.6508, G Loss: 2.1100\n",
      "Epoch [5/20], Step [741/2541], D Loss: 0.6504, G Loss: 2.1028\n",
      "Epoch [5/20], Step [742/2541], D Loss: 0.6505, G Loss: 2.1094\n",
      "Epoch [5/20], Step [743/2541], D Loss: 0.6502, G Loss: 2.0397\n",
      "Epoch [5/20], Step [744/2541], D Loss: 0.6505, G Loss: 2.0597\n",
      "Epoch [5/20], Step [745/2541], D Loss: 0.6506, G Loss: 2.0516\n",
      "Epoch [5/20], Step [746/2541], D Loss: 0.6505, G Loss: 2.0568\n",
      "Epoch [5/20], Step [747/2541], D Loss: 0.6503, G Loss: 2.0903\n",
      "Epoch [5/20], Step [748/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [5/20], Step [749/2541], D Loss: 0.6503, G Loss: 2.0626\n",
      "Epoch [5/20], Step [750/2541], D Loss: 0.6505, G Loss: 2.0797\n",
      "Epoch [5/20], Step [751/2541], D Loss: 0.6503, G Loss: 2.0828\n",
      "Epoch [5/20], Step [752/2541], D Loss: 0.6504, G Loss: 2.1249\n",
      "Epoch [5/20], Step [753/2541], D Loss: 0.6504, G Loss: 2.0828\n",
      "Epoch [5/20], Step [754/2541], D Loss: 0.6503, G Loss: 2.0733\n",
      "Epoch [5/20], Step [755/2541], D Loss: 0.6504, G Loss: 2.0904\n",
      "Epoch [5/20], Step [756/2541], D Loss: 0.6503, G Loss: 2.1006\n",
      "Epoch [5/20], Step [757/2541], D Loss: 0.6503, G Loss: 2.1009\n",
      "Epoch [5/20], Step [758/2541], D Loss: 0.6502, G Loss: 2.0283\n",
      "Epoch [5/20], Step [759/2541], D Loss: 0.6504, G Loss: 2.1248\n",
      "Epoch [5/20], Step [760/2541], D Loss: 0.6503, G Loss: 2.1115\n",
      "Epoch [5/20], Step [761/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [5/20], Step [762/2541], D Loss: 0.6503, G Loss: 2.1010\n",
      "Epoch [5/20], Step [763/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [5/20], Step [764/2541], D Loss: 0.6503, G Loss: 2.1044\n",
      "Epoch [5/20], Step [765/2541], D Loss: 0.6502, G Loss: 2.0606\n",
      "Epoch [5/20], Step [766/2541], D Loss: 0.6506, G Loss: 2.0866\n",
      "Epoch [5/20], Step [767/2541], D Loss: 0.6508, G Loss: 2.0820\n",
      "Epoch [5/20], Step [768/2541], D Loss: 0.6511, G Loss: 2.1021\n",
      "Epoch [5/20], Step [769/2541], D Loss: 0.6505, G Loss: 2.1045\n",
      "Epoch [5/20], Step [770/2541], D Loss: 0.6504, G Loss: 2.0870\n",
      "Epoch [5/20], Step [771/2541], D Loss: 0.6511, G Loss: 2.0889\n",
      "Epoch [5/20], Step [772/2541], D Loss: 0.6506, G Loss: 2.1212\n",
      "Epoch [5/20], Step [773/2541], D Loss: 0.6510, G Loss: 2.0885\n",
      "Epoch [5/20], Step [774/2541], D Loss: 0.6505, G Loss: 2.0704\n",
      "Epoch [5/20], Step [775/2541], D Loss: 0.6510, G Loss: 2.0561\n",
      "Epoch [5/20], Step [776/2541], D Loss: 0.6506, G Loss: 2.1127\n",
      "Epoch [5/20], Step [777/2541], D Loss: 0.6509, G Loss: 2.1306\n",
      "Epoch [5/20], Step [778/2541], D Loss: 0.6507, G Loss: 2.0597\n",
      "Epoch [5/20], Step [779/2541], D Loss: 0.6505, G Loss: 2.0871\n",
      "Epoch [5/20], Step [780/2541], D Loss: 0.6506, G Loss: 2.0930\n",
      "Epoch [5/20], Step [781/2541], D Loss: 0.6503, G Loss: 2.0861\n",
      "Epoch [5/20], Step [782/2541], D Loss: 0.6510, G Loss: 2.1394\n",
      "Epoch [5/20], Step [783/2541], D Loss: 0.6507, G Loss: 2.0942\n",
      "Epoch [5/20], Step [784/2541], D Loss: 0.6503, G Loss: 2.1027\n",
      "Epoch [5/20], Step [785/2541], D Loss: 0.6503, G Loss: 2.0671\n",
      "Epoch [5/20], Step [786/2541], D Loss: 0.6503, G Loss: 2.0680\n",
      "Epoch [5/20], Step [787/2541], D Loss: 0.6505, G Loss: 2.0675\n",
      "Epoch [5/20], Step [788/2541], D Loss: 0.6502, G Loss: 2.0924\n",
      "Epoch [5/20], Step [789/2541], D Loss: 0.6506, G Loss: 2.1014\n",
      "Epoch [5/20], Step [790/2541], D Loss: 0.6504, G Loss: 2.0307\n",
      "Epoch [5/20], Step [791/2541], D Loss: 0.6505, G Loss: 2.0812\n",
      "Epoch [5/20], Step [792/2541], D Loss: 0.6504, G Loss: 2.0654\n",
      "Epoch [5/20], Step [793/2541], D Loss: 0.6504, G Loss: 2.0672\n",
      "Epoch [5/20], Step [794/2541], D Loss: 0.6505, G Loss: 2.0685\n",
      "Epoch [5/20], Step [795/2541], D Loss: 0.6506, G Loss: 2.0092\n",
      "Epoch [5/20], Step [796/2541], D Loss: 0.6510, G Loss: 2.1457\n",
      "Epoch [5/20], Step [797/2541], D Loss: 0.6503, G Loss: 2.0866\n",
      "Epoch [5/20], Step [798/2541], D Loss: 0.6505, G Loss: 2.0887\n",
      "Epoch [5/20], Step [799/2541], D Loss: 0.6504, G Loss: 2.0676\n",
      "Epoch [5/20], Step [800/2541], D Loss: 0.6505, G Loss: 2.0884\n",
      "Epoch [5/20], Step [801/2541], D Loss: 0.6503, G Loss: 2.0834\n",
      "Epoch [5/20], Step [802/2541], D Loss: 0.6502, G Loss: 2.0624\n",
      "Epoch [5/20], Step [803/2541], D Loss: 0.6507, G Loss: 2.0915\n",
      "Epoch [5/20], Step [804/2541], D Loss: 0.6502, G Loss: 2.1342\n",
      "Epoch [5/20], Step [805/2541], D Loss: 0.6503, G Loss: 2.0895\n",
      "Epoch [5/20], Step [806/2541], D Loss: 0.6503, G Loss: 2.0487\n",
      "Epoch [5/20], Step [807/2541], D Loss: 0.6504, G Loss: 2.0830\n",
      "Epoch [5/20], Step [808/2541], D Loss: 0.6503, G Loss: 2.0756\n",
      "Epoch [5/20], Step [809/2541], D Loss: 0.6502, G Loss: 2.0580\n",
      "Epoch [5/20], Step [810/2541], D Loss: 0.6502, G Loss: 2.0667\n",
      "Epoch [5/20], Step [811/2541], D Loss: 0.6503, G Loss: 2.0981\n",
      "Epoch [5/20], Step [812/2541], D Loss: 0.6502, G Loss: 2.1035\n",
      "Epoch [5/20], Step [813/2541], D Loss: 0.6503, G Loss: 2.1236\n",
      "Epoch [5/20], Step [814/2541], D Loss: 0.6505, G Loss: 2.1114\n",
      "Epoch [5/20], Step [815/2541], D Loss: 0.6504, G Loss: 2.1088\n",
      "Epoch [5/20], Step [816/2541], D Loss: 0.6503, G Loss: 2.0444\n",
      "Epoch [5/20], Step [817/2541], D Loss: 0.6507, G Loss: 2.0235\n",
      "Epoch [5/20], Step [818/2541], D Loss: 0.6504, G Loss: 2.1395\n",
      "Epoch [5/20], Step [819/2541], D Loss: 0.6504, G Loss: 2.1437\n",
      "Epoch [5/20], Step [820/2541], D Loss: 0.6504, G Loss: 2.1106\n",
      "Epoch [5/20], Step [821/2541], D Loss: 0.6504, G Loss: 2.0648\n",
      "Epoch [5/20], Step [822/2541], D Loss: 0.6504, G Loss: 2.0453\n",
      "Epoch [5/20], Step [823/2541], D Loss: 0.6503, G Loss: 2.0494\n",
      "Epoch [5/20], Step [824/2541], D Loss: 0.6506, G Loss: 2.0929\n",
      "Epoch [5/20], Step [825/2541], D Loss: 0.6503, G Loss: 2.1000\n",
      "Epoch [5/20], Step [826/2541], D Loss: 0.6503, G Loss: 2.0677\n",
      "Epoch [5/20], Step [827/2541], D Loss: 0.6504, G Loss: 2.0816\n",
      "Epoch [5/20], Step [828/2541], D Loss: 0.6504, G Loss: 2.0329\n",
      "Epoch [5/20], Step [829/2541], D Loss: 0.6503, G Loss: 2.0550\n",
      "Epoch [5/20], Step [830/2541], D Loss: 0.6503, G Loss: 2.0699\n",
      "Epoch [5/20], Step [831/2541], D Loss: 0.6504, G Loss: 2.0777\n",
      "Epoch [5/20], Step [832/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [5/20], Step [833/2541], D Loss: 0.6502, G Loss: 2.1154\n",
      "Epoch [5/20], Step [834/2541], D Loss: 0.6503, G Loss: 2.1147\n",
      "Epoch [5/20], Step [835/2541], D Loss: 0.6503, G Loss: 2.0751\n",
      "Epoch [5/20], Step [836/2541], D Loss: 0.6504, G Loss: 2.0558\n",
      "Epoch [5/20], Step [837/2541], D Loss: 0.6504, G Loss: 2.0917\n",
      "Epoch [5/20], Step [838/2541], D Loss: 0.6502, G Loss: 2.1266\n",
      "Epoch [5/20], Step [839/2541], D Loss: 0.6503, G Loss: 2.1113\n",
      "Epoch [5/20], Step [840/2541], D Loss: 0.6506, G Loss: 2.0638\n",
      "Epoch [5/20], Step [841/2541], D Loss: 0.6503, G Loss: 2.0381\n",
      "Epoch [5/20], Step [842/2541], D Loss: 0.6505, G Loss: 2.0993\n",
      "Epoch [5/20], Step [843/2541], D Loss: 0.6508, G Loss: 2.0915\n",
      "Epoch [5/20], Step [844/2541], D Loss: 0.6502, G Loss: 2.1079\n",
      "Epoch [5/20], Step [845/2541], D Loss: 0.6505, G Loss: 2.1227\n",
      "Epoch [5/20], Step [846/2541], D Loss: 0.6506, G Loss: 2.0313\n",
      "Epoch [5/20], Step [847/2541], D Loss: 0.6503, G Loss: 2.0894\n",
      "Epoch [5/20], Step [848/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [5/20], Step [849/2541], D Loss: 0.6503, G Loss: 2.0960\n",
      "Epoch [5/20], Step [850/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [5/20], Step [851/2541], D Loss: 0.6504, G Loss: 2.1266\n",
      "Epoch [5/20], Step [852/2541], D Loss: 0.6505, G Loss: 2.0873\n",
      "Epoch [5/20], Step [853/2541], D Loss: 0.6504, G Loss: 2.0576\n",
      "Epoch [5/20], Step [854/2541], D Loss: 0.6506, G Loss: 2.0643\n",
      "Epoch [5/20], Step [855/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [5/20], Step [856/2541], D Loss: 0.6502, G Loss: 2.0297\n",
      "Epoch [5/20], Step [857/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [5/20], Step [858/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [5/20], Step [859/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [5/20], Step [860/2541], D Loss: 0.6503, G Loss: 2.1018\n",
      "Epoch [5/20], Step [861/2541], D Loss: 0.6502, G Loss: 2.0989\n",
      "Epoch [5/20], Step [862/2541], D Loss: 0.6503, G Loss: 2.0860\n",
      "Epoch [5/20], Step [863/2541], D Loss: 0.6503, G Loss: 2.0357\n",
      "Epoch [5/20], Step [864/2541], D Loss: 0.6503, G Loss: 2.1138\n",
      "Epoch [5/20], Step [865/2541], D Loss: 0.6502, G Loss: 2.1098\n",
      "Epoch [5/20], Step [866/2541], D Loss: 0.6502, G Loss: 2.1272\n",
      "Epoch [5/20], Step [867/2541], D Loss: 0.6502, G Loss: 2.1015\n",
      "Epoch [5/20], Step [868/2541], D Loss: 0.6503, G Loss: 2.1083\n",
      "Epoch [5/20], Step [869/2541], D Loss: 0.6504, G Loss: 2.0791\n",
      "Epoch [5/20], Step [870/2541], D Loss: 0.6503, G Loss: 2.0387\n",
      "Epoch [5/20], Step [871/2541], D Loss: 0.6503, G Loss: 2.1035\n",
      "Epoch [5/20], Step [872/2541], D Loss: 0.6502, G Loss: 2.0660\n",
      "Epoch [5/20], Step [873/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [5/20], Step [874/2541], D Loss: 0.6502, G Loss: 2.0647\n",
      "Epoch [5/20], Step [875/2541], D Loss: 0.6503, G Loss: 2.0682\n",
      "Epoch [5/20], Step [876/2541], D Loss: 0.6504, G Loss: 2.1097\n",
      "Epoch [5/20], Step [877/2541], D Loss: 0.6503, G Loss: 2.0954\n",
      "Epoch [5/20], Step [878/2541], D Loss: 0.6503, G Loss: 2.0854\n",
      "Epoch [5/20], Step [879/2541], D Loss: 0.6503, G Loss: 2.0315\n",
      "Epoch [5/20], Step [880/2541], D Loss: 0.6504, G Loss: 2.0644\n",
      "Epoch [5/20], Step [881/2541], D Loss: 0.6504, G Loss: 2.1004\n",
      "Epoch [5/20], Step [882/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [5/20], Step [883/2541], D Loss: 0.6505, G Loss: 2.0937\n",
      "Epoch [5/20], Step [884/2541], D Loss: 0.6503, G Loss: 2.0930\n",
      "Epoch [5/20], Step [885/2541], D Loss: 0.6503, G Loss: 2.0764\n",
      "Epoch [5/20], Step [886/2541], D Loss: 0.6503, G Loss: 2.0592\n",
      "Epoch [5/20], Step [887/2541], D Loss: 0.6503, G Loss: 2.0390\n",
      "Epoch [5/20], Step [888/2541], D Loss: 0.6502, G Loss: 2.0415\n",
      "Epoch [5/20], Step [889/2541], D Loss: 0.6503, G Loss: 2.1061\n",
      "Epoch [5/20], Step [890/2541], D Loss: 0.6502, G Loss: 2.0961\n",
      "Epoch [5/20], Step [891/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [5/20], Step [892/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [5/20], Step [893/2541], D Loss: 0.6503, G Loss: 2.0903\n",
      "Epoch [5/20], Step [894/2541], D Loss: 0.6502, G Loss: 2.0627\n",
      "Epoch [5/20], Step [895/2541], D Loss: 0.6503, G Loss: 2.0883\n",
      "Epoch [5/20], Step [896/2541], D Loss: 0.6502, G Loss: 2.1021\n",
      "Epoch [5/20], Step [897/2541], D Loss: 0.6503, G Loss: 2.0768\n",
      "Epoch [5/20], Step [898/2541], D Loss: 0.6504, G Loss: 2.0872\n",
      "Epoch [5/20], Step [899/2541], D Loss: 0.6504, G Loss: 2.1000\n",
      "Epoch [5/20], Step [900/2541], D Loss: 0.6503, G Loss: 2.0920\n",
      "Epoch [5/20], Step [901/2541], D Loss: 0.6504, G Loss: 2.0679\n",
      "Epoch [5/20], Step [902/2541], D Loss: 0.6504, G Loss: 2.0728\n",
      "Epoch [5/20], Step [903/2541], D Loss: 0.6503, G Loss: 2.1112\n",
      "Epoch [5/20], Step [904/2541], D Loss: 0.6502, G Loss: 2.0688\n",
      "Epoch [5/20], Step [905/2541], D Loss: 0.6503, G Loss: 2.1223\n",
      "Epoch [5/20], Step [906/2541], D Loss: 0.6503, G Loss: 2.0779\n",
      "Epoch [5/20], Step [907/2541], D Loss: 0.6503, G Loss: 2.0672\n",
      "Epoch [5/20], Step [908/2541], D Loss: 0.6504, G Loss: 2.0992\n",
      "Epoch [5/20], Step [909/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [5/20], Step [910/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [5/20], Step [911/2541], D Loss: 0.6504, G Loss: 2.1255\n",
      "Epoch [5/20], Step [912/2541], D Loss: 0.6503, G Loss: 2.1073\n",
      "Epoch [5/20], Step [913/2541], D Loss: 0.6504, G Loss: 2.0789\n",
      "Epoch [5/20], Step [914/2541], D Loss: 0.6502, G Loss: 2.0755\n",
      "Epoch [5/20], Step [915/2541], D Loss: 0.6503, G Loss: 2.0665\n",
      "Epoch [5/20], Step [916/2541], D Loss: 0.6503, G Loss: 2.1199\n",
      "Epoch [5/20], Step [917/2541], D Loss: 0.6502, G Loss: 2.1082\n",
      "Epoch [5/20], Step [918/2541], D Loss: 0.6503, G Loss: 2.1117\n",
      "Epoch [5/20], Step [919/2541], D Loss: 0.6503, G Loss: 2.1396\n",
      "Epoch [5/20], Step [920/2541], D Loss: 0.6503, G Loss: 2.1070\n",
      "Epoch [5/20], Step [921/2541], D Loss: 0.6502, G Loss: 2.0993\n",
      "Epoch [5/20], Step [922/2541], D Loss: 0.6504, G Loss: 2.0995\n",
      "Epoch [5/20], Step [923/2541], D Loss: 0.6502, G Loss: 2.0319\n",
      "Epoch [5/20], Step [924/2541], D Loss: 0.6502, G Loss: 2.0952\n",
      "Epoch [5/20], Step [925/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [5/20], Step [926/2541], D Loss: 0.6502, G Loss: 2.1063\n",
      "Epoch [5/20], Step [927/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [5/20], Step [928/2541], D Loss: 0.6502, G Loss: 2.1090\n",
      "Epoch [5/20], Step [929/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [5/20], Step [930/2541], D Loss: 0.6502, G Loss: 2.0575\n",
      "Epoch [5/20], Step [931/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [5/20], Step [932/2541], D Loss: 0.6502, G Loss: 2.0594\n",
      "Epoch [5/20], Step [933/2541], D Loss: 0.6502, G Loss: 2.0674\n",
      "Epoch [5/20], Step [934/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [5/20], Step [935/2541], D Loss: 0.6505, G Loss: 2.0441\n",
      "Epoch [5/20], Step [936/2541], D Loss: 0.6505, G Loss: 2.0549\n",
      "Epoch [5/20], Step [937/2541], D Loss: 0.6502, G Loss: 2.0524\n",
      "Epoch [5/20], Step [938/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [5/20], Step [939/2541], D Loss: 0.6504, G Loss: 2.0991\n",
      "Epoch [5/20], Step [940/2541], D Loss: 0.6502, G Loss: 2.1590\n",
      "Epoch [5/20], Step [941/2541], D Loss: 0.6502, G Loss: 2.0338\n",
      "Epoch [5/20], Step [942/2541], D Loss: 0.6503, G Loss: 2.1060\n",
      "Epoch [5/20], Step [943/2541], D Loss: 0.6503, G Loss: 2.1031\n",
      "Epoch [5/20], Step [944/2541], D Loss: 0.6505, G Loss: 2.0785\n",
      "Epoch [5/20], Step [945/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [5/20], Step [946/2541], D Loss: 0.6504, G Loss: 2.1399\n",
      "Epoch [5/20], Step [947/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [5/20], Step [948/2541], D Loss: 0.6503, G Loss: 2.1199\n",
      "Epoch [5/20], Step [949/2541], D Loss: 0.6505, G Loss: 2.0542\n",
      "Epoch [5/20], Step [950/2541], D Loss: 0.6503, G Loss: 2.0465\n",
      "Epoch [5/20], Step [951/2541], D Loss: 0.6503, G Loss: 2.0606\n",
      "Epoch [5/20], Step [952/2541], D Loss: 0.6503, G Loss: 2.0771\n",
      "Epoch [5/20], Step [953/2541], D Loss: 0.6503, G Loss: 2.1060\n",
      "Epoch [5/20], Step [954/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [5/20], Step [955/2541], D Loss: 0.6503, G Loss: 2.0745\n",
      "Epoch [5/20], Step [956/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [5/20], Step [957/2541], D Loss: 0.6531, G Loss: 2.0576\n",
      "Epoch [5/20], Step [958/2541], D Loss: 0.6507, G Loss: 2.0678\n",
      "Epoch [5/20], Step [959/2541], D Loss: 0.6503, G Loss: 2.0621\n",
      "Epoch [5/20], Step [960/2541], D Loss: 0.6509, G Loss: 2.1288\n",
      "Epoch [5/20], Step [961/2541], D Loss: 0.6507, G Loss: 2.0944\n",
      "Epoch [5/20], Step [962/2541], D Loss: 0.6504, G Loss: 2.0990\n",
      "Epoch [5/20], Step [963/2541], D Loss: 0.6507, G Loss: 2.0717\n",
      "Epoch [5/20], Step [964/2541], D Loss: 0.6503, G Loss: 2.0368\n",
      "Epoch [5/20], Step [965/2541], D Loss: 0.6504, G Loss: 2.0499\n",
      "Epoch [5/20], Step [966/2541], D Loss: 0.6503, G Loss: 2.0995\n",
      "Epoch [5/20], Step [967/2541], D Loss: 0.6502, G Loss: 2.0958\n",
      "Epoch [5/20], Step [968/2541], D Loss: 0.6503, G Loss: 2.0809\n",
      "Epoch [5/20], Step [969/2541], D Loss: 0.6503, G Loss: 2.1034\n",
      "Epoch [5/20], Step [970/2541], D Loss: 0.6502, G Loss: 2.1016\n",
      "Epoch [5/20], Step [971/2541], D Loss: 0.6502, G Loss: 2.0898\n",
      "Epoch [5/20], Step [972/2541], D Loss: 0.6502, G Loss: 2.1133\n",
      "Epoch [5/20], Step [973/2541], D Loss: 0.6502, G Loss: 2.0712\n",
      "Epoch [5/20], Step [974/2541], D Loss: 0.6502, G Loss: 2.1251\n",
      "Epoch [5/20], Step [975/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [5/20], Step [976/2541], D Loss: 0.6504, G Loss: 2.1110\n",
      "Epoch [5/20], Step [977/2541], D Loss: 0.6504, G Loss: 2.0767\n",
      "Epoch [5/20], Step [978/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [5/20], Step [979/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [5/20], Step [980/2541], D Loss: 0.6504, G Loss: 2.1098\n",
      "Epoch [5/20], Step [981/2541], D Loss: 0.6503, G Loss: 2.0592\n",
      "Epoch [5/20], Step [982/2541], D Loss: 0.6503, G Loss: 2.0274\n",
      "Epoch [5/20], Step [983/2541], D Loss: 0.6504, G Loss: 2.0743\n",
      "Epoch [5/20], Step [984/2541], D Loss: 0.6503, G Loss: 2.1084\n",
      "Epoch [5/20], Step [985/2541], D Loss: 0.6503, G Loss: 2.1382\n",
      "Epoch [5/20], Step [986/2541], D Loss: 0.6504, G Loss: 2.0921\n",
      "Epoch [5/20], Step [987/2541], D Loss: 0.6503, G Loss: 2.0745\n",
      "Epoch [5/20], Step [988/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [5/20], Step [989/2541], D Loss: 0.6510, G Loss: 2.0576\n",
      "Epoch [5/20], Step [990/2541], D Loss: 0.6507, G Loss: 2.0899\n",
      "Epoch [5/20], Step [991/2541], D Loss: 0.6504, G Loss: 2.0991\n",
      "Epoch [5/20], Step [992/2541], D Loss: 0.6504, G Loss: 2.1045\n",
      "Epoch [5/20], Step [993/2541], D Loss: 0.6504, G Loss: 2.0925\n",
      "Epoch [5/20], Step [994/2541], D Loss: 0.6503, G Loss: 2.0999\n",
      "Epoch [5/20], Step [995/2541], D Loss: 0.6502, G Loss: 2.0961\n",
      "Epoch [5/20], Step [996/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [5/20], Step [997/2541], D Loss: 0.6503, G Loss: 2.0999\n",
      "Epoch [5/20], Step [998/2541], D Loss: 0.6503, G Loss: 2.0892\n",
      "Epoch [5/20], Step [999/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [5/20], Step [1000/2541], D Loss: 0.6502, G Loss: 2.0621\n",
      "Epoch [5/20], Step [1001/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [5/20], Step [1002/2541], D Loss: 0.6503, G Loss: 2.0933\n",
      "Epoch [5/20], Step [1003/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [5/20], Step [1004/2541], D Loss: 0.6503, G Loss: 2.0778\n",
      "Epoch [5/20], Step [1005/2541], D Loss: 0.6505, G Loss: 2.1069\n",
      "Epoch [5/20], Step [1006/2541], D Loss: 0.6502, G Loss: 2.1190\n",
      "Epoch [5/20], Step [1007/2541], D Loss: 0.6503, G Loss: 2.0915\n",
      "Epoch [5/20], Step [1008/2541], D Loss: 0.6504, G Loss: 2.0689\n",
      "Epoch [5/20], Step [1009/2541], D Loss: 0.6504, G Loss: 2.0722\n",
      "Epoch [5/20], Step [1010/2541], D Loss: 0.6502, G Loss: 2.0923\n",
      "Epoch [5/20], Step [1011/2541], D Loss: 0.6502, G Loss: 2.0993\n",
      "Epoch [5/20], Step [1012/2541], D Loss: 0.6502, G Loss: 2.0653\n",
      "Epoch [5/20], Step [1013/2541], D Loss: 0.6503, G Loss: 2.0997\n",
      "Epoch [5/20], Step [1014/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [5/20], Step [1015/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [5/20], Step [1016/2541], D Loss: 0.6503, G Loss: 2.0936\n",
      "Epoch [5/20], Step [1017/2541], D Loss: 0.6503, G Loss: 2.1306\n",
      "Epoch [5/20], Step [1018/2541], D Loss: 0.6502, G Loss: 2.1023\n",
      "Epoch [5/20], Step [1019/2541], D Loss: 0.6502, G Loss: 2.1121\n",
      "Epoch [5/20], Step [1020/2541], D Loss: 0.6502, G Loss: 2.1004\n",
      "Epoch [5/20], Step [1021/2541], D Loss: 0.6502, G Loss: 2.1139\n",
      "Epoch [5/20], Step [1022/2541], D Loss: 0.6503, G Loss: 2.0839\n",
      "Epoch [5/20], Step [1023/2541], D Loss: 0.6503, G Loss: 2.0601\n",
      "Epoch [5/20], Step [1024/2541], D Loss: 0.6503, G Loss: 2.0662\n",
      "Epoch [5/20], Step [1025/2541], D Loss: 0.6504, G Loss: 2.0779\n",
      "Epoch [5/20], Step [1026/2541], D Loss: 0.6503, G Loss: 2.0750\n",
      "Epoch [5/20], Step [1027/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [5/20], Step [1028/2541], D Loss: 0.6502, G Loss: 2.0743\n",
      "Epoch [5/20], Step [1029/2541], D Loss: 0.6502, G Loss: 2.0611\n",
      "Epoch [5/20], Step [1030/2541], D Loss: 0.6502, G Loss: 2.0594\n",
      "Epoch [5/20], Step [1031/2541], D Loss: 0.6505, G Loss: 2.1063\n",
      "Epoch [5/20], Step [1032/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [5/20], Step [1033/2541], D Loss: 0.6502, G Loss: 2.0671\n",
      "Epoch [5/20], Step [1034/2541], D Loss: 0.6504, G Loss: 2.0854\n",
      "Epoch [5/20], Step [1035/2541], D Loss: 0.6502, G Loss: 2.1066\n",
      "Epoch [5/20], Step [1036/2541], D Loss: 0.6503, G Loss: 2.0994\n",
      "Epoch [5/20], Step [1037/2541], D Loss: 0.6503, G Loss: 2.0742\n",
      "Epoch [5/20], Step [1038/2541], D Loss: 0.6502, G Loss: 2.1014\n",
      "Epoch [5/20], Step [1039/2541], D Loss: 0.6502, G Loss: 2.1167\n",
      "Epoch [5/20], Step [1040/2541], D Loss: 0.6503, G Loss: 2.0858\n",
      "Epoch [5/20], Step [1041/2541], D Loss: 0.6503, G Loss: 2.0845\n",
      "Epoch [5/20], Step [1042/2541], D Loss: 0.6504, G Loss: 2.0936\n",
      "Epoch [5/20], Step [1043/2541], D Loss: 0.6503, G Loss: 2.0869\n",
      "Epoch [5/20], Step [1044/2541], D Loss: 0.6503, G Loss: 2.0709\n",
      "Epoch [5/20], Step [1045/2541], D Loss: 0.6522, G Loss: 2.0400\n",
      "Epoch [5/20], Step [1046/2541], D Loss: 0.6514, G Loss: 2.0700\n",
      "Epoch [5/20], Step [1047/2541], D Loss: 0.6509, G Loss: 2.0802\n",
      "Epoch [5/20], Step [1048/2541], D Loss: 0.6505, G Loss: 2.0740\n",
      "Epoch [5/20], Step [1049/2541], D Loss: 0.6503, G Loss: 2.0876\n",
      "Epoch [5/20], Step [1050/2541], D Loss: 0.6503, G Loss: 2.1058\n",
      "Epoch [5/20], Step [1051/2541], D Loss: 0.6505, G Loss: 2.1053\n",
      "Epoch [5/20], Step [1052/2541], D Loss: 0.6506, G Loss: 2.0937\n",
      "Epoch [5/20], Step [1053/2541], D Loss: 0.6516, G Loss: 2.0976\n",
      "Epoch [5/20], Step [1054/2541], D Loss: 0.6506, G Loss: 2.0914\n",
      "Epoch [5/20], Step [1055/2541], D Loss: 0.6506, G Loss: 2.0894\n",
      "Epoch [5/20], Step [1056/2541], D Loss: 0.6504, G Loss: 2.0848\n",
      "Epoch [5/20], Step [1057/2541], D Loss: 0.6507, G Loss: 2.0842\n",
      "Epoch [5/20], Step [1058/2541], D Loss: 0.6505, G Loss: 2.1037\n",
      "Epoch [5/20], Step [1059/2541], D Loss: 0.6504, G Loss: 2.0804\n",
      "Epoch [5/20], Step [1060/2541], D Loss: 0.6504, G Loss: 2.0807\n",
      "Epoch [5/20], Step [1061/2541], D Loss: 0.6504, G Loss: 2.0670\n",
      "Epoch [5/20], Step [1062/2541], D Loss: 0.6503, G Loss: 2.1022\n",
      "Epoch [5/20], Step [1063/2541], D Loss: 0.6503, G Loss: 2.1014\n",
      "Epoch [5/20], Step [1064/2541], D Loss: 0.6503, G Loss: 2.1433\n",
      "Epoch [5/20], Step [1065/2541], D Loss: 0.6504, G Loss: 2.1277\n",
      "Epoch [5/20], Step [1066/2541], D Loss: 0.6506, G Loss: 2.0999\n",
      "Epoch [5/20], Step [1067/2541], D Loss: 0.6504, G Loss: 2.0440\n",
      "Epoch [5/20], Step [1068/2541], D Loss: 0.6504, G Loss: 2.0882\n",
      "Epoch [5/20], Step [1069/2541], D Loss: 0.6504, G Loss: 2.1131\n",
      "Epoch [5/20], Step [1070/2541], D Loss: 0.6504, G Loss: 2.1061\n",
      "Epoch [5/20], Step [1071/2541], D Loss: 0.6504, G Loss: 2.0794\n",
      "Epoch [5/20], Step [1072/2541], D Loss: 0.6502, G Loss: 2.0440\n",
      "Epoch [5/20], Step [1073/2541], D Loss: 0.6503, G Loss: 2.1022\n",
      "Epoch [5/20], Step [1074/2541], D Loss: 0.6503, G Loss: 2.1053\n",
      "Epoch [5/20], Step [1075/2541], D Loss: 0.6504, G Loss: 2.0583\n",
      "Epoch [5/20], Step [1076/2541], D Loss: 0.6504, G Loss: 2.0842\n",
      "Epoch [5/20], Step [1077/2541], D Loss: 0.6502, G Loss: 2.1065\n",
      "Epoch [5/20], Step [1078/2541], D Loss: 0.6502, G Loss: 2.0670\n",
      "Epoch [5/20], Step [1079/2541], D Loss: 0.6503, G Loss: 2.0850\n",
      "Epoch [5/20], Step [1080/2541], D Loss: 0.6503, G Loss: 2.0655\n",
      "Epoch [5/20], Step [1081/2541], D Loss: 0.6505, G Loss: 2.0806\n",
      "Epoch [5/20], Step [1082/2541], D Loss: 0.6503, G Loss: 2.0904\n",
      "Epoch [5/20], Step [1083/2541], D Loss: 0.6502, G Loss: 2.1127\n",
      "Epoch [5/20], Step [1084/2541], D Loss: 0.6502, G Loss: 2.1066\n",
      "Epoch [5/20], Step [1085/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [5/20], Step [1086/2541], D Loss: 0.6502, G Loss: 2.1047\n",
      "Epoch [5/20], Step [1087/2541], D Loss: 0.6503, G Loss: 2.0322\n",
      "Epoch [5/20], Step [1088/2541], D Loss: 0.6503, G Loss: 2.0566\n",
      "Epoch [5/20], Step [1089/2541], D Loss: 0.6502, G Loss: 2.0630\n",
      "Epoch [5/20], Step [1090/2541], D Loss: 0.6503, G Loss: 2.1057\n",
      "Epoch [5/20], Step [1091/2541], D Loss: 0.6512, G Loss: 2.0968\n",
      "Epoch [5/20], Step [1092/2541], D Loss: 0.6505, G Loss: 2.0944\n",
      "Epoch [5/20], Step [1093/2541], D Loss: 0.6507, G Loss: 2.0746\n",
      "Epoch [5/20], Step [1094/2541], D Loss: 0.6504, G Loss: 2.0761\n",
      "Epoch [5/20], Step [1095/2541], D Loss: 0.6504, G Loss: 2.0777\n",
      "Epoch [5/20], Step [1096/2541], D Loss: 0.6502, G Loss: 2.0910\n",
      "Epoch [5/20], Step [1097/2541], D Loss: 0.6503, G Loss: 2.0892\n",
      "Epoch [5/20], Step [1098/2541], D Loss: 0.6502, G Loss: 2.0701\n",
      "Epoch [5/20], Step [1099/2541], D Loss: 0.6503, G Loss: 2.0932\n",
      "Epoch [5/20], Step [1100/2541], D Loss: 0.6503, G Loss: 2.1188\n",
      "Epoch [5/20], Step [1101/2541], D Loss: 0.6513, G Loss: 2.0997\n",
      "Epoch [5/20], Step [1102/2541], D Loss: 0.6508, G Loss: 2.0822\n",
      "Epoch [5/20], Step [1103/2541], D Loss: 0.6504, G Loss: 2.0130\n",
      "Epoch [5/20], Step [1104/2541], D Loss: 0.6502, G Loss: 2.0704\n",
      "Epoch [5/20], Step [1105/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [5/20], Step [1106/2541], D Loss: 0.6503, G Loss: 2.0793\n",
      "Epoch [5/20], Step [1107/2541], D Loss: 0.6503, G Loss: 2.0495\n",
      "Epoch [5/20], Step [1108/2541], D Loss: 0.6502, G Loss: 2.0494\n",
      "Epoch [5/20], Step [1109/2541], D Loss: 0.6506, G Loss: 2.0978\n",
      "Epoch [5/20], Step [1110/2541], D Loss: 0.6506, G Loss: 2.0776\n",
      "Epoch [5/20], Step [1111/2541], D Loss: 0.6506, G Loss: 2.1199\n",
      "Epoch [5/20], Step [1112/2541], D Loss: 0.6504, G Loss: 2.0988\n",
      "Epoch [5/20], Step [1113/2541], D Loss: 0.6504, G Loss: 2.0870\n",
      "Epoch [5/20], Step [1114/2541], D Loss: 0.6503, G Loss: 2.0690\n",
      "Epoch [5/20], Step [1115/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [5/20], Step [1116/2541], D Loss: 0.6502, G Loss: 2.0940\n",
      "Epoch [5/20], Step [1117/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [5/20], Step [1118/2541], D Loss: 0.6506, G Loss: 2.0562\n",
      "Epoch [5/20], Step [1119/2541], D Loss: 0.6503, G Loss: 2.1108\n",
      "Epoch [5/20], Step [1120/2541], D Loss: 0.6503, G Loss: 2.0951\n",
      "Epoch [5/20], Step [1121/2541], D Loss: 0.6504, G Loss: 2.0834\n",
      "Epoch [5/20], Step [1122/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [5/20], Step [1123/2541], D Loss: 0.6502, G Loss: 2.0660\n",
      "Epoch [5/20], Step [1124/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [5/20], Step [1125/2541], D Loss: 0.6506, G Loss: 2.0991\n",
      "Epoch [5/20], Step [1126/2541], D Loss: 0.6503, G Loss: 2.0849\n",
      "Epoch [5/20], Step [1127/2541], D Loss: 0.6503, G Loss: 2.0679\n",
      "Epoch [5/20], Step [1128/2541], D Loss: 0.6503, G Loss: 2.0854\n",
      "Epoch [5/20], Step [1129/2541], D Loss: 0.6505, G Loss: 2.0587\n",
      "Epoch [5/20], Step [1130/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [5/20], Step [1131/2541], D Loss: 0.6503, G Loss: 2.0640\n",
      "Epoch [5/20], Step [1132/2541], D Loss: 0.6502, G Loss: 2.0516\n",
      "Epoch [5/20], Step [1133/2541], D Loss: 0.6503, G Loss: 2.1016\n",
      "Epoch [5/20], Step [1134/2541], D Loss: 0.6502, G Loss: 2.0948\n",
      "Epoch [5/20], Step [1135/2541], D Loss: 0.6502, G Loss: 2.0978\n",
      "Epoch [5/20], Step [1136/2541], D Loss: 0.6503, G Loss: 2.1132\n",
      "Epoch [5/20], Step [1137/2541], D Loss: 0.6509, G Loss: 2.0744\n",
      "Epoch [5/20], Step [1138/2541], D Loss: 0.6505, G Loss: 2.0590\n",
      "Epoch [5/20], Step [1139/2541], D Loss: 0.6505, G Loss: 2.0618\n",
      "Epoch [5/20], Step [1140/2541], D Loss: 0.6502, G Loss: 2.0519\n",
      "Epoch [5/20], Step [1141/2541], D Loss: 0.6504, G Loss: 2.0891\n",
      "Epoch [5/20], Step [1142/2541], D Loss: 0.6504, G Loss: 2.1256\n",
      "Epoch [5/20], Step [1143/2541], D Loss: 0.6503, G Loss: 2.1259\n",
      "Epoch [5/20], Step [1144/2541], D Loss: 0.6512, G Loss: 2.0864\n",
      "Epoch [5/20], Step [1145/2541], D Loss: 0.6506, G Loss: 2.1051\n",
      "Epoch [5/20], Step [1146/2541], D Loss: 0.6506, G Loss: 2.0697\n",
      "Epoch [5/20], Step [1147/2541], D Loss: 0.6504, G Loss: 2.0871\n",
      "Epoch [5/20], Step [1148/2541], D Loss: 0.6503, G Loss: 2.0842\n",
      "Epoch [5/20], Step [1149/2541], D Loss: 0.6503, G Loss: 2.0911\n",
      "Epoch [5/20], Step [1150/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [5/20], Step [1151/2541], D Loss: 0.6502, G Loss: 2.0763\n",
      "Epoch [5/20], Step [1152/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [5/20], Step [1153/2541], D Loss: 0.6502, G Loss: 2.1048\n",
      "Epoch [5/20], Step [1154/2541], D Loss: 0.6502, G Loss: 2.1062\n",
      "Epoch [5/20], Step [1155/2541], D Loss: 0.6503, G Loss: 2.0878\n",
      "Epoch [5/20], Step [1156/2541], D Loss: 0.6503, G Loss: 2.0632\n",
      "Epoch [5/20], Step [1157/2541], D Loss: 0.6502, G Loss: 2.0560\n",
      "Epoch [5/20], Step [1158/2541], D Loss: 0.6504, G Loss: 2.0631\n",
      "Epoch [5/20], Step [1159/2541], D Loss: 0.6502, G Loss: 2.0954\n",
      "Epoch [5/20], Step [1160/2541], D Loss: 0.6503, G Loss: 2.1008\n",
      "Epoch [5/20], Step [1161/2541], D Loss: 0.6505, G Loss: 2.0882\n",
      "Epoch [5/20], Step [1162/2541], D Loss: 0.6503, G Loss: 2.0685\n",
      "Epoch [5/20], Step [1163/2541], D Loss: 0.6502, G Loss: 2.0714\n",
      "Epoch [5/20], Step [1164/2541], D Loss: 0.6502, G Loss: 2.1072\n",
      "Epoch [5/20], Step [1165/2541], D Loss: 0.6503, G Loss: 2.1024\n",
      "Epoch [5/20], Step [1166/2541], D Loss: 0.6506, G Loss: 2.0646\n",
      "Epoch [5/20], Step [1167/2541], D Loss: 0.6502, G Loss: 2.0662\n",
      "Epoch [5/20], Step [1168/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [5/20], Step [1169/2541], D Loss: 0.6506, G Loss: 2.0857\n",
      "Epoch [5/20], Step [1170/2541], D Loss: 0.6503, G Loss: 2.0619\n",
      "Epoch [5/20], Step [1171/2541], D Loss: 0.6502, G Loss: 2.0674\n",
      "Epoch [5/20], Step [1172/2541], D Loss: 0.6503, G Loss: 2.0819\n",
      "Epoch [5/20], Step [1173/2541], D Loss: 0.6502, G Loss: 2.0909\n",
      "Epoch [5/20], Step [1174/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [5/20], Step [1175/2541], D Loss: 0.6505, G Loss: 2.0921\n",
      "Epoch [5/20], Step [1176/2541], D Loss: 0.6502, G Loss: 2.1016\n",
      "Epoch [5/20], Step [1177/2541], D Loss: 0.6504, G Loss: 2.1084\n",
      "Epoch [5/20], Step [1178/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [5/20], Step [1179/2541], D Loss: 0.6503, G Loss: 2.0941\n",
      "Epoch [5/20], Step [1180/2541], D Loss: 0.6502, G Loss: 2.0578\n",
      "Epoch [5/20], Step [1181/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [5/20], Step [1182/2541], D Loss: 0.6502, G Loss: 2.0721\n",
      "Epoch [5/20], Step [1183/2541], D Loss: 0.6503, G Loss: 2.0893\n",
      "Epoch [5/20], Step [1184/2541], D Loss: 0.6505, G Loss: 2.0869\n",
      "Epoch [5/20], Step [1185/2541], D Loss: 0.6504, G Loss: 2.0530\n",
      "Epoch [5/20], Step [1186/2541], D Loss: 0.6504, G Loss: 2.0582\n",
      "Epoch [5/20], Step [1187/2541], D Loss: 0.6503, G Loss: 2.0955\n",
      "Epoch [5/20], Step [1188/2541], D Loss: 0.6502, G Loss: 2.1188\n",
      "Epoch [5/20], Step [1189/2541], D Loss: 0.6503, G Loss: 2.1019\n",
      "Epoch [5/20], Step [1190/2541], D Loss: 0.6503, G Loss: 2.0716\n",
      "Epoch [5/20], Step [1191/2541], D Loss: 0.6505, G Loss: 2.0857\n",
      "Epoch [5/20], Step [1192/2541], D Loss: 0.6505, G Loss: 2.1168\n",
      "Epoch [5/20], Step [1193/2541], D Loss: 0.6505, G Loss: 2.0795\n",
      "Epoch [5/20], Step [1194/2541], D Loss: 0.6502, G Loss: 2.0614\n",
      "Epoch [5/20], Step [1195/2541], D Loss: 0.6503, G Loss: 2.0668\n",
      "Epoch [5/20], Step [1196/2541], D Loss: 0.6503, G Loss: 2.0799\n",
      "Epoch [5/20], Step [1197/2541], D Loss: 0.6502, G Loss: 2.0990\n",
      "Epoch [5/20], Step [1198/2541], D Loss: 0.6502, G Loss: 2.1057\n",
      "Epoch [5/20], Step [1199/2541], D Loss: 0.6503, G Loss: 2.0723\n",
      "Epoch [5/20], Step [1200/2541], D Loss: 0.6503, G Loss: 2.0566\n",
      "Epoch [5/20], Step [1201/2541], D Loss: 0.6506, G Loss: 2.1134\n",
      "Epoch [5/20], Step [1202/2541], D Loss: 0.6516, G Loss: 2.1238\n",
      "Epoch [5/20], Step [1203/2541], D Loss: 0.6506, G Loss: 2.0601\n",
      "Epoch [5/20], Step [1204/2541], D Loss: 0.6504, G Loss: 2.0653\n",
      "Epoch [5/20], Step [1205/2541], D Loss: 0.6505, G Loss: 2.0834\n",
      "Epoch [5/20], Step [1206/2541], D Loss: 0.6503, G Loss: 2.0442\n",
      "Epoch [5/20], Step [1207/2541], D Loss: 0.6502, G Loss: 2.0985\n",
      "Epoch [5/20], Step [1208/2541], D Loss: 0.6503, G Loss: 2.0785\n",
      "Epoch [5/20], Step [1209/2541], D Loss: 0.6503, G Loss: 2.0681\n",
      "Epoch [5/20], Step [1210/2541], D Loss: 0.6503, G Loss: 2.0701\n",
      "Epoch [5/20], Step [1211/2541], D Loss: 0.6504, G Loss: 2.0813\n",
      "Epoch [5/20], Step [1212/2541], D Loss: 0.6502, G Loss: 2.0948\n",
      "Epoch [5/20], Step [1213/2541], D Loss: 0.6502, G Loss: 2.0284\n",
      "Epoch [5/20], Step [1214/2541], D Loss: 0.6503, G Loss: 2.0517\n",
      "Epoch [5/20], Step [1215/2541], D Loss: 0.6502, G Loss: 2.0712\n",
      "Epoch [5/20], Step [1216/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [5/20], Step [1217/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [5/20], Step [1218/2541], D Loss: 0.6502, G Loss: 2.0561\n",
      "Epoch [5/20], Step [1219/2541], D Loss: 0.6502, G Loss: 2.1228\n",
      "Epoch [5/20], Step [1220/2541], D Loss: 0.6502, G Loss: 2.1109\n",
      "Epoch [5/20], Step [1221/2541], D Loss: 0.6503, G Loss: 2.0974\n",
      "Epoch [5/20], Step [1222/2541], D Loss: 0.6503, G Loss: 2.0711\n",
      "Epoch [5/20], Step [1223/2541], D Loss: 0.6503, G Loss: 2.0640\n",
      "Epoch [5/20], Step [1224/2541], D Loss: 0.6503, G Loss: 2.0739\n",
      "Epoch [5/20], Step [1225/2541], D Loss: 0.6509, G Loss: 2.0904\n",
      "Epoch [5/20], Step [1226/2541], D Loss: 0.6508, G Loss: 2.0875\n",
      "Epoch [5/20], Step [1227/2541], D Loss: 0.6505, G Loss: 2.0998\n",
      "Epoch [5/20], Step [1228/2541], D Loss: 0.6504, G Loss: 2.0715\n",
      "Epoch [5/20], Step [1229/2541], D Loss: 0.6505, G Loss: 2.0963\n",
      "Epoch [5/20], Step [1230/2541], D Loss: 0.6503, G Loss: 2.0562\n",
      "Epoch [5/20], Step [1231/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [5/20], Step [1232/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [5/20], Step [1233/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [5/20], Step [1234/2541], D Loss: 0.6504, G Loss: 2.1024\n",
      "Epoch [5/20], Step [1235/2541], D Loss: 0.6503, G Loss: 2.0719\n",
      "Epoch [5/20], Step [1236/2541], D Loss: 0.6504, G Loss: 2.0857\n",
      "Epoch [5/20], Step [1237/2541], D Loss: 0.6504, G Loss: 2.1005\n",
      "Epoch [5/20], Step [1238/2541], D Loss: 0.6504, G Loss: 2.0574\n",
      "Epoch [5/20], Step [1239/2541], D Loss: 0.6504, G Loss: 2.0963\n",
      "Epoch [5/20], Step [1240/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [5/20], Step [1241/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [5/20], Step [1242/2541], D Loss: 0.6502, G Loss: 2.1112\n",
      "Epoch [5/20], Step [1243/2541], D Loss: 0.6503, G Loss: 2.0607\n",
      "Epoch [5/20], Step [1244/2541], D Loss: 0.6502, G Loss: 2.0709\n",
      "Epoch [5/20], Step [1245/2541], D Loss: 0.6502, G Loss: 2.0468\n",
      "Epoch [5/20], Step [1246/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [5/20], Step [1247/2541], D Loss: 0.6503, G Loss: 2.0643\n",
      "Epoch [5/20], Step [1248/2541], D Loss: 0.6502, G Loss: 2.1076\n",
      "Epoch [5/20], Step [1249/2541], D Loss: 0.6503, G Loss: 2.0572\n",
      "Epoch [5/20], Step [1250/2541], D Loss: 0.6502, G Loss: 2.1118\n",
      "Epoch [5/20], Step [1251/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [5/20], Step [1252/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [5/20], Step [1253/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [5/20], Step [1254/2541], D Loss: 0.6503, G Loss: 2.0885\n",
      "Epoch [5/20], Step [1255/2541], D Loss: 0.6504, G Loss: 2.0833\n",
      "Epoch [5/20], Step [1256/2541], D Loss: 0.6504, G Loss: 2.1115\n",
      "Epoch [5/20], Step [1257/2541], D Loss: 0.6503, G Loss: 2.0779\n",
      "Epoch [5/20], Step [1258/2541], D Loss: 0.6502, G Loss: 2.0652\n",
      "Epoch [5/20], Step [1259/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [5/20], Step [1260/2541], D Loss: 0.6503, G Loss: 2.0558\n",
      "Epoch [5/20], Step [1261/2541], D Loss: 0.6503, G Loss: 2.0532\n",
      "Epoch [5/20], Step [1262/2541], D Loss: 0.6503, G Loss: 2.0838\n",
      "Epoch [5/20], Step [1263/2541], D Loss: 0.6503, G Loss: 2.0716\n",
      "Epoch [5/20], Step [1264/2541], D Loss: 0.6503, G Loss: 2.0415\n",
      "Epoch [5/20], Step [1265/2541], D Loss: 0.6503, G Loss: 2.0773\n",
      "Epoch [5/20], Step [1266/2541], D Loss: 0.6502, G Loss: 2.1068\n",
      "Epoch [5/20], Step [1267/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [5/20], Step [1268/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [5/20], Step [1269/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [5/20], Step [1270/2541], D Loss: 0.6502, G Loss: 2.0615\n",
      "Epoch [5/20], Step [1271/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [5/20], Step [1272/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [5/20], Step [1273/2541], D Loss: 0.6503, G Loss: 2.3171\n",
      "Epoch [5/20], Step [1274/2541], D Loss: 0.6503, G Loss: 2.0773\n",
      "Epoch [5/20], Step [1275/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [5/20], Step [1276/2541], D Loss: 0.6502, G Loss: 2.0591\n",
      "Epoch [5/20], Step [1277/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [5/20], Step [1278/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [5/20], Step [1279/2541], D Loss: 0.6503, G Loss: 2.0853\n",
      "Epoch [5/20], Step [1280/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [5/20], Step [1281/2541], D Loss: 0.6503, G Loss: 2.0904\n",
      "Epoch [5/20], Step [1282/2541], D Loss: 0.6502, G Loss: 2.0424\n",
      "Epoch [5/20], Step [1283/2541], D Loss: 0.6503, G Loss: 2.0678\n",
      "Epoch [5/20], Step [1284/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [5/20], Step [1285/2541], D Loss: 0.6502, G Loss: 2.0440\n",
      "Epoch [5/20], Step [1286/2541], D Loss: 0.6502, G Loss: 2.0459\n",
      "Epoch [5/20], Step [1287/2541], D Loss: 0.6503, G Loss: 2.0599\n",
      "Epoch [5/20], Step [1288/2541], D Loss: 0.6503, G Loss: 2.0764\n",
      "Epoch [5/20], Step [1289/2541], D Loss: 0.6503, G Loss: 2.1033\n",
      "Epoch [5/20], Step [1290/2541], D Loss: 0.6504, G Loss: 2.0657\n",
      "Epoch [5/20], Step [1291/2541], D Loss: 0.6502, G Loss: 2.0675\n",
      "Epoch [5/20], Step [1292/2541], D Loss: 0.6502, G Loss: 2.0587\n",
      "Epoch [5/20], Step [1293/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [5/20], Step [1294/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [5/20], Step [1295/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [5/20], Step [1296/2541], D Loss: 0.6502, G Loss: 2.0974\n",
      "Epoch [5/20], Step [1297/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [5/20], Step [1298/2541], D Loss: 0.6502, G Loss: 2.0983\n",
      "Epoch [5/20], Step [1299/2541], D Loss: 0.6502, G Loss: 2.0637\n",
      "Epoch [5/20], Step [1300/2541], D Loss: 0.6504, G Loss: 2.1121\n",
      "Epoch [5/20], Step [1301/2541], D Loss: 0.6503, G Loss: 2.1212\n",
      "Epoch [5/20], Step [1302/2541], D Loss: 0.6503, G Loss: 2.1032\n",
      "Epoch [5/20], Step [1303/2541], D Loss: 0.6505, G Loss: 2.0956\n",
      "Epoch [5/20], Step [1304/2541], D Loss: 0.6503, G Loss: 2.0756\n",
      "Epoch [5/20], Step [1305/2541], D Loss: 0.6504, G Loss: 2.0512\n",
      "Epoch [5/20], Step [1306/2541], D Loss: 0.6506, G Loss: 2.1049\n",
      "Epoch [5/20], Step [1307/2541], D Loss: 0.6504, G Loss: 2.0737\n",
      "Epoch [5/20], Step [1308/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [5/20], Step [1309/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [5/20], Step [1310/2541], D Loss: 0.6502, G Loss: 2.0662\n",
      "Epoch [5/20], Step [1311/2541], D Loss: 0.6503, G Loss: 2.0638\n",
      "Epoch [5/20], Step [1312/2541], D Loss: 0.6504, G Loss: 2.1049\n",
      "Epoch [5/20], Step [1313/2541], D Loss: 0.6502, G Loss: 2.1245\n",
      "Epoch [5/20], Step [1314/2541], D Loss: 0.6502, G Loss: 2.1129\n",
      "Epoch [5/20], Step [1315/2541], D Loss: 0.6504, G Loss: 2.0950\n",
      "Epoch [5/20], Step [1316/2541], D Loss: 0.6502, G Loss: 2.0640\n",
      "Epoch [5/20], Step [1317/2541], D Loss: 0.6502, G Loss: 2.0460\n",
      "Epoch [5/20], Step [1318/2541], D Loss: 0.6505, G Loss: 2.1007\n",
      "Epoch [5/20], Step [1319/2541], D Loss: 0.6515, G Loss: 2.0798\n",
      "Epoch [5/20], Step [1320/2541], D Loss: 0.6506, G Loss: 2.0817\n",
      "Epoch [5/20], Step [1321/2541], D Loss: 0.6504, G Loss: 2.0893\n",
      "Epoch [5/20], Step [1322/2541], D Loss: 0.6504, G Loss: 2.1144\n",
      "Epoch [5/20], Step [1323/2541], D Loss: 0.6505, G Loss: 2.0952\n",
      "Epoch [5/20], Step [1324/2541], D Loss: 0.6502, G Loss: 2.0497\n",
      "Epoch [5/20], Step [1325/2541], D Loss: 0.6502, G Loss: 2.0950\n",
      "Epoch [5/20], Step [1326/2541], D Loss: 0.6503, G Loss: 2.0977\n",
      "Epoch [5/20], Step [1327/2541], D Loss: 0.6502, G Loss: 2.0975\n",
      "Epoch [5/20], Step [1328/2541], D Loss: 0.6503, G Loss: 2.0742\n",
      "Epoch [5/20], Step [1329/2541], D Loss: 0.6502, G Loss: 2.1012\n",
      "Epoch [5/20], Step [1330/2541], D Loss: 0.6502, G Loss: 2.0893\n",
      "Epoch [5/20], Step [1331/2541], D Loss: 0.6502, G Loss: 2.0587\n",
      "Epoch [5/20], Step [1332/2541], D Loss: 0.6504, G Loss: 2.1051\n",
      "Epoch [5/20], Step [1333/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [5/20], Step [1334/2541], D Loss: 0.6504, G Loss: 2.1071\n",
      "Epoch [5/20], Step [1335/2541], D Loss: 0.6503, G Loss: 2.0813\n",
      "Epoch [5/20], Step [1336/2541], D Loss: 0.6503, G Loss: 2.0914\n",
      "Epoch [5/20], Step [1337/2541], D Loss: 0.6503, G Loss: 2.1063\n",
      "Epoch [5/20], Step [1338/2541], D Loss: 0.6504, G Loss: 2.0790\n",
      "Epoch [5/20], Step [1339/2541], D Loss: 0.6504, G Loss: 2.1224\n",
      "Epoch [5/20], Step [1340/2541], D Loss: 0.6505, G Loss: 2.0890\n",
      "Epoch [5/20], Step [1341/2541], D Loss: 0.6504, G Loss: 2.0845\n",
      "Epoch [5/20], Step [1342/2541], D Loss: 0.6503, G Loss: 2.0808\n",
      "Epoch [5/20], Step [1343/2541], D Loss: 0.6503, G Loss: 2.0970\n",
      "Epoch [5/20], Step [1344/2541], D Loss: 0.6503, G Loss: 2.0907\n",
      "Epoch [5/20], Step [1345/2541], D Loss: 0.6504, G Loss: 2.0665\n",
      "Epoch [5/20], Step [1346/2541], D Loss: 0.6505, G Loss: 2.0928\n",
      "Epoch [5/20], Step [1347/2541], D Loss: 0.6503, G Loss: 2.1035\n",
      "Epoch [5/20], Step [1348/2541], D Loss: 0.6502, G Loss: 2.0587\n",
      "Epoch [5/20], Step [1349/2541], D Loss: 0.6502, G Loss: 2.0581\n",
      "Epoch [5/20], Step [1350/2541], D Loss: 0.6502, G Loss: 2.0557\n",
      "Epoch [5/20], Step [1351/2541], D Loss: 0.6505, G Loss: 2.1240\n",
      "Epoch [5/20], Step [1352/2541], D Loss: 0.6503, G Loss: 2.0617\n",
      "Epoch [5/20], Step [1353/2541], D Loss: 0.6503, G Loss: 2.0572\n",
      "Epoch [5/20], Step [1354/2541], D Loss: 0.6502, G Loss: 2.1030\n",
      "Epoch [5/20], Step [1355/2541], D Loss: 0.6503, G Loss: 2.0668\n",
      "Epoch [5/20], Step [1356/2541], D Loss: 0.6510, G Loss: 2.0399\n",
      "Epoch [5/20], Step [1357/2541], D Loss: 0.6508, G Loss: 2.1223\n",
      "Epoch [5/20], Step [1358/2541], D Loss: 0.6504, G Loss: 2.0934\n",
      "Epoch [5/20], Step [1359/2541], D Loss: 0.6502, G Loss: 2.0632\n",
      "Epoch [5/20], Step [1360/2541], D Loss: 0.6503, G Loss: 2.1039\n",
      "Epoch [5/20], Step [1361/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [5/20], Step [1362/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [5/20], Step [1363/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [5/20], Step [1364/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [5/20], Step [1365/2541], D Loss: 0.6502, G Loss: 2.0531\n",
      "Epoch [5/20], Step [1366/2541], D Loss: 0.6502, G Loss: 2.0574\n",
      "Epoch [5/20], Step [1367/2541], D Loss: 0.6503, G Loss: 2.0934\n",
      "Epoch [5/20], Step [1368/2541], D Loss: 0.6503, G Loss: 2.1023\n",
      "Epoch [5/20], Step [1369/2541], D Loss: 0.6502, G Loss: 2.0963\n",
      "Epoch [5/20], Step [1370/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [5/20], Step [1371/2541], D Loss: 0.6503, G Loss: 2.0705\n",
      "Epoch [5/20], Step [1372/2541], D Loss: 0.6504, G Loss: 2.1124\n",
      "Epoch [5/20], Step [1373/2541], D Loss: 0.6503, G Loss: 2.0718\n",
      "Epoch [5/20], Step [1374/2541], D Loss: 0.6503, G Loss: 2.0883\n",
      "Epoch [5/20], Step [1375/2541], D Loss: 0.6502, G Loss: 2.0921\n",
      "Epoch [5/20], Step [1376/2541], D Loss: 0.6503, G Loss: 2.0802\n",
      "Epoch [5/20], Step [1377/2541], D Loss: 0.6502, G Loss: 2.0640\n",
      "Epoch [5/20], Step [1378/2541], D Loss: 0.6502, G Loss: 2.0671\n",
      "Epoch [5/20], Step [1379/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [5/20], Step [1380/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [5/20], Step [1381/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [5/20], Step [1382/2541], D Loss: 0.6503, G Loss: 2.0695\n",
      "Epoch [5/20], Step [1383/2541], D Loss: 0.6502, G Loss: 2.0947\n",
      "Epoch [5/20], Step [1384/2541], D Loss: 0.6503, G Loss: 2.0927\n",
      "Epoch [5/20], Step [1385/2541], D Loss: 0.6504, G Loss: 2.1224\n",
      "Epoch [5/20], Step [1386/2541], D Loss: 0.6503, G Loss: 2.1092\n",
      "Epoch [5/20], Step [1387/2541], D Loss: 0.6504, G Loss: 2.0944\n",
      "Epoch [5/20], Step [1388/2541], D Loss: 0.6506, G Loss: 2.0889\n",
      "Epoch [5/20], Step [1389/2541], D Loss: 0.6503, G Loss: 2.0839\n",
      "Epoch [5/20], Step [1390/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [5/20], Step [1391/2541], D Loss: 0.6503, G Loss: 2.0723\n",
      "Epoch [5/20], Step [1392/2541], D Loss: 0.6505, G Loss: 2.1391\n",
      "Epoch [5/20], Step [1393/2541], D Loss: 0.6507, G Loss: 2.0692\n",
      "Epoch [5/20], Step [1394/2541], D Loss: 0.6504, G Loss: 2.0538\n",
      "Epoch [5/20], Step [1395/2541], D Loss: 0.6505, G Loss: 2.0689\n",
      "Epoch [5/20], Step [1396/2541], D Loss: 0.6506, G Loss: 2.0996\n",
      "Epoch [5/20], Step [1397/2541], D Loss: 0.6503, G Loss: 2.0894\n",
      "Epoch [5/20], Step [1398/2541], D Loss: 0.6504, G Loss: 2.0960\n",
      "Epoch [5/20], Step [1399/2541], D Loss: 0.6503, G Loss: 2.0587\n",
      "Epoch [5/20], Step [1400/2541], D Loss: 0.6503, G Loss: 2.0625\n",
      "Epoch [5/20], Step [1401/2541], D Loss: 0.6503, G Loss: 2.0793\n",
      "Epoch [5/20], Step [1402/2541], D Loss: 0.6503, G Loss: 2.0838\n",
      "Epoch [5/20], Step [1403/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [5/20], Step [1404/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [5/20], Step [1405/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [5/20], Step [1406/2541], D Loss: 0.6502, G Loss: 2.0586\n",
      "Epoch [5/20], Step [1407/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [5/20], Step [1408/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [5/20], Step [1409/2541], D Loss: 0.6502, G Loss: 2.0624\n",
      "Epoch [5/20], Step [1410/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [5/20], Step [1411/2541], D Loss: 0.6503, G Loss: 2.0659\n",
      "Epoch [5/20], Step [1412/2541], D Loss: 0.6503, G Loss: 2.0944\n",
      "Epoch [5/20], Step [1413/2541], D Loss: 0.6503, G Loss: 2.0877\n",
      "Epoch [5/20], Step [1414/2541], D Loss: 0.6503, G Loss: 2.1227\n",
      "Epoch [5/20], Step [1415/2541], D Loss: 0.6502, G Loss: 2.1105\n",
      "Epoch [5/20], Step [1416/2541], D Loss: 0.6504, G Loss: 2.0858\n",
      "Epoch [5/20], Step [1417/2541], D Loss: 0.6502, G Loss: 2.1169\n",
      "Epoch [5/20], Step [1418/2541], D Loss: 0.6502, G Loss: 2.0613\n",
      "Epoch [5/20], Step [1419/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [5/20], Step [1420/2541], D Loss: 0.6502, G Loss: 2.0907\n",
      "Epoch [5/20], Step [1421/2541], D Loss: 0.6502, G Loss: 2.0693\n",
      "Epoch [5/20], Step [1422/2541], D Loss: 0.6503, G Loss: 2.0758\n",
      "Epoch [5/20], Step [1423/2541], D Loss: 0.6502, G Loss: 2.1052\n",
      "Epoch [5/20], Step [1424/2541], D Loss: 0.6503, G Loss: 2.0628\n",
      "Epoch [5/20], Step [1425/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [5/20], Step [1426/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [5/20], Step [1427/2541], D Loss: 0.6511, G Loss: 2.1025\n",
      "Epoch [5/20], Step [1428/2541], D Loss: 0.6512, G Loss: 2.1182\n",
      "Epoch [5/20], Step [1429/2541], D Loss: 0.6507, G Loss: 2.0952\n",
      "Epoch [5/20], Step [1430/2541], D Loss: 0.6504, G Loss: 2.0727\n",
      "Epoch [5/20], Step [1431/2541], D Loss: 0.6507, G Loss: 2.0771\n",
      "Epoch [5/20], Step [1432/2541], D Loss: 0.6506, G Loss: 2.0976\n",
      "Epoch [5/20], Step [1433/2541], D Loss: 0.6504, G Loss: 2.1067\n",
      "Epoch [5/20], Step [1434/2541], D Loss: 0.6502, G Loss: 2.0509\n",
      "Epoch [5/20], Step [1435/2541], D Loss: 0.6504, G Loss: 2.1194\n",
      "Epoch [5/20], Step [1436/2541], D Loss: 0.6503, G Loss: 2.1130\n",
      "Epoch [5/20], Step [1437/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [5/20], Step [1438/2541], D Loss: 0.6503, G Loss: 2.0787\n",
      "Epoch [5/20], Step [1439/2541], D Loss: 0.6504, G Loss: 2.0997\n",
      "Epoch [5/20], Step [1440/2541], D Loss: 0.6502, G Loss: 2.1199\n",
      "Epoch [5/20], Step [1441/2541], D Loss: 0.6503, G Loss: 2.0830\n",
      "Epoch [5/20], Step [1442/2541], D Loss: 0.6504, G Loss: 2.0752\n",
      "Epoch [5/20], Step [1443/2541], D Loss: 0.6503, G Loss: 2.0640\n",
      "Epoch [5/20], Step [1444/2541], D Loss: 0.6503, G Loss: 2.0342\n",
      "Epoch [5/20], Step [1445/2541], D Loss: 0.6503, G Loss: 2.1052\n",
      "Epoch [5/20], Step [1446/2541], D Loss: 0.6504, G Loss: 2.1000\n",
      "Epoch [5/20], Step [1447/2541], D Loss: 0.6505, G Loss: 2.1031\n",
      "Epoch [5/20], Step [1448/2541], D Loss: 0.6505, G Loss: 2.0312\n",
      "Epoch [5/20], Step [1449/2541], D Loss: 0.6505, G Loss: 2.0807\n",
      "Epoch [5/20], Step [1450/2541], D Loss: 0.6503, G Loss: 2.0988\n",
      "Epoch [5/20], Step [1451/2541], D Loss: 0.6502, G Loss: 2.1038\n",
      "Epoch [5/20], Step [1452/2541], D Loss: 0.6502, G Loss: 2.1144\n",
      "Epoch [5/20], Step [1453/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [5/20], Step [1454/2541], D Loss: 0.6502, G Loss: 2.0610\n",
      "Epoch [5/20], Step [1455/2541], D Loss: 0.6503, G Loss: 2.0957\n",
      "Epoch [5/20], Step [1456/2541], D Loss: 0.6503, G Loss: 2.0826\n",
      "Epoch [5/20], Step [1457/2541], D Loss: 0.6502, G Loss: 2.0469\n",
      "Epoch [5/20], Step [1458/2541], D Loss: 0.6503, G Loss: 2.0925\n",
      "Epoch [5/20], Step [1459/2541], D Loss: 0.6503, G Loss: 2.0766\n",
      "Epoch [5/20], Step [1460/2541], D Loss: 0.6502, G Loss: 2.0607\n",
      "Epoch [5/20], Step [1461/2541], D Loss: 0.6503, G Loss: 2.0876\n",
      "Epoch [5/20], Step [1462/2541], D Loss: 0.6502, G Loss: 2.1154\n",
      "Epoch [5/20], Step [1463/2541], D Loss: 0.6503, G Loss: 2.0782\n",
      "Epoch [5/20], Step [1464/2541], D Loss: 0.6503, G Loss: 2.1022\n",
      "Epoch [5/20], Step [1465/2541], D Loss: 0.6502, G Loss: 2.1117\n",
      "Epoch [5/20], Step [1466/2541], D Loss: 0.6503, G Loss: 2.0779\n",
      "Epoch [5/20], Step [1467/2541], D Loss: 0.6507, G Loss: 2.0893\n",
      "Epoch [5/20], Step [1468/2541], D Loss: 0.6506, G Loss: 2.1090\n",
      "Epoch [5/20], Step [1469/2541], D Loss: 0.6504, G Loss: 2.0696\n",
      "Epoch [5/20], Step [1470/2541], D Loss: 0.6502, G Loss: 2.0714\n",
      "Epoch [5/20], Step [1471/2541], D Loss: 0.6503, G Loss: 2.0782\n",
      "Epoch [5/20], Step [1472/2541], D Loss: 0.6502, G Loss: 2.0985\n",
      "Epoch [5/20], Step [1473/2541], D Loss: 0.6502, G Loss: 2.0922\n",
      "Epoch [5/20], Step [1474/2541], D Loss: 0.6503, G Loss: 2.1069\n",
      "Epoch [5/20], Step [1475/2541], D Loss: 0.6505, G Loss: 2.0645\n",
      "Epoch [5/20], Step [1476/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [5/20], Step [1477/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [5/20], Step [1478/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [5/20], Step [1479/2541], D Loss: 0.6503, G Loss: 2.1161\n",
      "Epoch [5/20], Step [1480/2541], D Loss: 0.6502, G Loss: 2.1083\n",
      "Epoch [5/20], Step [1481/2541], D Loss: 0.6503, G Loss: 2.1127\n",
      "Epoch [5/20], Step [1482/2541], D Loss: 0.6502, G Loss: 2.0948\n",
      "Epoch [5/20], Step [1483/2541], D Loss: 0.6502, G Loss: 2.0699\n",
      "Epoch [5/20], Step [1484/2541], D Loss: 0.6502, G Loss: 2.0342\n",
      "Epoch [5/20], Step [1485/2541], D Loss: 0.6504, G Loss: 2.0592\n",
      "Epoch [5/20], Step [1486/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [5/20], Step [1487/2541], D Loss: 0.6503, G Loss: 2.0882\n",
      "Epoch [5/20], Step [1488/2541], D Loss: 0.6503, G Loss: 2.1003\n",
      "Epoch [5/20], Step [1489/2541], D Loss: 0.6502, G Loss: 2.1062\n",
      "Epoch [5/20], Step [1490/2541], D Loss: 0.6506, G Loss: 2.0731\n",
      "Epoch [5/20], Step [1491/2541], D Loss: 0.6503, G Loss: 2.0420\n",
      "Epoch [5/20], Step [1492/2541], D Loss: 0.6502, G Loss: 2.0673\n",
      "Epoch [5/20], Step [1493/2541], D Loss: 0.6503, G Loss: 2.0796\n",
      "Epoch [5/20], Step [1494/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [5/20], Step [1495/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [5/20], Step [1496/2541], D Loss: 0.6508, G Loss: 2.0749\n",
      "Epoch [5/20], Step [1497/2541], D Loss: 0.6505, G Loss: 2.0874\n",
      "Epoch [5/20], Step [1498/2541], D Loss: 0.6506, G Loss: 2.1017\n",
      "Epoch [5/20], Step [1499/2541], D Loss: 0.6502, G Loss: 2.0523\n",
      "Epoch [5/20], Step [1500/2541], D Loss: 0.6503, G Loss: 2.0773\n",
      "Epoch [5/20], Step [1501/2541], D Loss: 0.6502, G Loss: 2.1055\n",
      "Epoch [5/20], Step [1502/2541], D Loss: 0.6503, G Loss: 2.0779\n",
      "Epoch [5/20], Step [1503/2541], D Loss: 0.6503, G Loss: 2.0823\n",
      "Epoch [5/20], Step [1504/2541], D Loss: 0.6502, G Loss: 2.0574\n",
      "Epoch [5/20], Step [1505/2541], D Loss: 0.6502, G Loss: 2.0541\n",
      "Epoch [5/20], Step [1506/2541], D Loss: 0.6504, G Loss: 2.1231\n",
      "Epoch [5/20], Step [1507/2541], D Loss: 0.6503, G Loss: 2.1082\n",
      "Epoch [5/20], Step [1508/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [5/20], Step [1509/2541], D Loss: 0.6503, G Loss: 2.0733\n",
      "Epoch [5/20], Step [1510/2541], D Loss: 0.6504, G Loss: 2.0813\n",
      "Epoch [5/20], Step [1511/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [5/20], Step [1512/2541], D Loss: 0.6502, G Loss: 2.0473\n",
      "Epoch [5/20], Step [1513/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [5/20], Step [1514/2541], D Loss: 0.6502, G Loss: 2.0712\n",
      "Epoch [5/20], Step [1515/2541], D Loss: 0.6503, G Loss: 2.0673\n",
      "Epoch [5/20], Step [1516/2541], D Loss: 0.6508, G Loss: 2.0672\n",
      "Epoch [5/20], Step [1517/2541], D Loss: 0.6504, G Loss: 2.0848\n",
      "Epoch [5/20], Step [1518/2541], D Loss: 0.6505, G Loss: 2.0828\n",
      "Epoch [5/20], Step [1519/2541], D Loss: 0.6504, G Loss: 2.0695\n",
      "Epoch [5/20], Step [1520/2541], D Loss: 0.6503, G Loss: 2.0712\n",
      "Epoch [5/20], Step [1521/2541], D Loss: 0.6502, G Loss: 2.0970\n",
      "Epoch [5/20], Step [1522/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [5/20], Step [1523/2541], D Loss: 0.6502, G Loss: 2.0471\n",
      "Epoch [5/20], Step [1524/2541], D Loss: 0.6504, G Loss: 2.0779\n",
      "Epoch [5/20], Step [1525/2541], D Loss: 0.6502, G Loss: 2.1040\n",
      "Epoch [5/20], Step [1526/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [5/20], Step [1527/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [5/20], Step [1528/2541], D Loss: 0.6504, G Loss: 2.1128\n",
      "Epoch [5/20], Step [1529/2541], D Loss: 0.6503, G Loss: 2.1076\n",
      "Epoch [5/20], Step [1530/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [5/20], Step [1531/2541], D Loss: 0.6503, G Loss: 2.0794\n",
      "Epoch [5/20], Step [1532/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [5/20], Step [1533/2541], D Loss: 0.6502, G Loss: 2.1012\n",
      "Epoch [5/20], Step [1534/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [5/20], Step [1535/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [5/20], Step [1536/2541], D Loss: 0.6503, G Loss: 2.1169\n",
      "Epoch [5/20], Step [1537/2541], D Loss: 0.6503, G Loss: 2.1040\n",
      "Epoch [5/20], Step [1538/2541], D Loss: 0.6502, G Loss: 2.1051\n",
      "Epoch [5/20], Step [1539/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [5/20], Step [1540/2541], D Loss: 0.6503, G Loss: 2.0866\n",
      "Epoch [5/20], Step [1541/2541], D Loss: 0.6502, G Loss: 2.0627\n",
      "Epoch [5/20], Step [1542/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [5/20], Step [1543/2541], D Loss: 0.6506, G Loss: 2.1291\n",
      "Epoch [5/20], Step [1544/2541], D Loss: 0.6504, G Loss: 2.3702\n",
      "Epoch [5/20], Step [1545/2541], D Loss: 0.6504, G Loss: 2.1018\n",
      "Epoch [5/20], Step [1546/2541], D Loss: 0.6503, G Loss: 2.0577\n",
      "Epoch [5/20], Step [1547/2541], D Loss: 0.6503, G Loss: 2.0616\n",
      "Epoch [5/20], Step [1548/2541], D Loss: 0.6503, G Loss: 2.0858\n",
      "Epoch [5/20], Step [1549/2541], D Loss: 0.6502, G Loss: 2.0952\n",
      "Epoch [5/20], Step [1550/2541], D Loss: 0.6502, G Loss: 2.0922\n",
      "Epoch [5/20], Step [1551/2541], D Loss: 0.6502, G Loss: 2.0639\n",
      "Epoch [5/20], Step [1552/2541], D Loss: 0.6502, G Loss: 2.0716\n",
      "Epoch [5/20], Step [1553/2541], D Loss: 0.6502, G Loss: 2.0629\n",
      "Epoch [5/20], Step [1554/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [5/20], Step [1555/2541], D Loss: 0.6502, G Loss: 2.0996\n",
      "Epoch [5/20], Step [1556/2541], D Loss: 0.6502, G Loss: 2.0986\n",
      "Epoch [5/20], Step [1557/2541], D Loss: 0.6502, G Loss: 2.0467\n",
      "Epoch [5/20], Step [1558/2541], D Loss: 0.6503, G Loss: 2.0943\n",
      "Epoch [5/20], Step [1559/2541], D Loss: 0.6510, G Loss: 2.0911\n",
      "Epoch [5/20], Step [1560/2541], D Loss: 0.6505, G Loss: 2.0706\n",
      "Epoch [5/20], Step [1561/2541], D Loss: 0.6506, G Loss: 2.0776\n",
      "Epoch [5/20], Step [1562/2541], D Loss: 0.6504, G Loss: 2.0802\n",
      "Epoch [5/20], Step [1563/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [5/20], Step [1564/2541], D Loss: 0.6509, G Loss: 2.1110\n",
      "Epoch [5/20], Step [1565/2541], D Loss: 0.6505, G Loss: 2.0774\n",
      "Epoch [5/20], Step [1566/2541], D Loss: 0.6504, G Loss: 2.0451\n",
      "Epoch [5/20], Step [1567/2541], D Loss: 0.6504, G Loss: 2.0636\n",
      "Epoch [5/20], Step [1568/2541], D Loss: 0.6504, G Loss: 2.0813\n",
      "Epoch [5/20], Step [1569/2541], D Loss: 0.6506, G Loss: 2.0965\n",
      "Epoch [5/20], Step [1570/2541], D Loss: 0.6504, G Loss: 2.0741\n",
      "Epoch [5/20], Step [1571/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [5/20], Step [1572/2541], D Loss: 0.6502, G Loss: 2.1035\n",
      "Epoch [5/20], Step [1573/2541], D Loss: 0.6503, G Loss: 2.0559\n",
      "Epoch [5/20], Step [1574/2541], D Loss: 0.6504, G Loss: 2.0928\n",
      "Epoch [5/20], Step [1575/2541], D Loss: 0.6503, G Loss: 2.1253\n",
      "Epoch [5/20], Step [1576/2541], D Loss: 0.6503, G Loss: 2.0951\n",
      "Epoch [5/20], Step [1577/2541], D Loss: 0.6502, G Loss: 2.0675\n",
      "Epoch [5/20], Step [1578/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [5/20], Step [1579/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [5/20], Step [1580/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [5/20], Step [1581/2541], D Loss: 0.6502, G Loss: 2.1039\n",
      "Epoch [5/20], Step [1582/2541], D Loss: 0.6508, G Loss: 2.0794\n",
      "Epoch [5/20], Step [1583/2541], D Loss: 0.6504, G Loss: 2.0871\n",
      "Epoch [5/20], Step [1584/2541], D Loss: 0.6506, G Loss: 2.0857\n",
      "Epoch [5/20], Step [1585/2541], D Loss: 0.6503, G Loss: 2.0929\n",
      "Epoch [5/20], Step [1586/2541], D Loss: 0.6503, G Loss: 2.0727\n",
      "Epoch [5/20], Step [1587/2541], D Loss: 0.6505, G Loss: 2.1079\n",
      "Epoch [5/20], Step [1588/2541], D Loss: 0.6506, G Loss: 2.0919\n",
      "Epoch [5/20], Step [1589/2541], D Loss: 0.6511, G Loss: 2.0682\n",
      "Epoch [5/20], Step [1590/2541], D Loss: 0.6503, G Loss: 2.0988\n",
      "Epoch [5/20], Step [1591/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [5/20], Step [1592/2541], D Loss: 0.6503, G Loss: 2.0924\n",
      "Epoch [5/20], Step [1593/2541], D Loss: 0.6504, G Loss: 2.0830\n",
      "Epoch [5/20], Step [1594/2541], D Loss: 0.6506, G Loss: 2.0876\n",
      "Epoch [5/20], Step [1595/2541], D Loss: 0.6505, G Loss: 2.0773\n",
      "Epoch [5/20], Step [1596/2541], D Loss: 0.6504, G Loss: 2.0577\n",
      "Epoch [5/20], Step [1597/2541], D Loss: 0.6503, G Loss: 2.0947\n",
      "Epoch [5/20], Step [1598/2541], D Loss: 0.6503, G Loss: 2.1115\n",
      "Epoch [5/20], Step [1599/2541], D Loss: 0.6509, G Loss: 2.0739\n",
      "Epoch [5/20], Step [1600/2541], D Loss: 0.6506, G Loss: 2.0880\n",
      "Epoch [5/20], Step [1601/2541], D Loss: 0.6505, G Loss: 2.0744\n",
      "Epoch [5/20], Step [1602/2541], D Loss: 0.6503, G Loss: 2.0961\n",
      "Epoch [5/20], Step [1603/2541], D Loss: 0.6503, G Loss: 2.2176\n",
      "Epoch [5/20], Step [1604/2541], D Loss: 0.6503, G Loss: 2.0649\n",
      "Epoch [5/20], Step [1605/2541], D Loss: 0.6503, G Loss: 2.0834\n",
      "Epoch [5/20], Step [1606/2541], D Loss: 0.6503, G Loss: 2.0790\n",
      "Epoch [5/20], Step [1607/2541], D Loss: 0.6502, G Loss: 2.0553\n",
      "Epoch [5/20], Step [1608/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [5/20], Step [1609/2541], D Loss: 0.6502, G Loss: 2.0876\n",
      "Epoch [5/20], Step [1610/2541], D Loss: 0.6503, G Loss: 2.0765\n",
      "Epoch [5/20], Step [1611/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [5/20], Step [1612/2541], D Loss: 0.6502, G Loss: 2.0608\n",
      "Epoch [5/20], Step [1613/2541], D Loss: 0.6502, G Loss: 2.0585\n",
      "Epoch [5/20], Step [1614/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [5/20], Step [1615/2541], D Loss: 0.6503, G Loss: 2.0942\n",
      "Epoch [5/20], Step [1616/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [5/20], Step [1617/2541], D Loss: 0.6502, G Loss: 2.1107\n",
      "Epoch [5/20], Step [1618/2541], D Loss: 0.6503, G Loss: 2.0844\n",
      "Epoch [5/20], Step [1619/2541], D Loss: 0.6502, G Loss: 2.0627\n",
      "Epoch [5/20], Step [1620/2541], D Loss: 0.6502, G Loss: 2.0561\n",
      "Epoch [5/20], Step [1621/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [5/20], Step [1622/2541], D Loss: 0.6503, G Loss: 2.1421\n",
      "Epoch [5/20], Step [1623/2541], D Loss: 0.6507, G Loss: 2.0509\n",
      "Epoch [5/20], Step [1624/2541], D Loss: 0.6503, G Loss: 2.0682\n",
      "Epoch [5/20], Step [1625/2541], D Loss: 0.6504, G Loss: 2.0798\n",
      "Epoch [5/20], Step [1626/2541], D Loss: 0.6503, G Loss: 2.0893\n",
      "Epoch [5/20], Step [1627/2541], D Loss: 0.6504, G Loss: 2.1070\n",
      "Epoch [5/20], Step [1628/2541], D Loss: 0.6505, G Loss: 2.0988\n",
      "Epoch [5/20], Step [1629/2541], D Loss: 0.6505, G Loss: 2.0715\n",
      "Epoch [5/20], Step [1630/2541], D Loss: 0.6503, G Loss: 2.0836\n",
      "Epoch [5/20], Step [1631/2541], D Loss: 0.6503, G Loss: 2.0994\n",
      "Epoch [5/20], Step [1632/2541], D Loss: 0.6502, G Loss: 2.1115\n",
      "Epoch [5/20], Step [1633/2541], D Loss: 0.6503, G Loss: 2.0636\n",
      "Epoch [5/20], Step [1634/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [5/20], Step [1635/2541], D Loss: 0.6503, G Loss: 2.0687\n",
      "Epoch [5/20], Step [1636/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [5/20], Step [1637/2541], D Loss: 0.6502, G Loss: 2.0955\n",
      "Epoch [5/20], Step [1638/2541], D Loss: 0.6502, G Loss: 2.0658\n",
      "Epoch [5/20], Step [1639/2541], D Loss: 0.6504, G Loss: 2.0529\n",
      "Epoch [5/20], Step [1640/2541], D Loss: 0.6503, G Loss: 2.0914\n",
      "Epoch [5/20], Step [1641/2541], D Loss: 0.6502, G Loss: 2.0679\n",
      "Epoch [5/20], Step [1642/2541], D Loss: 0.6502, G Loss: 2.0643\n",
      "Epoch [5/20], Step [1643/2541], D Loss: 0.6502, G Loss: 2.0662\n",
      "Epoch [5/20], Step [1644/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [5/20], Step [1645/2541], D Loss: 0.6503, G Loss: 2.0951\n",
      "Epoch [5/20], Step [1646/2541], D Loss: 0.6502, G Loss: 2.0952\n",
      "Epoch [5/20], Step [1647/2541], D Loss: 0.6504, G Loss: 2.0814\n",
      "Epoch [5/20], Step [1648/2541], D Loss: 0.6503, G Loss: 2.0624\n",
      "Epoch [5/20], Step [1649/2541], D Loss: 0.6503, G Loss: 2.0794\n",
      "Epoch [5/20], Step [1650/2541], D Loss: 0.6503, G Loss: 2.0864\n",
      "Epoch [5/20], Step [1651/2541], D Loss: 0.6503, G Loss: 2.1197\n",
      "Epoch [5/20], Step [1652/2541], D Loss: 0.6503, G Loss: 2.0456\n",
      "Epoch [5/20], Step [1653/2541], D Loss: 0.6503, G Loss: 2.0829\n",
      "Epoch [5/20], Step [1654/2541], D Loss: 0.6504, G Loss: 2.1203\n",
      "Epoch [5/20], Step [1655/2541], D Loss: 0.6503, G Loss: 2.1048\n",
      "Epoch [5/20], Step [1656/2541], D Loss: 0.6503, G Loss: 2.1192\n",
      "Epoch [5/20], Step [1657/2541], D Loss: 0.6503, G Loss: 2.0854\n",
      "Epoch [5/20], Step [1658/2541], D Loss: 0.6502, G Loss: 2.0620\n",
      "Epoch [5/20], Step [1659/2541], D Loss: 0.6503, G Loss: 2.0753\n",
      "Epoch [5/20], Step [1660/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [5/20], Step [1661/2541], D Loss: 0.6502, G Loss: 2.1008\n",
      "Epoch [5/20], Step [1662/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [5/20], Step [1663/2541], D Loss: 0.6502, G Loss: 2.0985\n",
      "Epoch [5/20], Step [1664/2541], D Loss: 0.6502, G Loss: 2.0996\n",
      "Epoch [5/20], Step [1665/2541], D Loss: 0.6502, G Loss: 2.0802\n",
      "Epoch [5/20], Step [1666/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [5/20], Step [1667/2541], D Loss: 0.6503, G Loss: 2.0831\n",
      "Epoch [5/20], Step [1668/2541], D Loss: 0.6502, G Loss: 2.0632\n",
      "Epoch [5/20], Step [1669/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [5/20], Step [1670/2541], D Loss: 0.6503, G Loss: 2.1134\n",
      "Epoch [5/20], Step [1671/2541], D Loss: 0.6503, G Loss: 2.0825\n",
      "Epoch [5/20], Step [1672/2541], D Loss: 0.6503, G Loss: 2.0933\n",
      "Epoch [5/20], Step [1673/2541], D Loss: 0.6502, G Loss: 2.0668\n",
      "Epoch [5/20], Step [1674/2541], D Loss: 0.6503, G Loss: 2.0481\n",
      "Epoch [5/20], Step [1675/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [5/20], Step [1676/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [5/20], Step [1677/2541], D Loss: 0.6502, G Loss: 2.0677\n",
      "Epoch [5/20], Step [1678/2541], D Loss: 0.6503, G Loss: 2.0980\n",
      "Epoch [5/20], Step [1679/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [5/20], Step [1680/2541], D Loss: 0.6503, G Loss: 2.0703\n",
      "Epoch [5/20], Step [1681/2541], D Loss: 0.6502, G Loss: 2.0719\n",
      "Epoch [5/20], Step [1682/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [5/20], Step [1683/2541], D Loss: 0.6502, G Loss: 2.1168\n",
      "Epoch [5/20], Step [1684/2541], D Loss: 0.6503, G Loss: 2.0754\n",
      "Epoch [5/20], Step [1685/2541], D Loss: 0.6503, G Loss: 2.0509\n",
      "Epoch [5/20], Step [1686/2541], D Loss: 0.6503, G Loss: 2.0785\n",
      "Epoch [5/20], Step [1687/2541], D Loss: 0.6502, G Loss: 2.1081\n",
      "Epoch [5/20], Step [1688/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [5/20], Step [1689/2541], D Loss: 0.6502, G Loss: 2.0247\n",
      "Epoch [5/20], Step [1690/2541], D Loss: 0.6502, G Loss: 2.0959\n",
      "Epoch [5/20], Step [1691/2541], D Loss: 0.6502, G Loss: 2.1070\n",
      "Epoch [5/20], Step [1692/2541], D Loss: 0.6502, G Loss: 2.0979\n",
      "Epoch [5/20], Step [1693/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [5/20], Step [1694/2541], D Loss: 0.6502, G Loss: 2.0657\n",
      "Epoch [5/20], Step [1695/2541], D Loss: 0.6503, G Loss: 2.0988\n",
      "Epoch [5/20], Step [1696/2541], D Loss: 0.6503, G Loss: 2.0829\n",
      "Epoch [5/20], Step [1697/2541], D Loss: 0.6503, G Loss: 2.0659\n",
      "Epoch [5/20], Step [1698/2541], D Loss: 0.6503, G Loss: 2.1094\n",
      "Epoch [5/20], Step [1699/2541], D Loss: 0.6502, G Loss: 2.1009\n",
      "Epoch [5/20], Step [1700/2541], D Loss: 0.6502, G Loss: 2.0571\n",
      "Epoch [5/20], Step [1701/2541], D Loss: 0.6502, G Loss: 2.0679\n",
      "Epoch [5/20], Step [1702/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [5/20], Step [1703/2541], D Loss: 0.6502, G Loss: 2.0939\n",
      "Epoch [5/20], Step [1704/2541], D Loss: 0.6502, G Loss: 2.0983\n",
      "Epoch [5/20], Step [1705/2541], D Loss: 0.6502, G Loss: 2.0560\n",
      "Epoch [5/20], Step [1706/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [5/20], Step [1707/2541], D Loss: 0.6502, G Loss: 2.1082\n",
      "Epoch [5/20], Step [1708/2541], D Loss: 0.6502, G Loss: 2.0719\n",
      "Epoch [5/20], Step [1709/2541], D Loss: 0.6502, G Loss: 2.0674\n",
      "Epoch [5/20], Step [1710/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [5/20], Step [1711/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [5/20], Step [1712/2541], D Loss: 0.6502, G Loss: 2.0997\n",
      "Epoch [5/20], Step [1713/2541], D Loss: 0.6502, G Loss: 2.0672\n",
      "Epoch [5/20], Step [1714/2541], D Loss: 0.6502, G Loss: 2.1012\n",
      "Epoch [5/20], Step [1715/2541], D Loss: 0.6503, G Loss: 2.0665\n",
      "Epoch [5/20], Step [1716/2541], D Loss: 0.6504, G Loss: 2.0894\n",
      "Epoch [5/20], Step [1717/2541], D Loss: 0.6504, G Loss: 2.0542\n",
      "Epoch [5/20], Step [1718/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [5/20], Step [1719/2541], D Loss: 0.6502, G Loss: 2.1165\n",
      "Epoch [5/20], Step [1720/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [5/20], Step [1721/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [5/20], Step [1722/2541], D Loss: 0.6502, G Loss: 2.0649\n",
      "Epoch [5/20], Step [1723/2541], D Loss: 0.6502, G Loss: 2.1096\n",
      "Epoch [5/20], Step [1724/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [5/20], Step [1725/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [5/20], Step [1726/2541], D Loss: 0.6502, G Loss: 2.0949\n",
      "Epoch [5/20], Step [1727/2541], D Loss: 0.6503, G Loss: 2.0849\n",
      "Epoch [5/20], Step [1728/2541], D Loss: 0.6503, G Loss: 2.0902\n",
      "Epoch [5/20], Step [1729/2541], D Loss: 0.6504, G Loss: 2.0830\n",
      "Epoch [5/20], Step [1730/2541], D Loss: 0.6503, G Loss: 2.0811\n",
      "Epoch [5/20], Step [1731/2541], D Loss: 0.6502, G Loss: 2.0898\n",
      "Epoch [5/20], Step [1732/2541], D Loss: 0.6502, G Loss: 2.0650\n",
      "Epoch [5/20], Step [1733/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [5/20], Step [1734/2541], D Loss: 0.6502, G Loss: 2.0972\n",
      "Epoch [5/20], Step [1735/2541], D Loss: 0.6502, G Loss: 2.1026\n",
      "Epoch [5/20], Step [1736/2541], D Loss: 0.6502, G Loss: 2.1082\n",
      "Epoch [5/20], Step [1737/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [5/20], Step [1738/2541], D Loss: 0.6502, G Loss: 2.0602\n",
      "Epoch [5/20], Step [1739/2541], D Loss: 0.6502, G Loss: 2.0663\n",
      "Epoch [5/20], Step [1740/2541], D Loss: 0.6502, G Loss: 2.1210\n",
      "Epoch [5/20], Step [1741/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [5/20], Step [1742/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [5/20], Step [1743/2541], D Loss: 0.6502, G Loss: 2.0620\n",
      "Epoch [5/20], Step [1744/2541], D Loss: 0.6502, G Loss: 2.0990\n",
      "Epoch [5/20], Step [1745/2541], D Loss: 0.6502, G Loss: 2.0530\n",
      "Epoch [5/20], Step [1746/2541], D Loss: 0.6502, G Loss: 2.1023\n",
      "Epoch [5/20], Step [1747/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [5/20], Step [1748/2541], D Loss: 0.6503, G Loss: 2.0786\n",
      "Epoch [5/20], Step [1749/2541], D Loss: 0.6503, G Loss: 2.0713\n",
      "Epoch [5/20], Step [1750/2541], D Loss: 0.6503, G Loss: 2.0826\n",
      "Epoch [5/20], Step [1751/2541], D Loss: 0.6503, G Loss: 2.0902\n",
      "Epoch [5/20], Step [1752/2541], D Loss: 0.6504, G Loss: 2.1046\n",
      "Epoch [5/20], Step [1753/2541], D Loss: 0.6504, G Loss: 2.0788\n",
      "Epoch [5/20], Step [1754/2541], D Loss: 0.6502, G Loss: 2.0492\n",
      "Epoch [5/20], Step [1755/2541], D Loss: 0.6503, G Loss: 2.0836\n",
      "Epoch [5/20], Step [1756/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [5/20], Step [1757/2541], D Loss: 0.6503, G Loss: 2.1012\n",
      "Epoch [5/20], Step [1758/2541], D Loss: 0.6503, G Loss: 2.0875\n",
      "Epoch [5/20], Step [1759/2541], D Loss: 0.6502, G Loss: 2.0459\n",
      "Epoch [5/20], Step [1760/2541], D Loss: 0.6503, G Loss: 2.0770\n",
      "Epoch [5/20], Step [1761/2541], D Loss: 0.6503, G Loss: 2.0664\n",
      "Epoch [5/20], Step [1762/2541], D Loss: 0.6503, G Loss: 2.0744\n",
      "Epoch [5/20], Step [1763/2541], D Loss: 0.6502, G Loss: 2.0483\n",
      "Epoch [5/20], Step [1764/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [5/20], Step [1765/2541], D Loss: 0.6502, G Loss: 2.0641\n",
      "Epoch [5/20], Step [1766/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [5/20], Step [1767/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [5/20], Step [1768/2541], D Loss: 0.6504, G Loss: 2.0973\n",
      "Epoch [5/20], Step [1769/2541], D Loss: 0.6503, G Loss: 2.0991\n",
      "Epoch [5/20], Step [1770/2541], D Loss: 0.6503, G Loss: 2.0907\n",
      "Epoch [5/20], Step [1771/2541], D Loss: 0.6503, G Loss: 2.0684\n",
      "Epoch [5/20], Step [1772/2541], D Loss: 0.6503, G Loss: 2.0838\n",
      "Epoch [5/20], Step [1773/2541], D Loss: 0.6503, G Loss: 2.1105\n",
      "Epoch [5/20], Step [1774/2541], D Loss: 0.6504, G Loss: 2.0847\n",
      "Epoch [5/20], Step [1775/2541], D Loss: 0.6503, G Loss: 2.0792\n",
      "Epoch [5/20], Step [1776/2541], D Loss: 0.6503, G Loss: 2.1199\n",
      "Epoch [5/20], Step [1777/2541], D Loss: 0.6502, G Loss: 2.1096\n",
      "Epoch [5/20], Step [1778/2541], D Loss: 0.6503, G Loss: 2.1900\n",
      "Epoch [5/20], Step [1779/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [5/20], Step [1780/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [5/20], Step [1781/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [5/20], Step [1782/2541], D Loss: 0.6502, G Loss: 2.1014\n",
      "Epoch [5/20], Step [1783/2541], D Loss: 0.6503, G Loss: 2.0713\n",
      "Epoch [5/20], Step [1784/2541], D Loss: 0.6504, G Loss: 2.0815\n",
      "Epoch [5/20], Step [1785/2541], D Loss: 0.6505, G Loss: 2.0344\n",
      "Epoch [5/20], Step [1786/2541], D Loss: 0.6503, G Loss: 2.0841\n",
      "Epoch [5/20], Step [1787/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [5/20], Step [1788/2541], D Loss: 0.6502, G Loss: 2.0660\n",
      "Epoch [5/20], Step [1789/2541], D Loss: 0.6502, G Loss: 2.0556\n",
      "Epoch [5/20], Step [1790/2541], D Loss: 0.6502, G Loss: 2.0553\n",
      "Epoch [5/20], Step [1791/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [5/20], Step [1792/2541], D Loss: 0.6502, G Loss: 2.0999\n",
      "Epoch [5/20], Step [1793/2541], D Loss: 0.6506, G Loss: 2.0703\n",
      "Epoch [5/20], Step [1794/2541], D Loss: 0.6503, G Loss: 2.1024\n",
      "Epoch [5/20], Step [1795/2541], D Loss: 0.6503, G Loss: 2.0538\n",
      "Epoch [5/20], Step [1796/2541], D Loss: 0.6504, G Loss: 2.0791\n",
      "Epoch [5/20], Step [1797/2541], D Loss: 0.6502, G Loss: 2.0932\n",
      "Epoch [5/20], Step [1798/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [5/20], Step [1799/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [5/20], Step [1800/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [5/20], Step [1801/2541], D Loss: 0.6503, G Loss: 2.1260\n",
      "Epoch [5/20], Step [1802/2541], D Loss: 0.6503, G Loss: 2.0681\n",
      "Epoch [5/20], Step [1803/2541], D Loss: 0.6502, G Loss: 2.1013\n",
      "Epoch [5/20], Step [1804/2541], D Loss: 0.6502, G Loss: 2.0725\n",
      "Epoch [5/20], Step [1805/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [5/20], Step [1806/2541], D Loss: 0.6505, G Loss: 2.1097\n",
      "Epoch [5/20], Step [1807/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [5/20], Step [1808/2541], D Loss: 0.6506, G Loss: 2.0612\n",
      "Epoch [5/20], Step [1809/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [5/20], Step [1810/2541], D Loss: 0.6502, G Loss: 2.1106\n",
      "Epoch [5/20], Step [1811/2541], D Loss: 0.6503, G Loss: 2.0847\n",
      "Epoch [5/20], Step [1812/2541], D Loss: 0.6504, G Loss: 2.0965\n",
      "Epoch [5/20], Step [1813/2541], D Loss: 0.6504, G Loss: 2.0707\n",
      "Epoch [5/20], Step [1814/2541], D Loss: 0.6505, G Loss: 2.1027\n",
      "Epoch [5/20], Step [1815/2541], D Loss: 0.6503, G Loss: 2.0848\n",
      "Epoch [5/20], Step [1816/2541], D Loss: 0.6502, G Loss: 2.0623\n",
      "Epoch [5/20], Step [1817/2541], D Loss: 0.6505, G Loss: 2.0762\n",
      "Epoch [5/20], Step [1818/2541], D Loss: 0.6502, G Loss: 2.0547\n",
      "Epoch [5/20], Step [1819/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [5/20], Step [1820/2541], D Loss: 0.6503, G Loss: 2.0867\n",
      "Epoch [5/20], Step [1821/2541], D Loss: 0.6502, G Loss: 2.1092\n",
      "Epoch [5/20], Step [1822/2541], D Loss: 0.6503, G Loss: 2.0684\n",
      "Epoch [5/20], Step [1823/2541], D Loss: 0.6502, G Loss: 2.0607\n",
      "Epoch [5/20], Step [1824/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [5/20], Step [1825/2541], D Loss: 0.6502, G Loss: 2.2358\n",
      "Epoch [5/20], Step [1826/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [5/20], Step [1827/2541], D Loss: 0.6502, G Loss: 2.0683\n",
      "Epoch [5/20], Step [1828/2541], D Loss: 0.6502, G Loss: 2.0698\n",
      "Epoch [5/20], Step [1829/2541], D Loss: 0.6502, G Loss: 2.1126\n",
      "Epoch [5/20], Step [1830/2541], D Loss: 0.6503, G Loss: 2.0675\n",
      "Epoch [5/20], Step [1831/2541], D Loss: 0.6503, G Loss: 2.0884\n",
      "Epoch [5/20], Step [1832/2541], D Loss: 0.6502, G Loss: 2.0620\n",
      "Epoch [5/20], Step [1833/2541], D Loss: 0.6502, G Loss: 2.1015\n",
      "Epoch [5/20], Step [1834/2541], D Loss: 0.6502, G Loss: 2.0638\n",
      "Epoch [5/20], Step [1835/2541], D Loss: 0.6503, G Loss: 2.0750\n",
      "Epoch [5/20], Step [1836/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [5/20], Step [1837/2541], D Loss: 0.6503, G Loss: 2.0826\n",
      "Epoch [5/20], Step [1838/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [5/20], Step [1839/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [5/20], Step [1840/2541], D Loss: 0.6503, G Loss: 2.0847\n",
      "Epoch [5/20], Step [1841/2541], D Loss: 0.6503, G Loss: 2.0745\n",
      "Epoch [5/20], Step [1842/2541], D Loss: 0.6505, G Loss: 2.0649\n",
      "Epoch [5/20], Step [1843/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [5/20], Step [1844/2541], D Loss: 0.6503, G Loss: 2.0991\n",
      "Epoch [5/20], Step [1845/2541], D Loss: 0.6512, G Loss: 2.2326\n",
      "Epoch [5/20], Step [1846/2541], D Loss: 0.6527, G Loss: 2.4708\n",
      "Epoch [5/20], Step [1847/2541], D Loss: 0.6743, G Loss: 1.4562\n",
      "Epoch [5/20], Step [1848/2541], D Loss: 0.7101, G Loss: 2.1988\n",
      "Epoch [5/20], Step [1849/2541], D Loss: 0.6778, G Loss: 2.1598\n",
      "Epoch [5/20], Step [1850/2541], D Loss: 0.6588, G Loss: 2.0861\n",
      "Epoch [5/20], Step [1851/2541], D Loss: 0.6628, G Loss: 2.0974\n",
      "Epoch [5/20], Step [1852/2541], D Loss: 0.6566, G Loss: 2.0777\n",
      "Epoch [5/20], Step [1853/2541], D Loss: 0.6638, G Loss: 2.2731\n",
      "Epoch [5/20], Step [1854/2541], D Loss: 0.6589, G Loss: 2.0649\n",
      "Epoch [5/20], Step [1855/2541], D Loss: 0.6523, G Loss: 2.3488\n",
      "Epoch [5/20], Step [1856/2541], D Loss: 0.6584, G Loss: 1.9888\n",
      "Epoch [5/20], Step [1857/2541], D Loss: 0.6635, G Loss: 2.2672\n",
      "Epoch [5/20], Step [1858/2541], D Loss: 0.6629, G Loss: 2.2436\n",
      "Epoch [5/20], Step [1859/2541], D Loss: 0.6581, G Loss: 2.0006\n",
      "Epoch [5/20], Step [1860/2541], D Loss: 0.6536, G Loss: 2.0696\n",
      "Epoch [5/20], Step [1861/2541], D Loss: 0.6522, G Loss: 2.0133\n",
      "Epoch [5/20], Step [1862/2541], D Loss: 0.6516, G Loss: 2.0600\n",
      "Epoch [5/20], Step [1863/2541], D Loss: 0.6510, G Loss: 2.0953\n",
      "Epoch [5/20], Step [1864/2541], D Loss: 0.6510, G Loss: 2.1113\n",
      "Epoch [5/20], Step [1865/2541], D Loss: 0.6511, G Loss: 2.0993\n",
      "Epoch [5/20], Step [1866/2541], D Loss: 0.6509, G Loss: 2.0124\n",
      "Epoch [5/20], Step [1867/2541], D Loss: 0.6539, G Loss: 1.9555\n",
      "Epoch [5/20], Step [1868/2541], D Loss: 0.6539, G Loss: 2.0103\n",
      "Epoch [5/20], Step [1869/2541], D Loss: 0.6533, G Loss: 2.0712\n",
      "Epoch [5/20], Step [1870/2541], D Loss: 0.6512, G Loss: 2.1428\n",
      "Epoch [5/20], Step [1871/2541], D Loss: 0.6506, G Loss: 2.1081\n",
      "Epoch [5/20], Step [1872/2541], D Loss: 0.6511, G Loss: 2.0813\n",
      "Epoch [5/20], Step [1873/2541], D Loss: 0.6503, G Loss: 2.0784\n",
      "Epoch [5/20], Step [1874/2541], D Loss: 0.6515, G Loss: 2.0848\n",
      "Epoch [5/20], Step [1875/2541], D Loss: 0.6508, G Loss: 2.0416\n",
      "Epoch [5/20], Step [1876/2541], D Loss: 0.6505, G Loss: 2.0574\n",
      "Epoch [5/20], Step [1877/2541], D Loss: 0.6504, G Loss: 1.9775\n",
      "Epoch [5/20], Step [1878/2541], D Loss: 0.6508, G Loss: 2.0916\n",
      "Epoch [5/20], Step [1879/2541], D Loss: 0.6509, G Loss: 2.1083\n",
      "Epoch [5/20], Step [1880/2541], D Loss: 0.6544, G Loss: 2.0990\n",
      "Epoch [5/20], Step [1881/2541], D Loss: 0.6518, G Loss: 2.0253\n",
      "Epoch [5/20], Step [1882/2541], D Loss: 0.6521, G Loss: 2.0770\n",
      "Epoch [5/20], Step [1883/2541], D Loss: 0.6503, G Loss: 2.1205\n",
      "Epoch [5/20], Step [1884/2541], D Loss: 0.6505, G Loss: 2.1098\n",
      "Epoch [5/20], Step [1885/2541], D Loss: 0.6507, G Loss: 2.0420\n",
      "Epoch [5/20], Step [1886/2541], D Loss: 0.6504, G Loss: 2.0349\n",
      "Epoch [5/20], Step [1887/2541], D Loss: 0.6503, G Loss: 1.9800\n",
      "Epoch [5/20], Step [1888/2541], D Loss: 0.6505, G Loss: 2.1044\n",
      "Epoch [5/20], Step [1889/2541], D Loss: 0.6508, G Loss: 2.0811\n",
      "Epoch [5/20], Step [1890/2541], D Loss: 0.6505, G Loss: 2.0848\n",
      "Epoch [5/20], Step [1891/2541], D Loss: 0.6504, G Loss: 2.1783\n",
      "Epoch [5/20], Step [1892/2541], D Loss: 0.6509, G Loss: 2.1106\n",
      "Epoch [5/20], Step [1893/2541], D Loss: 0.6510, G Loss: 2.1374\n",
      "Epoch [5/20], Step [1894/2541], D Loss: 0.6505, G Loss: 2.1390\n",
      "Epoch [5/20], Step [1895/2541], D Loss: 0.6506, G Loss: 2.0378\n",
      "Epoch [5/20], Step [1896/2541], D Loss: 0.6507, G Loss: 2.0911\n",
      "Epoch [5/20], Step [1897/2541], D Loss: 0.6503, G Loss: 2.0812\n",
      "Epoch [5/20], Step [1898/2541], D Loss: 0.6505, G Loss: 2.0274\n",
      "Epoch [5/20], Step [1899/2541], D Loss: 0.6507, G Loss: 2.0951\n",
      "Epoch [5/20], Step [1900/2541], D Loss: 0.6504, G Loss: 2.0762\n",
      "Epoch [5/20], Step [1901/2541], D Loss: 0.6506, G Loss: 2.0931\n",
      "Epoch [5/20], Step [1902/2541], D Loss: 0.6502, G Loss: 2.0477\n",
      "Epoch [5/20], Step [1903/2541], D Loss: 0.6503, G Loss: 2.0895\n",
      "Epoch [5/20], Step [1904/2541], D Loss: 0.6502, G Loss: 2.0110\n",
      "Epoch [5/20], Step [1905/2541], D Loss: 0.6503, G Loss: 2.1032\n",
      "Epoch [5/20], Step [1906/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [5/20], Step [1907/2541], D Loss: 0.6504, G Loss: 2.1270\n",
      "Epoch [5/20], Step [1908/2541], D Loss: 0.6503, G Loss: 2.1054\n",
      "Epoch [5/20], Step [1909/2541], D Loss: 0.6502, G Loss: 2.0244\n",
      "Epoch [5/20], Step [1910/2541], D Loss: 0.6503, G Loss: 2.1209\n",
      "Epoch [5/20], Step [1911/2541], D Loss: 0.6502, G Loss: 2.1431\n",
      "Epoch [5/20], Step [1912/2541], D Loss: 0.6503, G Loss: 2.0521\n",
      "Epoch [5/20], Step [1913/2541], D Loss: 0.6508, G Loss: 2.0679\n",
      "Epoch [5/20], Step [1914/2541], D Loss: 0.6502, G Loss: 2.0536\n",
      "Epoch [5/20], Step [1915/2541], D Loss: 0.6502, G Loss: 2.0124\n",
      "Epoch [5/20], Step [1916/2541], D Loss: 0.6507, G Loss: 2.0576\n",
      "Epoch [5/20], Step [1917/2541], D Loss: 0.6511, G Loss: 2.0889\n",
      "Epoch [5/20], Step [1918/2541], D Loss: 0.6503, G Loss: 2.0974\n",
      "Epoch [5/20], Step [1919/2541], D Loss: 0.6506, G Loss: 2.0892\n",
      "Epoch [5/20], Step [1920/2541], D Loss: 0.6518, G Loss: 2.1049\n",
      "Epoch [5/20], Step [1921/2541], D Loss: 0.6509, G Loss: 2.2086\n",
      "Epoch [5/20], Step [1922/2541], D Loss: 0.6513, G Loss: 2.1271\n",
      "Epoch [5/20], Step [1923/2541], D Loss: 0.6509, G Loss: 2.0622\n",
      "Epoch [5/20], Step [1924/2541], D Loss: 0.6507, G Loss: 2.0796\n",
      "Epoch [5/20], Step [1925/2541], D Loss: 0.6506, G Loss: 2.0863\n",
      "Epoch [5/20], Step [1926/2541], D Loss: 0.6508, G Loss: 2.0874\n",
      "Epoch [5/20], Step [1927/2541], D Loss: 0.6505, G Loss: 2.1123\n",
      "Epoch [5/20], Step [1928/2541], D Loss: 0.6503, G Loss: 2.0828\n",
      "Epoch [5/20], Step [1929/2541], D Loss: 0.6503, G Loss: 2.0622\n",
      "Epoch [5/20], Step [1930/2541], D Loss: 0.6513, G Loss: 2.0429\n",
      "Epoch [5/20], Step [1931/2541], D Loss: 0.6507, G Loss: 2.0554\n",
      "Epoch [5/20], Step [1932/2541], D Loss: 0.6504, G Loss: 2.1170\n",
      "Epoch [5/20], Step [1933/2541], D Loss: 0.6504, G Loss: 2.0988\n",
      "Epoch [5/20], Step [1934/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [5/20], Step [1935/2541], D Loss: 0.6505, G Loss: 2.0621\n",
      "Epoch [5/20], Step [1936/2541], D Loss: 0.6503, G Loss: 2.0705\n",
      "Epoch [5/20], Step [1937/2541], D Loss: 0.6502, G Loss: 2.0722\n",
      "Epoch [5/20], Step [1938/2541], D Loss: 0.6506, G Loss: 2.0412\n",
      "Epoch [5/20], Step [1939/2541], D Loss: 0.6504, G Loss: 2.0488\n",
      "Epoch [5/20], Step [1940/2541], D Loss: 0.6512, G Loss: 2.1018\n",
      "Epoch [5/20], Step [1941/2541], D Loss: 0.6504, G Loss: 2.0867\n",
      "Epoch [5/20], Step [1942/2541], D Loss: 0.6505, G Loss: 2.0222\n",
      "Epoch [5/20], Step [1943/2541], D Loss: 0.6503, G Loss: 2.0741\n",
      "Epoch [5/20], Step [1944/2541], D Loss: 0.6508, G Loss: 2.1316\n",
      "Epoch [5/20], Step [1945/2541], D Loss: 0.6507, G Loss: 2.1468\n",
      "Epoch [5/20], Step [1946/2541], D Loss: 0.6503, G Loss: 2.1150\n",
      "Epoch [5/20], Step [1947/2541], D Loss: 0.6504, G Loss: 2.0052\n",
      "Epoch [5/20], Step [1948/2541], D Loss: 0.6505, G Loss: 2.1323\n",
      "Epoch [5/20], Step [1949/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [5/20], Step [1950/2541], D Loss: 0.6502, G Loss: 2.0540\n",
      "Epoch [5/20], Step [1951/2541], D Loss: 0.6502, G Loss: 2.0083\n",
      "Epoch [5/20], Step [1952/2541], D Loss: 0.6504, G Loss: 2.0624\n",
      "Epoch [5/20], Step [1953/2541], D Loss: 0.6502, G Loss: 2.0262\n",
      "Epoch [5/20], Step [1954/2541], D Loss: 0.6503, G Loss: 2.0772\n",
      "Epoch [5/20], Step [1955/2541], D Loss: 0.6503, G Loss: 2.0725\n",
      "Epoch [5/20], Step [1956/2541], D Loss: 0.6503, G Loss: 2.1048\n",
      "Epoch [5/20], Step [1957/2541], D Loss: 0.6503, G Loss: 2.0726\n",
      "Epoch [5/20], Step [1958/2541], D Loss: 0.6503, G Loss: 2.0754\n",
      "Epoch [5/20], Step [1959/2541], D Loss: 0.6503, G Loss: 2.1261\n",
      "Epoch [5/20], Step [1960/2541], D Loss: 0.6505, G Loss: 2.0432\n",
      "Epoch [5/20], Step [1961/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [5/20], Step [1962/2541], D Loss: 0.6507, G Loss: 2.1161\n",
      "Epoch [5/20], Step [1963/2541], D Loss: 0.6507, G Loss: 2.0502\n",
      "Epoch [5/20], Step [1964/2541], D Loss: 0.6503, G Loss: 2.0500\n",
      "Epoch [5/20], Step [1965/2541], D Loss: 0.6503, G Loss: 2.0972\n",
      "Epoch [5/20], Step [1966/2541], D Loss: 0.6504, G Loss: 2.1143\n",
      "Epoch [5/20], Step [1967/2541], D Loss: 0.6502, G Loss: 2.0876\n",
      "Epoch [5/20], Step [1968/2541], D Loss: 0.6503, G Loss: 2.1318\n",
      "Epoch [5/20], Step [1969/2541], D Loss: 0.6504, G Loss: 2.1311\n",
      "Epoch [5/20], Step [1970/2541], D Loss: 0.6504, G Loss: 2.0698\n",
      "Epoch [5/20], Step [1971/2541], D Loss: 0.6503, G Loss: 2.0550\n",
      "Epoch [5/20], Step [1972/2541], D Loss: 0.6503, G Loss: 2.0743\n",
      "Epoch [5/20], Step [1973/2541], D Loss: 0.6504, G Loss: 2.1102\n",
      "Epoch [5/20], Step [1974/2541], D Loss: 0.6502, G Loss: 2.1007\n",
      "Epoch [5/20], Step [1975/2541], D Loss: 0.6502, G Loss: 2.0599\n",
      "Epoch [5/20], Step [1976/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [5/20], Step [1977/2541], D Loss: 0.6503, G Loss: 2.0786\n",
      "Epoch [5/20], Step [1978/2541], D Loss: 0.6503, G Loss: 2.1097\n",
      "Epoch [5/20], Step [1979/2541], D Loss: 0.6502, G Loss: 2.0376\n",
      "Epoch [5/20], Step [1980/2541], D Loss: 0.6503, G Loss: 2.1063\n",
      "Epoch [5/20], Step [1981/2541], D Loss: 0.6502, G Loss: 2.0594\n",
      "Epoch [5/20], Step [1982/2541], D Loss: 0.6504, G Loss: 2.0747\n",
      "Epoch [5/20], Step [1983/2541], D Loss: 0.6504, G Loss: 2.0808\n",
      "Epoch [5/20], Step [1984/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [5/20], Step [1985/2541], D Loss: 0.6502, G Loss: 2.1042\n",
      "Epoch [5/20], Step [1986/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [5/20], Step [1987/2541], D Loss: 0.6509, G Loss: 2.1092\n",
      "Epoch [5/20], Step [1988/2541], D Loss: 0.6504, G Loss: 2.1325\n",
      "Epoch [5/20], Step [1989/2541], D Loss: 0.6507, G Loss: 2.1101\n",
      "Epoch [5/20], Step [1990/2541], D Loss: 0.6503, G Loss: 2.0717\n",
      "Epoch [5/20], Step [1991/2541], D Loss: 0.6503, G Loss: 2.0899\n",
      "Epoch [5/20], Step [1992/2541], D Loss: 0.6502, G Loss: 2.0590\n",
      "Epoch [5/20], Step [1993/2541], D Loss: 0.6503, G Loss: 2.0570\n",
      "Epoch [5/20], Step [1994/2541], D Loss: 0.6509, G Loss: 2.1506\n",
      "Epoch [5/20], Step [1995/2541], D Loss: 0.6504, G Loss: 2.0794\n",
      "Epoch [5/20], Step [1996/2541], D Loss: 0.6506, G Loss: 2.1163\n",
      "Epoch [5/20], Step [1997/2541], D Loss: 0.6505, G Loss: 2.0986\n",
      "Epoch [5/20], Step [1998/2541], D Loss: 0.6507, G Loss: 2.1251\n",
      "Epoch [5/20], Step [1999/2541], D Loss: 0.6503, G Loss: 2.0873\n",
      "Epoch [5/20], Step [2000/2541], D Loss: 0.6506, G Loss: 2.0820\n",
      "Epoch [5/20], Step [2001/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [5/20], Step [2002/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [5/20], Step [2003/2541], D Loss: 0.6503, G Loss: 2.1027\n",
      "Epoch [5/20], Step [2004/2541], D Loss: 0.6502, G Loss: 2.1124\n",
      "Epoch [5/20], Step [2005/2541], D Loss: 0.6503, G Loss: 2.0912\n",
      "Epoch [5/20], Step [2006/2541], D Loss: 0.6502, G Loss: 2.1086\n",
      "Epoch [5/20], Step [2007/2541], D Loss: 0.6503, G Loss: 2.0755\n",
      "Epoch [5/20], Step [2008/2541], D Loss: 0.6502, G Loss: 2.0591\n",
      "Epoch [5/20], Step [2009/2541], D Loss: 0.6509, G Loss: 2.0203\n",
      "Epoch [5/20], Step [2010/2541], D Loss: 0.6513, G Loss: 2.1445\n",
      "Epoch [5/20], Step [2011/2541], D Loss: 0.6506, G Loss: 2.0756\n",
      "Epoch [5/20], Step [2012/2541], D Loss: 0.6503, G Loss: 2.1362\n",
      "Epoch [5/20], Step [2013/2541], D Loss: 0.6502, G Loss: 2.0678\n",
      "Epoch [5/20], Step [2014/2541], D Loss: 0.6504, G Loss: 2.0460\n",
      "Epoch [5/20], Step [2015/2541], D Loss: 0.6507, G Loss: 2.0533\n",
      "Epoch [5/20], Step [2016/2541], D Loss: 0.6503, G Loss: 2.0752\n",
      "Epoch [5/20], Step [2017/2541], D Loss: 0.6513, G Loss: 2.0946\n",
      "Epoch [5/20], Step [2018/2541], D Loss: 0.6513, G Loss: 2.0730\n",
      "Epoch [5/20], Step [2019/2541], D Loss: 0.6505, G Loss: 2.0539\n",
      "Epoch [5/20], Step [2020/2541], D Loss: 0.6505, G Loss: 2.0829\n",
      "Epoch [5/20], Step [2021/2541], D Loss: 0.6505, G Loss: 2.0933\n",
      "Epoch [5/20], Step [2022/2541], D Loss: 0.6503, G Loss: 2.0984\n",
      "Epoch [5/20], Step [2023/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [5/20], Step [2024/2541], D Loss: 0.6502, G Loss: 2.0340\n",
      "Epoch [5/20], Step [2025/2541], D Loss: 0.6502, G Loss: 2.0435\n",
      "Epoch [5/20], Step [2026/2541], D Loss: 0.6506, G Loss: 2.0632\n",
      "Epoch [5/20], Step [2027/2541], D Loss: 0.6514, G Loss: 2.0543\n",
      "Epoch [5/20], Step [2028/2541], D Loss: 0.6505, G Loss: 2.1312\n",
      "Epoch [5/20], Step [2029/2541], D Loss: 0.6503, G Loss: 2.1061\n",
      "Epoch [5/20], Step [2030/2541], D Loss: 0.6504, G Loss: 2.0693\n",
      "Epoch [5/20], Step [2031/2541], D Loss: 0.6503, G Loss: 2.0847\n",
      "Epoch [5/20], Step [2032/2541], D Loss: 0.6505, G Loss: 2.0818\n",
      "Epoch [5/20], Step [2033/2541], D Loss: 0.6502, G Loss: 2.0588\n",
      "Epoch [5/20], Step [2034/2541], D Loss: 0.6503, G Loss: 2.1168\n",
      "Epoch [5/20], Step [2035/2541], D Loss: 0.6504, G Loss: 2.1053\n",
      "Epoch [5/20], Step [2036/2541], D Loss: 0.6505, G Loss: 2.0828\n",
      "Epoch [5/20], Step [2037/2541], D Loss: 0.6502, G Loss: 2.0621\n",
      "Epoch [5/20], Step [2038/2541], D Loss: 0.6503, G Loss: 2.0240\n",
      "Epoch [5/20], Step [2039/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [5/20], Step [2040/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [5/20], Step [2041/2541], D Loss: 0.6503, G Loss: 2.1190\n",
      "Epoch [5/20], Step [2042/2541], D Loss: 0.6502, G Loss: 2.1238\n",
      "Epoch [5/20], Step [2043/2541], D Loss: 0.6503, G Loss: 2.1287\n",
      "Epoch [5/20], Step [2044/2541], D Loss: 0.6504, G Loss: 2.0412\n",
      "Epoch [5/20], Step [2045/2541], D Loss: 0.6504, G Loss: 2.0881\n",
      "Epoch [5/20], Step [2046/2541], D Loss: 0.6503, G Loss: 2.1028\n",
      "Epoch [5/20], Step [2047/2541], D Loss: 0.6503, G Loss: 2.0789\n",
      "Epoch [5/20], Step [2048/2541], D Loss: 0.6503, G Loss: 2.1013\n",
      "Epoch [5/20], Step [2049/2541], D Loss: 0.6502, G Loss: 2.0099\n",
      "Epoch [5/20], Step [2050/2541], D Loss: 0.6502, G Loss: 2.1221\n",
      "Epoch [5/20], Step [2051/2541], D Loss: 0.6503, G Loss: 2.0607\n",
      "Epoch [5/20], Step [2052/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [5/20], Step [2053/2541], D Loss: 0.6505, G Loss: 2.0634\n",
      "Epoch [5/20], Step [2054/2541], D Loss: 0.6505, G Loss: 2.0946\n",
      "Epoch [5/20], Step [2055/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [5/20], Step [2056/2541], D Loss: 0.6505, G Loss: 2.0904\n",
      "Epoch [5/20], Step [2057/2541], D Loss: 0.6503, G Loss: 2.0638\n",
      "Epoch [5/20], Step [2058/2541], D Loss: 0.6503, G Loss: 2.0702\n",
      "Epoch [5/20], Step [2059/2541], D Loss: 0.6506, G Loss: 2.1502\n",
      "Epoch [5/20], Step [2060/2541], D Loss: 0.6509, G Loss: 2.0620\n",
      "Epoch [5/20], Step [2061/2541], D Loss: 0.6503, G Loss: 2.0773\n",
      "Epoch [5/20], Step [2062/2541], D Loss: 0.6502, G Loss: 2.0709\n",
      "Epoch [5/20], Step [2063/2541], D Loss: 0.6502, G Loss: 2.0518\n",
      "Epoch [5/20], Step [2064/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [5/20], Step [2065/2541], D Loss: 0.6503, G Loss: 2.1102\n",
      "Epoch [5/20], Step [2066/2541], D Loss: 0.6504, G Loss: 2.0908\n",
      "Epoch [5/20], Step [2067/2541], D Loss: 0.6502, G Loss: 2.0492\n",
      "Epoch [5/20], Step [2068/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [5/20], Step [2069/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [5/20], Step [2070/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [5/20], Step [2071/2541], D Loss: 0.6503, G Loss: 2.0752\n",
      "Epoch [5/20], Step [2072/2541], D Loss: 0.6506, G Loss: 2.1324\n",
      "Epoch [5/20], Step [2073/2541], D Loss: 0.6504, G Loss: 2.0968\n",
      "Epoch [5/20], Step [2074/2541], D Loss: 0.6502, G Loss: 2.0953\n",
      "Epoch [5/20], Step [2075/2541], D Loss: 0.6502, G Loss: 2.0271\n",
      "Epoch [5/20], Step [2076/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [5/20], Step [2077/2541], D Loss: 0.6502, G Loss: 2.1048\n",
      "Epoch [5/20], Step [2078/2541], D Loss: 0.6502, G Loss: 2.1068\n",
      "Epoch [5/20], Step [2079/2541], D Loss: 0.6503, G Loss: 2.0951\n",
      "Epoch [5/20], Step [2080/2541], D Loss: 0.6502, G Loss: 2.0601\n",
      "Epoch [5/20], Step [2081/2541], D Loss: 0.6502, G Loss: 2.0639\n",
      "Epoch [5/20], Step [2082/2541], D Loss: 0.6503, G Loss: 2.0807\n",
      "Epoch [5/20], Step [2083/2541], D Loss: 0.6502, G Loss: 2.1004\n",
      "Epoch [5/20], Step [2084/2541], D Loss: 0.6503, G Loss: 2.1050\n",
      "Epoch [5/20], Step [2085/2541], D Loss: 0.6502, G Loss: 2.0658\n",
      "Epoch [5/20], Step [2086/2541], D Loss: 0.6503, G Loss: 2.0637\n",
      "Epoch [5/20], Step [2087/2541], D Loss: 0.6503, G Loss: 1.9987\n",
      "Epoch [5/20], Step [2088/2541], D Loss: 0.6502, G Loss: 2.0604\n",
      "Epoch [5/20], Step [2089/2541], D Loss: 0.6503, G Loss: 2.1316\n",
      "Epoch [5/20], Step [2090/2541], D Loss: 0.6505, G Loss: 2.0955\n",
      "Epoch [5/20], Step [2091/2541], D Loss: 0.6507, G Loss: 2.0932\n",
      "Epoch [5/20], Step [2092/2541], D Loss: 0.6503, G Loss: 2.0762\n",
      "Epoch [5/20], Step [2093/2541], D Loss: 0.6503, G Loss: 2.0450\n",
      "Epoch [5/20], Step [2094/2541], D Loss: 0.6503, G Loss: 2.0840\n",
      "Epoch [5/20], Step [2095/2541], D Loss: 0.6503, G Loss: 2.0501\n",
      "Epoch [5/20], Step [2096/2541], D Loss: 0.6502, G Loss: 2.0560\n",
      "Epoch [5/20], Step [2097/2541], D Loss: 0.6503, G Loss: 2.1146\n",
      "Epoch [5/20], Step [2098/2541], D Loss: 0.6506, G Loss: 2.1040\n",
      "Epoch [5/20], Step [2099/2541], D Loss: 0.6503, G Loss: 2.1685\n",
      "Epoch [5/20], Step [2100/2541], D Loss: 0.6506, G Loss: 2.1072\n",
      "Epoch [5/20], Step [2101/2541], D Loss: 0.6503, G Loss: 2.0467\n",
      "Epoch [5/20], Step [2102/2541], D Loss: 0.6504, G Loss: 2.0862\n",
      "Epoch [5/20], Step [2103/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [5/20], Step [2104/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [5/20], Step [2105/2541], D Loss: 0.6502, G Loss: 2.0576\n",
      "Epoch [5/20], Step [2106/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [5/20], Step [2107/2541], D Loss: 0.6504, G Loss: 2.0862\n",
      "Epoch [5/20], Step [2108/2541], D Loss: 0.6502, G Loss: 2.0645\n",
      "Epoch [5/20], Step [2109/2541], D Loss: 0.6503, G Loss: 2.1297\n",
      "Epoch [5/20], Step [2110/2541], D Loss: 0.6503, G Loss: 2.0987\n",
      "Epoch [5/20], Step [2111/2541], D Loss: 0.6502, G Loss: 2.1606\n",
      "Epoch [5/20], Step [2112/2541], D Loss: 0.6505, G Loss: 2.1146\n",
      "Epoch [5/20], Step [2113/2541], D Loss: 0.6538, G Loss: 2.1712\n",
      "Epoch [5/20], Step [2114/2541], D Loss: 0.6545, G Loss: 2.0337\n",
      "Epoch [5/20], Step [2115/2541], D Loss: 0.6509, G Loss: 1.9892\n",
      "Epoch [5/20], Step [2116/2541], D Loss: 0.6513, G Loss: 2.1250\n",
      "Epoch [5/20], Step [2117/2541], D Loss: 0.6506, G Loss: 2.1777\n",
      "Epoch [5/20], Step [2118/2541], D Loss: 0.6513, G Loss: 2.0829\n",
      "Epoch [5/20], Step [2119/2541], D Loss: 0.6504, G Loss: 2.0338\n",
      "Epoch [5/20], Step [2120/2541], D Loss: 0.6509, G Loss: 2.1072\n",
      "Epoch [5/20], Step [2121/2541], D Loss: 0.6504, G Loss: 2.1002\n",
      "Epoch [5/20], Step [2122/2541], D Loss: 0.6503, G Loss: 2.0678\n",
      "Epoch [5/20], Step [2123/2541], D Loss: 0.6503, G Loss: 2.0744\n",
      "Epoch [5/20], Step [2124/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [5/20], Step [2125/2541], D Loss: 0.6504, G Loss: 2.1174\n",
      "Epoch [5/20], Step [2126/2541], D Loss: 0.6504, G Loss: 2.0849\n",
      "Epoch [5/20], Step [2127/2541], D Loss: 0.6506, G Loss: 2.0933\n",
      "Epoch [5/20], Step [2128/2541], D Loss: 0.6507, G Loss: 2.0889\n",
      "Epoch [5/20], Step [2129/2541], D Loss: 0.6503, G Loss: 2.0509\n",
      "Epoch [5/20], Step [2130/2541], D Loss: 0.6503, G Loss: 2.1272\n",
      "Epoch [5/20], Step [2131/2541], D Loss: 0.6506, G Loss: 2.0835\n",
      "Epoch [5/20], Step [2132/2541], D Loss: 0.6503, G Loss: 2.0886\n",
      "Epoch [5/20], Step [2133/2541], D Loss: 0.6504, G Loss: 2.0920\n",
      "Epoch [5/20], Step [2134/2541], D Loss: 0.6502, G Loss: 2.0898\n",
      "Epoch [5/20], Step [2135/2541], D Loss: 0.6502, G Loss: 2.0562\n",
      "Epoch [5/20], Step [2136/2541], D Loss: 0.6502, G Loss: 2.1058\n",
      "Epoch [5/20], Step [2137/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [5/20], Step [2138/2541], D Loss: 0.6502, G Loss: 2.0672\n",
      "Epoch [5/20], Step [2139/2541], D Loss: 0.6505, G Loss: 2.0629\n",
      "Epoch [5/20], Step [2140/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [5/20], Step [2141/2541], D Loss: 0.6503, G Loss: 2.0740\n",
      "Epoch [5/20], Step [2142/2541], D Loss: 0.6502, G Loss: 2.0166\n",
      "Epoch [5/20], Step [2143/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [5/20], Step [2144/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [5/20], Step [2145/2541], D Loss: 0.6502, G Loss: 2.0562\n",
      "Epoch [5/20], Step [2146/2541], D Loss: 0.6504, G Loss: 2.0714\n",
      "Epoch [5/20], Step [2147/2541], D Loss: 0.6503, G Loss: 2.0678\n",
      "Epoch [5/20], Step [2148/2541], D Loss: 0.6502, G Loss: 2.0516\n",
      "Epoch [5/20], Step [2149/2541], D Loss: 0.6504, G Loss: 2.0993\n",
      "Epoch [5/20], Step [2150/2541], D Loss: 0.6503, G Loss: 2.1392\n",
      "Epoch [5/20], Step [2151/2541], D Loss: 0.6506, G Loss: 2.1033\n",
      "Epoch [5/20], Step [2152/2541], D Loss: 0.6503, G Loss: 2.0562\n",
      "Epoch [5/20], Step [2153/2541], D Loss: 0.6516, G Loss: 2.1578\n",
      "Epoch [5/20], Step [2154/2541], D Loss: 0.6507, G Loss: 2.0952\n",
      "Epoch [5/20], Step [2155/2541], D Loss: 0.6504, G Loss: 2.0628\n",
      "Epoch [5/20], Step [2156/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [5/20], Step [2157/2541], D Loss: 0.6502, G Loss: 2.1012\n",
      "Epoch [5/20], Step [2158/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [5/20], Step [2159/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [5/20], Step [2160/2541], D Loss: 0.6505, G Loss: 2.0875\n",
      "Epoch [5/20], Step [2161/2541], D Loss: 0.6502, G Loss: 2.1134\n",
      "Epoch [5/20], Step [2162/2541], D Loss: 0.6503, G Loss: 2.0394\n",
      "Epoch [5/20], Step [2163/2541], D Loss: 0.6503, G Loss: 2.0691\n",
      "Epoch [5/20], Step [2164/2541], D Loss: 0.6502, G Loss: 2.0390\n",
      "Epoch [5/20], Step [2165/2541], D Loss: 0.6503, G Loss: 2.0998\n",
      "Epoch [5/20], Step [2166/2541], D Loss: 0.6504, G Loss: 2.1031\n",
      "Epoch [5/20], Step [2167/2541], D Loss: 0.6503, G Loss: 2.0822\n",
      "Epoch [5/20], Step [2168/2541], D Loss: 0.6502, G Loss: 2.1018\n",
      "Epoch [5/20], Step [2169/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [5/20], Step [2170/2541], D Loss: 0.6502, G Loss: 2.0688\n",
      "Epoch [5/20], Step [2171/2541], D Loss: 0.6507, G Loss: 2.1168\n",
      "Epoch [5/20], Step [2172/2541], D Loss: 0.6504, G Loss: 2.1201\n",
      "Epoch [5/20], Step [2173/2541], D Loss: 0.6505, G Loss: 2.0848\n",
      "Epoch [5/20], Step [2174/2541], D Loss: 0.6503, G Loss: 2.0659\n",
      "Epoch [5/20], Step [2175/2541], D Loss: 0.6504, G Loss: 2.0571\n",
      "Epoch [5/20], Step [2176/2541], D Loss: 0.6505, G Loss: 2.1299\n",
      "Epoch [5/20], Step [2177/2541], D Loss: 0.6507, G Loss: 2.1058\n",
      "Epoch [5/20], Step [2178/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [5/20], Step [2179/2541], D Loss: 0.6502, G Loss: 2.0596\n",
      "Epoch [5/20], Step [2180/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [5/20], Step [2181/2541], D Loss: 0.6503, G Loss: 2.0969\n",
      "Epoch [5/20], Step [2182/2541], D Loss: 0.6502, G Loss: 2.1058\n",
      "Epoch [5/20], Step [2183/2541], D Loss: 0.6502, G Loss: 2.1240\n",
      "Epoch [5/20], Step [2184/2541], D Loss: 0.6502, G Loss: 2.1265\n",
      "Epoch [5/20], Step [2185/2541], D Loss: 0.6502, G Loss: 2.1160\n",
      "Epoch [5/20], Step [2186/2541], D Loss: 0.6503, G Loss: 2.0889\n",
      "Epoch [5/20], Step [2187/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [5/20], Step [2188/2541], D Loss: 0.6503, G Loss: 2.0694\n",
      "Epoch [5/20], Step [2189/2541], D Loss: 0.6503, G Loss: 2.0736\n",
      "Epoch [5/20], Step [2190/2541], D Loss: 0.6503, G Loss: 2.1113\n",
      "Epoch [5/20], Step [2191/2541], D Loss: 0.6502, G Loss: 2.1144\n",
      "Epoch [5/20], Step [2192/2541], D Loss: 0.6510, G Loss: 2.0863\n",
      "Epoch [5/20], Step [2193/2541], D Loss: 0.6506, G Loss: 2.0570\n",
      "Epoch [5/20], Step [2194/2541], D Loss: 0.6503, G Loss: 2.0798\n",
      "Epoch [5/20], Step [2195/2541], D Loss: 0.6503, G Loss: 2.0933\n",
      "Epoch [5/20], Step [2196/2541], D Loss: 0.6502, G Loss: 2.1043\n",
      "Epoch [5/20], Step [2197/2541], D Loss: 0.6502, G Loss: 2.1402\n",
      "Epoch [5/20], Step [2198/2541], D Loss: 0.6503, G Loss: 2.1194\n",
      "Epoch [5/20], Step [2199/2541], D Loss: 0.6502, G Loss: 2.0363\n",
      "Epoch [5/20], Step [2200/2541], D Loss: 0.6508, G Loss: 2.1488\n",
      "Epoch [5/20], Step [2201/2541], D Loss: 0.6503, G Loss: 2.1376\n",
      "Epoch [5/20], Step [2202/2541], D Loss: 0.6507, G Loss: 2.1108\n",
      "Epoch [5/20], Step [2203/2541], D Loss: 0.6503, G Loss: 2.0794\n",
      "Epoch [5/20], Step [2204/2541], D Loss: 0.6503, G Loss: 2.0835\n",
      "Epoch [5/20], Step [2205/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [5/20], Step [2206/2541], D Loss: 0.6503, G Loss: 2.0859\n",
      "Epoch [5/20], Step [2207/2541], D Loss: 0.6503, G Loss: 2.0959\n",
      "Epoch [5/20], Step [2208/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [5/20], Step [2209/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [5/20], Step [2210/2541], D Loss: 0.6506, G Loss: 2.1240\n",
      "Epoch [5/20], Step [2211/2541], D Loss: 0.6503, G Loss: 2.0914\n",
      "Epoch [5/20], Step [2212/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [5/20], Step [2213/2541], D Loss: 0.6503, G Loss: 2.0965\n",
      "Epoch [5/20], Step [2214/2541], D Loss: 0.6503, G Loss: 2.1077\n",
      "Epoch [5/20], Step [2215/2541], D Loss: 0.6502, G Loss: 2.0977\n",
      "Epoch [5/20], Step [2216/2541], D Loss: 0.6503, G Loss: 2.1056\n",
      "Epoch [5/20], Step [2217/2541], D Loss: 0.6503, G Loss: 2.0917\n",
      "Epoch [5/20], Step [2218/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [5/20], Step [2219/2541], D Loss: 0.6503, G Loss: 2.1053\n",
      "Epoch [5/20], Step [2220/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [5/20], Step [2221/2541], D Loss: 0.6506, G Loss: 2.0716\n",
      "Epoch [5/20], Step [2222/2541], D Loss: 0.6506, G Loss: 2.0278\n",
      "Epoch [5/20], Step [2223/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [5/20], Step [2224/2541], D Loss: 0.6505, G Loss: 2.1423\n",
      "Epoch [5/20], Step [2225/2541], D Loss: 0.6505, G Loss: 2.1029\n",
      "Epoch [5/20], Step [2226/2541], D Loss: 0.6503, G Loss: 2.0783\n",
      "Epoch [5/20], Step [2227/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [5/20], Step [2228/2541], D Loss: 0.6504, G Loss: 2.0662\n",
      "Epoch [5/20], Step [2229/2541], D Loss: 0.6502, G Loss: 2.0596\n",
      "Epoch [5/20], Step [2230/2541], D Loss: 0.6502, G Loss: 2.0702\n",
      "Epoch [5/20], Step [2231/2541], D Loss: 0.6504, G Loss: 2.0831\n",
      "Epoch [5/20], Step [2232/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [5/20], Step [2233/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [5/20], Step [2234/2541], D Loss: 0.6502, G Loss: 2.1032\n",
      "Epoch [5/20], Step [2235/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [5/20], Step [2236/2541], D Loss: 0.6502, G Loss: 2.0763\n",
      "Epoch [5/20], Step [2237/2541], D Loss: 0.6503, G Loss: 2.1050\n",
      "Epoch [5/20], Step [2238/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [5/20], Step [2239/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [5/20], Step [2240/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [5/20], Step [2241/2541], D Loss: 0.6503, G Loss: 2.0708\n",
      "Epoch [5/20], Step [2242/2541], D Loss: 0.6504, G Loss: 2.1281\n",
      "Epoch [5/20], Step [2243/2541], D Loss: 0.6503, G Loss: 2.1200\n",
      "Epoch [5/20], Step [2244/2541], D Loss: 0.6503, G Loss: 2.0639\n",
      "Epoch [5/20], Step [2245/2541], D Loss: 0.6502, G Loss: 2.0650\n",
      "Epoch [5/20], Step [2246/2541], D Loss: 0.6502, G Loss: 2.0989\n",
      "Epoch [5/20], Step [2247/2541], D Loss: 0.6502, G Loss: 2.0649\n",
      "Epoch [5/20], Step [2248/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [5/20], Step [2249/2541], D Loss: 0.6502, G Loss: 2.0623\n",
      "Epoch [5/20], Step [2250/2541], D Loss: 0.6502, G Loss: 2.0990\n",
      "Epoch [5/20], Step [2251/2541], D Loss: 0.6502, G Loss: 2.0351\n",
      "Epoch [5/20], Step [2252/2541], D Loss: 0.6502, G Loss: 2.0656\n",
      "Epoch [5/20], Step [2253/2541], D Loss: 0.6502, G Loss: 2.1209\n",
      "Epoch [5/20], Step [2254/2541], D Loss: 0.6503, G Loss: 2.0583\n",
      "Epoch [5/20], Step [2255/2541], D Loss: 0.6502, G Loss: 2.0647\n",
      "Epoch [5/20], Step [2256/2541], D Loss: 0.6502, G Loss: 2.0594\n",
      "Epoch [5/20], Step [2257/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [5/20], Step [2258/2541], D Loss: 0.6503, G Loss: 2.1101\n",
      "Epoch [5/20], Step [2259/2541], D Loss: 0.6503, G Loss: 2.1086\n",
      "Epoch [5/20], Step [2260/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [5/20], Step [2261/2541], D Loss: 0.6503, G Loss: 2.0684\n",
      "Epoch [5/20], Step [2262/2541], D Loss: 0.6503, G Loss: 2.0811\n",
      "Epoch [5/20], Step [2263/2541], D Loss: 0.6504, G Loss: 2.1116\n",
      "Epoch [5/20], Step [2264/2541], D Loss: 0.6502, G Loss: 2.0712\n",
      "Epoch [5/20], Step [2265/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [5/20], Step [2266/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [5/20], Step [2267/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [5/20], Step [2268/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [5/20], Step [2269/2541], D Loss: 0.6502, G Loss: 2.1159\n",
      "Epoch [5/20], Step [2270/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [5/20], Step [2271/2541], D Loss: 0.6502, G Loss: 2.0613\n",
      "Epoch [5/20], Step [2272/2541], D Loss: 0.6502, G Loss: 2.0652\n",
      "Epoch [5/20], Step [2273/2541], D Loss: 0.6502, G Loss: 2.0717\n",
      "Epoch [5/20], Step [2274/2541], D Loss: 0.6502, G Loss: 2.0452\n",
      "Epoch [5/20], Step [2275/2541], D Loss: 0.6502, G Loss: 2.0909\n",
      "Epoch [5/20], Step [2276/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [5/20], Step [2277/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [5/20], Step [2278/2541], D Loss: 0.6502, G Loss: 2.0988\n",
      "Epoch [5/20], Step [2279/2541], D Loss: 0.6502, G Loss: 2.0717\n",
      "Epoch [5/20], Step [2280/2541], D Loss: 0.6502, G Loss: 2.1027\n",
      "Epoch [5/20], Step [2281/2541], D Loss: 0.6503, G Loss: 2.0842\n",
      "Epoch [5/20], Step [2282/2541], D Loss: 0.6502, G Loss: 2.0670\n",
      "Epoch [5/20], Step [2283/2541], D Loss: 0.6502, G Loss: 2.0636\n",
      "Epoch [5/20], Step [2284/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [5/20], Step [2285/2541], D Loss: 0.6503, G Loss: 2.1040\n",
      "Epoch [5/20], Step [2286/2541], D Loss: 0.6503, G Loss: 2.1051\n",
      "Epoch [5/20], Step [2287/2541], D Loss: 0.6502, G Loss: 2.1095\n",
      "Epoch [5/20], Step [2288/2541], D Loss: 0.6503, G Loss: 2.0772\n",
      "Epoch [5/20], Step [2289/2541], D Loss: 0.6502, G Loss: 2.0716\n",
      "Epoch [5/20], Step [2290/2541], D Loss: 0.6503, G Loss: 2.1091\n",
      "Epoch [5/20], Step [2291/2541], D Loss: 0.6502, G Loss: 2.0963\n",
      "Epoch [5/20], Step [2292/2541], D Loss: 0.6502, G Loss: 2.1105\n",
      "Epoch [5/20], Step [2293/2541], D Loss: 0.6502, G Loss: 2.0941\n",
      "Epoch [5/20], Step [2294/2541], D Loss: 0.6503, G Loss: 2.1041\n",
      "Epoch [5/20], Step [2295/2541], D Loss: 0.6503, G Loss: 2.0986\n",
      "Epoch [5/20], Step [2296/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [5/20], Step [2297/2541], D Loss: 0.6503, G Loss: 2.0724\n",
      "Epoch [5/20], Step [2298/2541], D Loss: 0.6503, G Loss: 2.0698\n",
      "Epoch [5/20], Step [2299/2541], D Loss: 0.6502, G Loss: 2.1080\n",
      "Epoch [5/20], Step [2300/2541], D Loss: 0.6502, G Loss: 2.0927\n",
      "Epoch [5/20], Step [2301/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [5/20], Step [2302/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [5/20], Step [2303/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [5/20], Step [2304/2541], D Loss: 0.6507, G Loss: 2.1400\n",
      "Epoch [5/20], Step [2305/2541], D Loss: 0.6505, G Loss: 2.1036\n",
      "Epoch [5/20], Step [2306/2541], D Loss: 0.6504, G Loss: 2.0786\n",
      "Epoch [5/20], Step [2307/2541], D Loss: 0.6503, G Loss: 2.0644\n",
      "Epoch [5/20], Step [2308/2541], D Loss: 0.6506, G Loss: 2.0683\n",
      "Epoch [5/20], Step [2309/2541], D Loss: 0.6504, G Loss: 2.0379\n",
      "Epoch [5/20], Step [2310/2541], D Loss: 0.6503, G Loss: 2.0896\n",
      "Epoch [5/20], Step [2311/2541], D Loss: 0.6502, G Loss: 2.0959\n",
      "Epoch [5/20], Step [2312/2541], D Loss: 0.6502, G Loss: 2.1082\n",
      "Epoch [5/20], Step [2313/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [5/20], Step [2314/2541], D Loss: 0.6502, G Loss: 2.0676\n",
      "Epoch [5/20], Step [2315/2541], D Loss: 0.6503, G Loss: 2.0386\n",
      "Epoch [5/20], Step [2316/2541], D Loss: 0.6503, G Loss: 2.0920\n",
      "Epoch [5/20], Step [2317/2541], D Loss: 0.6505, G Loss: 2.0694\n",
      "Epoch [5/20], Step [2318/2541], D Loss: 0.6502, G Loss: 2.0734\n",
      "Epoch [5/20], Step [2319/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [5/20], Step [2320/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [5/20], Step [2321/2541], D Loss: 0.6503, G Loss: 2.0824\n",
      "Epoch [5/20], Step [2322/2541], D Loss: 0.6503, G Loss: 2.0898\n",
      "Epoch [5/20], Step [2323/2541], D Loss: 0.6504, G Loss: 2.1061\n",
      "Epoch [5/20], Step [2324/2541], D Loss: 0.6503, G Loss: 2.0767\n",
      "Epoch [5/20], Step [2325/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [5/20], Step [2326/2541], D Loss: 0.6503, G Loss: 2.1101\n",
      "Epoch [5/20], Step [2327/2541], D Loss: 0.6503, G Loss: 2.1331\n",
      "Epoch [5/20], Step [2328/2541], D Loss: 0.6503, G Loss: 2.1219\n",
      "Epoch [5/20], Step [2329/2541], D Loss: 0.6503, G Loss: 2.0628\n",
      "Epoch [5/20], Step [2330/2541], D Loss: 0.6502, G Loss: 2.0673\n",
      "Epoch [5/20], Step [2331/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [5/20], Step [2332/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [5/20], Step [2333/2541], D Loss: 0.6502, G Loss: 2.0964\n",
      "Epoch [5/20], Step [2334/2541], D Loss: 0.6502, G Loss: 2.0620\n",
      "Epoch [5/20], Step [2335/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [5/20], Step [2336/2541], D Loss: 0.6504, G Loss: 2.1231\n",
      "Epoch [5/20], Step [2337/2541], D Loss: 0.6504, G Loss: 2.1021\n",
      "Epoch [5/20], Step [2338/2541], D Loss: 0.6503, G Loss: 2.0783\n",
      "Epoch [5/20], Step [2339/2541], D Loss: 0.6502, G Loss: 2.0811\n",
      "Epoch [5/20], Step [2340/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [5/20], Step [2341/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [5/20], Step [2342/2541], D Loss: 0.6502, G Loss: 2.0969\n",
      "Epoch [5/20], Step [2343/2541], D Loss: 0.6502, G Loss: 2.0675\n",
      "Epoch [5/20], Step [2344/2541], D Loss: 0.6503, G Loss: 2.1140\n",
      "Epoch [5/20], Step [2345/2541], D Loss: 0.6503, G Loss: 2.1029\n",
      "Epoch [5/20], Step [2346/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [5/20], Step [2347/2541], D Loss: 0.6503, G Loss: 2.0570\n",
      "Epoch [5/20], Step [2348/2541], D Loss: 0.6502, G Loss: 2.0508\n",
      "Epoch [5/20], Step [2349/2541], D Loss: 0.6503, G Loss: 2.1202\n",
      "Epoch [5/20], Step [2350/2541], D Loss: 0.6503, G Loss: 2.0764\n",
      "Epoch [5/20], Step [2351/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [5/20], Step [2352/2541], D Loss: 0.6502, G Loss: 2.0627\n",
      "Epoch [5/20], Step [2353/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [5/20], Step [2354/2541], D Loss: 0.6502, G Loss: 2.1044\n",
      "Epoch [5/20], Step [2355/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [5/20], Step [2356/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [5/20], Step [2357/2541], D Loss: 0.6503, G Loss: 2.0813\n",
      "Epoch [5/20], Step [2358/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [5/20], Step [2359/2541], D Loss: 0.6503, G Loss: 2.0839\n",
      "Epoch [5/20], Step [2360/2541], D Loss: 0.6502, G Loss: 2.0699\n",
      "Epoch [5/20], Step [2361/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [5/20], Step [2362/2541], D Loss: 0.6502, G Loss: 2.0921\n",
      "Epoch [5/20], Step [2363/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [5/20], Step [2364/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [5/20], Step [2365/2541], D Loss: 0.6502, G Loss: 2.0665\n",
      "Epoch [5/20], Step [2366/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [5/20], Step [2367/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [5/20], Step [2368/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [5/20], Step [2369/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [5/20], Step [2370/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [5/20], Step [2371/2541], D Loss: 0.6506, G Loss: 2.1353\n",
      "Epoch [5/20], Step [2372/2541], D Loss: 0.6512, G Loss: 2.0669\n",
      "Epoch [5/20], Step [2373/2541], D Loss: 0.6504, G Loss: 2.0722\n",
      "Epoch [5/20], Step [2374/2541], D Loss: 0.6504, G Loss: 2.0853\n",
      "Epoch [5/20], Step [2375/2541], D Loss: 0.6503, G Loss: 2.1000\n",
      "Epoch [5/20], Step [2376/2541], D Loss: 0.6504, G Loss: 2.0476\n",
      "Epoch [5/20], Step [2377/2541], D Loss: 0.6502, G Loss: 2.0711\n",
      "Epoch [5/20], Step [2378/2541], D Loss: 0.6503, G Loss: 2.1030\n",
      "Epoch [5/20], Step [2379/2541], D Loss: 0.6504, G Loss: 2.0961\n",
      "Epoch [5/20], Step [2380/2541], D Loss: 0.6503, G Loss: 2.1012\n",
      "Epoch [5/20], Step [2381/2541], D Loss: 0.6503, G Loss: 2.0931\n",
      "Epoch [5/20], Step [2382/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [5/20], Step [2383/2541], D Loss: 0.6502, G Loss: 2.0634\n",
      "Epoch [5/20], Step [2384/2541], D Loss: 0.6502, G Loss: 2.0893\n",
      "Epoch [5/20], Step [2385/2541], D Loss: 0.6502, G Loss: 2.0589\n",
      "Epoch [5/20], Step [2386/2541], D Loss: 0.6503, G Loss: 2.0946\n",
      "Epoch [5/20], Step [2387/2541], D Loss: 0.6502, G Loss: 2.0633\n",
      "Epoch [5/20], Step [2388/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [5/20], Step [2389/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [5/20], Step [2390/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [5/20], Step [2391/2541], D Loss: 0.6502, G Loss: 2.1399\n",
      "Epoch [5/20], Step [2392/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [5/20], Step [2393/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [5/20], Step [2394/2541], D Loss: 0.6502, G Loss: 2.1003\n",
      "Epoch [5/20], Step [2395/2541], D Loss: 0.6502, G Loss: 2.0652\n",
      "Epoch [5/20], Step [2396/2541], D Loss: 0.6504, G Loss: 2.1040\n",
      "Epoch [5/20], Step [2397/2541], D Loss: 0.6502, G Loss: 2.1126\n",
      "Epoch [5/20], Step [2398/2541], D Loss: 0.6503, G Loss: 2.0699\n",
      "Epoch [5/20], Step [2399/2541], D Loss: 0.6502, G Loss: 2.0570\n",
      "Epoch [5/20], Step [2400/2541], D Loss: 0.6505, G Loss: 2.1091\n",
      "Epoch [5/20], Step [2401/2541], D Loss: 0.6502, G Loss: 2.1081\n",
      "Epoch [5/20], Step [2402/2541], D Loss: 0.6502, G Loss: 2.0673\n",
      "Epoch [5/20], Step [2403/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [5/20], Step [2404/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [5/20], Step [2405/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [5/20], Step [2406/2541], D Loss: 0.6502, G Loss: 2.1033\n",
      "Epoch [5/20], Step [2407/2541], D Loss: 0.6502, G Loss: 2.0651\n",
      "Epoch [5/20], Step [2408/2541], D Loss: 0.6502, G Loss: 2.0608\n",
      "Epoch [5/20], Step [2409/2541], D Loss: 0.6502, G Loss: 2.0956\n",
      "Epoch [5/20], Step [2410/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [5/20], Step [2411/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [5/20], Step [2412/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [5/20], Step [2413/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [5/20], Step [2414/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [5/20], Step [2415/2541], D Loss: 0.6506, G Loss: 2.1116\n",
      "Epoch [5/20], Step [2416/2541], D Loss: 0.6503, G Loss: 2.1096\n",
      "Epoch [5/20], Step [2417/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [5/20], Step [2418/2541], D Loss: 0.6504, G Loss: 2.0667\n",
      "Epoch [5/20], Step [2419/2541], D Loss: 0.6503, G Loss: 2.0450\n",
      "Epoch [5/20], Step [2420/2541], D Loss: 0.6503, G Loss: 2.0872\n",
      "Epoch [5/20], Step [2421/2541], D Loss: 0.6502, G Loss: 2.0730\n",
      "Epoch [5/20], Step [2422/2541], D Loss: 0.6502, G Loss: 2.0619\n",
      "Epoch [5/20], Step [2423/2541], D Loss: 0.6502, G Loss: 2.0635\n",
      "Epoch [5/20], Step [2424/2541], D Loss: 0.6503, G Loss: 2.1017\n",
      "Epoch [5/20], Step [2425/2541], D Loss: 0.6504, G Loss: 2.0561\n",
      "Epoch [5/20], Step [2426/2541], D Loss: 0.6503, G Loss: 2.1175\n",
      "Epoch [5/20], Step [2427/2541], D Loss: 0.6503, G Loss: 2.1199\n",
      "Epoch [5/20], Step [2428/2541], D Loss: 0.6502, G Loss: 2.0999\n",
      "Epoch [5/20], Step [2429/2541], D Loss: 0.6503, G Loss: 2.0674\n",
      "Epoch [5/20], Step [2430/2541], D Loss: 0.6502, G Loss: 2.0556\n",
      "Epoch [5/20], Step [2431/2541], D Loss: 0.6503, G Loss: 2.0641\n",
      "Epoch [5/20], Step [2432/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [5/20], Step [2433/2541], D Loss: 0.6502, G Loss: 2.0954\n",
      "Epoch [5/20], Step [2434/2541], D Loss: 0.6503, G Loss: 2.0613\n",
      "Epoch [5/20], Step [2435/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [5/20], Step [2436/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [5/20], Step [2437/2541], D Loss: 0.6503, G Loss: 2.0763\n",
      "Epoch [5/20], Step [2438/2541], D Loss: 0.6503, G Loss: 2.1026\n",
      "Epoch [5/20], Step [2439/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [5/20], Step [2440/2541], D Loss: 0.6502, G Loss: 2.0620\n",
      "Epoch [5/20], Step [2441/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [5/20], Step [2442/2541], D Loss: 0.6503, G Loss: 2.1019\n",
      "Epoch [5/20], Step [2443/2541], D Loss: 0.6502, G Loss: 2.1082\n",
      "Epoch [5/20], Step [2444/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [5/20], Step [2445/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [5/20], Step [2446/2541], D Loss: 0.6503, G Loss: 2.0811\n",
      "Epoch [5/20], Step [2447/2541], D Loss: 0.6502, G Loss: 2.1095\n",
      "Epoch [5/20], Step [2448/2541], D Loss: 0.6503, G Loss: 2.0849\n",
      "Epoch [5/20], Step [2449/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [5/20], Step [2450/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [5/20], Step [2451/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [5/20], Step [2452/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [5/20], Step [2453/2541], D Loss: 0.6502, G Loss: 2.0910\n",
      "Epoch [5/20], Step [2454/2541], D Loss: 0.6502, G Loss: 2.0953\n",
      "Epoch [5/20], Step [2455/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [5/20], Step [2456/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [5/20], Step [2457/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [5/20], Step [2458/2541], D Loss: 0.6502, G Loss: 2.0595\n",
      "Epoch [5/20], Step [2459/2541], D Loss: 0.6502, G Loss: 2.0990\n",
      "Epoch [5/20], Step [2460/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [5/20], Step [2461/2541], D Loss: 0.6502, G Loss: 2.0925\n",
      "Epoch [5/20], Step [2462/2541], D Loss: 0.6502, G Loss: 2.0961\n",
      "Epoch [5/20], Step [2463/2541], D Loss: 0.6502, G Loss: 2.0936\n",
      "Epoch [5/20], Step [2464/2541], D Loss: 0.6502, G Loss: 2.0419\n",
      "Epoch [5/20], Step [2465/2541], D Loss: 0.6502, G Loss: 2.0978\n",
      "Epoch [5/20], Step [2466/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [5/20], Step [2467/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [5/20], Step [2468/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [5/20], Step [2469/2541], D Loss: 0.6503, G Loss: 2.0806\n",
      "Epoch [5/20], Step [2470/2541], D Loss: 0.6503, G Loss: 2.0856\n",
      "Epoch [5/20], Step [2471/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [5/20], Step [2472/2541], D Loss: 0.6503, G Loss: 2.1232\n",
      "Epoch [5/20], Step [2473/2541], D Loss: 0.6504, G Loss: 2.0872\n",
      "Epoch [5/20], Step [2474/2541], D Loss: 0.6502, G Loss: 2.0379\n",
      "Epoch [5/20], Step [2475/2541], D Loss: 0.6502, G Loss: 2.0985\n",
      "Epoch [5/20], Step [2476/2541], D Loss: 0.6502, G Loss: 2.0979\n",
      "Epoch [5/20], Step [2477/2541], D Loss: 0.6502, G Loss: 2.0484\n",
      "Epoch [5/20], Step [2478/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [5/20], Step [2479/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [5/20], Step [2480/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [5/20], Step [2481/2541], D Loss: 0.6502, G Loss: 2.1112\n",
      "Epoch [5/20], Step [2482/2541], D Loss: 0.6504, G Loss: 2.0768\n",
      "Epoch [5/20], Step [2483/2541], D Loss: 0.6503, G Loss: 2.0606\n",
      "Epoch [5/20], Step [2484/2541], D Loss: 0.6503, G Loss: 2.0681\n",
      "Epoch [5/20], Step [2485/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [5/20], Step [2486/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [5/20], Step [2487/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [5/20], Step [2488/2541], D Loss: 0.6502, G Loss: 2.0644\n",
      "Epoch [5/20], Step [2489/2541], D Loss: 0.6502, G Loss: 2.0544\n",
      "Epoch [5/20], Step [2490/2541], D Loss: 0.6502, G Loss: 2.0664\n",
      "Epoch [5/20], Step [2491/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [5/20], Step [2492/2541], D Loss: 0.6503, G Loss: 2.0853\n",
      "Epoch [5/20], Step [2493/2541], D Loss: 0.6502, G Loss: 2.0950\n",
      "Epoch [5/20], Step [2494/2541], D Loss: 0.6505, G Loss: 2.0580\n",
      "Epoch [5/20], Step [2495/2541], D Loss: 0.6502, G Loss: 2.0952\n",
      "Epoch [5/20], Step [2496/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [5/20], Step [2497/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [5/20], Step [2498/2541], D Loss: 0.6502, G Loss: 2.0734\n",
      "Epoch [5/20], Step [2499/2541], D Loss: 0.6503, G Loss: 2.1136\n",
      "Epoch [5/20], Step [2500/2541], D Loss: 0.6502, G Loss: 2.1071\n",
      "Epoch [5/20], Step [2501/2541], D Loss: 0.6504, G Loss: 2.0914\n",
      "Epoch [5/20], Step [2502/2541], D Loss: 0.6502, G Loss: 2.0611\n",
      "Epoch [5/20], Step [2503/2541], D Loss: 0.6506, G Loss: 2.0465\n",
      "Epoch [5/20], Step [2504/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [5/20], Step [2505/2541], D Loss: 0.6506, G Loss: 2.0868\n",
      "Epoch [5/20], Step [2506/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [5/20], Step [2507/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [5/20], Step [2508/2541], D Loss: 0.6502, G Loss: 2.0699\n",
      "Epoch [5/20], Step [2509/2541], D Loss: 0.6504, G Loss: 2.0966\n",
      "Epoch [5/20], Step [2510/2541], D Loss: 0.6503, G Loss: 2.0857\n",
      "Epoch [5/20], Step [2511/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [5/20], Step [2512/2541], D Loss: 0.6502, G Loss: 2.0638\n",
      "Epoch [5/20], Step [2513/2541], D Loss: 0.6506, G Loss: 2.0633\n",
      "Epoch [5/20], Step [2514/2541], D Loss: 0.6509, G Loss: 2.0877\n",
      "Epoch [5/20], Step [2515/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [5/20], Step [2516/2541], D Loss: 0.6502, G Loss: 2.0948\n",
      "Epoch [5/20], Step [2517/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [5/20], Step [2518/2541], D Loss: 0.6502, G Loss: 2.1040\n",
      "Epoch [5/20], Step [2519/2541], D Loss: 0.6503, G Loss: 2.1019\n",
      "Epoch [5/20], Step [2520/2541], D Loss: 0.6503, G Loss: 2.1059\n",
      "Epoch [5/20], Step [2521/2541], D Loss: 0.6502, G Loss: 2.0966\n",
      "Epoch [5/20], Step [2522/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [5/20], Step [2523/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [5/20], Step [2524/2541], D Loss: 0.6504, G Loss: 2.1005\n",
      "Epoch [5/20], Step [2525/2541], D Loss: 0.6503, G Loss: 2.1126\n",
      "Epoch [5/20], Step [2526/2541], D Loss: 0.6503, G Loss: 2.0866\n",
      "Epoch [5/20], Step [2527/2541], D Loss: 0.6502, G Loss: 2.0676\n",
      "Epoch [5/20], Step [2528/2541], D Loss: 0.6503, G Loss: 2.0785\n",
      "Epoch [5/20], Step [2529/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [5/20], Step [2530/2541], D Loss: 0.6502, G Loss: 2.1075\n",
      "Epoch [5/20], Step [2531/2541], D Loss: 0.6502, G Loss: 2.1162\n",
      "Epoch [5/20], Step [2532/2541], D Loss: 0.6502, G Loss: 2.1020\n",
      "Epoch [5/20], Step [2533/2541], D Loss: 0.6502, G Loss: 2.1000\n",
      "Epoch [5/20], Step [2534/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [5/20], Step [2535/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [5/20], Step [2536/2541], D Loss: 0.6502, G Loss: 2.0433\n",
      "Epoch [5/20], Step [2537/2541], D Loss: 0.6502, G Loss: 2.1001\n",
      "Epoch [5/20], Step [2538/2541], D Loss: 0.6502, G Loss: 2.0991\n",
      "Epoch [5/20], Step [2539/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [5/20], Step [2540/2541], D Loss: 0.6547, G Loss: 2.2401\n",
      "Models saved after epoch 5\n",
      "Epoch [6/20], Step [0/2541], D Loss: 0.6615, G Loss: 1.9675\n",
      "Epoch [6/20], Step [1/2541], D Loss: 0.6509, G Loss: 1.9499\n",
      "Epoch [6/20], Step [2/2541], D Loss: 0.6521, G Loss: 2.1591\n",
      "Epoch [6/20], Step [3/2541], D Loss: 0.6510, G Loss: 2.1402\n",
      "Epoch [6/20], Step [4/2541], D Loss: 0.6506, G Loss: 2.0561\n",
      "Epoch [6/20], Step [5/2541], D Loss: 0.6507, G Loss: 2.0702\n",
      "Epoch [6/20], Step [6/2541], D Loss: 0.6503, G Loss: 2.1022\n",
      "Epoch [6/20], Step [7/2541], D Loss: 0.6503, G Loss: 2.1434\n",
      "Epoch [6/20], Step [8/2541], D Loss: 0.6505, G Loss: 2.0386\n",
      "Epoch [6/20], Step [9/2541], D Loss: 0.6503, G Loss: 2.0872\n",
      "Epoch [6/20], Step [10/2541], D Loss: 0.6503, G Loss: 2.0995\n",
      "Epoch [6/20], Step [11/2541], D Loss: 0.6503, G Loss: 2.0849\n",
      "Epoch [6/20], Step [12/2541], D Loss: 0.6502, G Loss: 2.0704\n",
      "Epoch [6/20], Step [13/2541], D Loss: 0.6503, G Loss: 2.0542\n",
      "Epoch [6/20], Step [14/2541], D Loss: 0.6503, G Loss: 2.0944\n",
      "Epoch [6/20], Step [15/2541], D Loss: 0.6502, G Loss: 2.1258\n",
      "Epoch [6/20], Step [16/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [6/20], Step [17/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [6/20], Step [18/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [6/20], Step [19/2541], D Loss: 0.6502, G Loss: 2.0548\n",
      "Epoch [6/20], Step [20/2541], D Loss: 0.6502, G Loss: 2.1145\n",
      "Epoch [6/20], Step [21/2541], D Loss: 0.6502, G Loss: 2.0536\n",
      "Epoch [6/20], Step [22/2541], D Loss: 0.6507, G Loss: 2.0671\n",
      "Epoch [6/20], Step [23/2541], D Loss: 0.6503, G Loss: 2.1171\n",
      "Epoch [6/20], Step [24/2541], D Loss: 0.6503, G Loss: 2.1030\n",
      "Epoch [6/20], Step [25/2541], D Loss: 0.6502, G Loss: 2.1261\n",
      "Epoch [6/20], Step [26/2541], D Loss: 0.6504, G Loss: 2.0682\n",
      "Epoch [6/20], Step [27/2541], D Loss: 0.6503, G Loss: 2.0968\n",
      "Epoch [6/20], Step [28/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [6/20], Step [29/2541], D Loss: 0.6502, G Loss: 2.0510\n",
      "Epoch [6/20], Step [30/2541], D Loss: 0.6503, G Loss: 2.0838\n",
      "Epoch [6/20], Step [31/2541], D Loss: 0.6503, G Loss: 2.0772\n",
      "Epoch [6/20], Step [32/2541], D Loss: 0.6502, G Loss: 2.1154\n",
      "Epoch [6/20], Step [33/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [6/20], Step [34/2541], D Loss: 0.6502, G Loss: 2.0677\n",
      "Epoch [6/20], Step [35/2541], D Loss: 0.6504, G Loss: 2.1103\n",
      "Epoch [6/20], Step [36/2541], D Loss: 0.6503, G Loss: 2.1047\n",
      "Epoch [6/20], Step [37/2541], D Loss: 0.6502, G Loss: 2.0580\n",
      "Epoch [6/20], Step [38/2541], D Loss: 0.6503, G Loss: 2.0949\n",
      "Epoch [6/20], Step [39/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [6/20], Step [40/2541], D Loss: 0.6503, G Loss: 2.0696\n",
      "Epoch [6/20], Step [41/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [6/20], Step [42/2541], D Loss: 0.6505, G Loss: 2.0740\n",
      "Epoch [6/20], Step [43/2541], D Loss: 0.6504, G Loss: 2.0949\n",
      "Epoch [6/20], Step [44/2541], D Loss: 0.6505, G Loss: 2.0705\n",
      "Epoch [6/20], Step [45/2541], D Loss: 0.6503, G Loss: 2.0867\n",
      "Epoch [6/20], Step [46/2541], D Loss: 0.6503, G Loss: 2.0698\n",
      "Epoch [6/20], Step [47/2541], D Loss: 0.6504, G Loss: 2.0913\n",
      "Epoch [6/20], Step [48/2541], D Loss: 0.6503, G Loss: 2.0799\n",
      "Epoch [6/20], Step [49/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [6/20], Step [50/2541], D Loss: 0.6502, G Loss: 2.0686\n",
      "Epoch [6/20], Step [51/2541], D Loss: 0.6502, G Loss: 2.0999\n",
      "Epoch [6/20], Step [52/2541], D Loss: 0.6503, G Loss: 2.0803\n",
      "Epoch [6/20], Step [53/2541], D Loss: 0.6503, G Loss: 2.0803\n",
      "Epoch [6/20], Step [54/2541], D Loss: 0.6509, G Loss: 2.0979\n",
      "Epoch [6/20], Step [55/2541], D Loss: 0.6505, G Loss: 2.0872\n",
      "Epoch [6/20], Step [56/2541], D Loss: 0.6503, G Loss: 2.0701\n",
      "Epoch [6/20], Step [57/2541], D Loss: 0.6518, G Loss: 2.0469\n",
      "Epoch [6/20], Step [58/2541], D Loss: 0.6504, G Loss: 2.0964\n",
      "Epoch [6/20], Step [59/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [6/20], Step [60/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [61/2541], D Loss: 0.6503, G Loss: 2.1102\n",
      "Epoch [6/20], Step [62/2541], D Loss: 0.6505, G Loss: 2.0628\n",
      "Epoch [6/20], Step [63/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [6/20], Step [64/2541], D Loss: 0.6508, G Loss: 2.1625\n",
      "Epoch [6/20], Step [65/2541], D Loss: 0.6506, G Loss: 2.1052\n",
      "Epoch [6/20], Step [66/2541], D Loss: 0.6503, G Loss: 2.0824\n",
      "Epoch [6/20], Step [67/2541], D Loss: 0.6502, G Loss: 2.0546\n",
      "Epoch [6/20], Step [68/2541], D Loss: 0.6503, G Loss: 2.0560\n",
      "Epoch [6/20], Step [69/2541], D Loss: 0.6503, G Loss: 2.1156\n",
      "Epoch [6/20], Step [70/2541], D Loss: 0.6503, G Loss: 2.0668\n",
      "Epoch [6/20], Step [71/2541], D Loss: 0.6502, G Loss: 2.0573\n",
      "Epoch [6/20], Step [72/2541], D Loss: 0.6503, G Loss: 2.0898\n",
      "Epoch [6/20], Step [73/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [6/20], Step [74/2541], D Loss: 0.6502, G Loss: 2.0952\n",
      "Epoch [6/20], Step [75/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [6/20], Step [76/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [6/20], Step [77/2541], D Loss: 0.6502, G Loss: 2.0987\n",
      "Epoch [6/20], Step [78/2541], D Loss: 0.6502, G Loss: 2.0549\n",
      "Epoch [6/20], Step [79/2541], D Loss: 0.6502, G Loss: 2.0944\n",
      "Epoch [6/20], Step [80/2541], D Loss: 0.6510, G Loss: 2.0712\n",
      "Epoch [6/20], Step [81/2541], D Loss: 0.6509, G Loss: 2.0819\n",
      "Epoch [6/20], Step [82/2541], D Loss: 0.6507, G Loss: 2.0901\n",
      "Epoch [6/20], Step [83/2541], D Loss: 0.6506, G Loss: 2.0945\n",
      "Epoch [6/20], Step [84/2541], D Loss: 0.6509, G Loss: 2.0782\n",
      "Epoch [6/20], Step [85/2541], D Loss: 0.6503, G Loss: 2.1055\n",
      "Epoch [6/20], Step [86/2541], D Loss: 0.6504, G Loss: 2.0921\n",
      "Epoch [6/20], Step [87/2541], D Loss: 0.6502, G Loss: 2.0607\n",
      "Epoch [6/20], Step [88/2541], D Loss: 0.6507, G Loss: 2.0756\n",
      "Epoch [6/20], Step [89/2541], D Loss: 0.6510, G Loss: 2.0347\n",
      "Epoch [6/20], Step [90/2541], D Loss: 0.6503, G Loss: 2.0668\n",
      "Epoch [6/20], Step [91/2541], D Loss: 0.6505, G Loss: 2.0935\n",
      "Epoch [6/20], Step [92/2541], D Loss: 0.6503, G Loss: 2.0756\n",
      "Epoch [6/20], Step [93/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [94/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [6/20], Step [95/2541], D Loss: 0.6503, G Loss: 2.0555\n",
      "Epoch [6/20], Step [96/2541], D Loss: 0.6503, G Loss: 2.0675\n",
      "Epoch [6/20], Step [97/2541], D Loss: 0.6502, G Loss: 2.0569\n",
      "Epoch [6/20], Step [98/2541], D Loss: 0.6502, G Loss: 2.0387\n",
      "Epoch [6/20], Step [99/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [6/20], Step [100/2541], D Loss: 0.6502, G Loss: 2.0757\n",
      "Epoch [6/20], Step [101/2541], D Loss: 0.6502, G Loss: 2.0590\n",
      "Epoch [6/20], Step [102/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [6/20], Step [103/2541], D Loss: 0.6503, G Loss: 2.0848\n",
      "Epoch [6/20], Step [104/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [6/20], Step [105/2541], D Loss: 0.6503, G Loss: 2.0470\n",
      "Epoch [6/20], Step [106/2541], D Loss: 0.6503, G Loss: 2.0920\n",
      "Epoch [6/20], Step [107/2541], D Loss: 0.6503, G Loss: 2.0872\n",
      "Epoch [6/20], Step [108/2541], D Loss: 0.6503, G Loss: 2.0883\n",
      "Epoch [6/20], Step [109/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [6/20], Step [110/2541], D Loss: 0.6503, G Loss: 2.0581\n",
      "Epoch [6/20], Step [111/2541], D Loss: 0.6502, G Loss: 2.0629\n",
      "Epoch [6/20], Step [112/2541], D Loss: 0.6503, G Loss: 2.0793\n",
      "Epoch [6/20], Step [113/2541], D Loss: 0.6502, G Loss: 2.1148\n",
      "Epoch [6/20], Step [114/2541], D Loss: 0.6502, G Loss: 2.1105\n",
      "Epoch [6/20], Step [115/2541], D Loss: 0.6503, G Loss: 2.1057\n",
      "Epoch [6/20], Step [116/2541], D Loss: 0.6503, G Loss: 2.0683\n",
      "Epoch [6/20], Step [117/2541], D Loss: 0.6502, G Loss: 2.0685\n",
      "Epoch [6/20], Step [118/2541], D Loss: 0.6505, G Loss: 2.0750\n",
      "Epoch [6/20], Step [119/2541], D Loss: 0.6503, G Loss: 2.1022\n",
      "Epoch [6/20], Step [120/2541], D Loss: 0.6503, G Loss: 2.0905\n",
      "Epoch [6/20], Step [121/2541], D Loss: 0.6503, G Loss: 2.0675\n",
      "Epoch [6/20], Step [122/2541], D Loss: 0.6504, G Loss: 2.0876\n",
      "Epoch [6/20], Step [123/2541], D Loss: 0.6504, G Loss: 2.0865\n",
      "Epoch [6/20], Step [124/2541], D Loss: 0.6503, G Loss: 2.0985\n",
      "Epoch [6/20], Step [125/2541], D Loss: 0.6503, G Loss: 2.0802\n",
      "Epoch [6/20], Step [126/2541], D Loss: 0.6502, G Loss: 2.0442\n",
      "Epoch [6/20], Step [127/2541], D Loss: 0.6503, G Loss: 2.0890\n",
      "Epoch [6/20], Step [128/2541], D Loss: 0.6502, G Loss: 2.0959\n",
      "Epoch [6/20], Step [129/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [6/20], Step [130/2541], D Loss: 0.6502, G Loss: 2.0698\n",
      "Epoch [6/20], Step [131/2541], D Loss: 0.6502, G Loss: 2.1099\n",
      "Epoch [6/20], Step [132/2541], D Loss: 0.6502, G Loss: 2.1034\n",
      "Epoch [6/20], Step [133/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [6/20], Step [134/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [135/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [6/20], Step [136/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [6/20], Step [137/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [6/20], Step [138/2541], D Loss: 0.6502, G Loss: 2.0740\n",
      "Epoch [6/20], Step [139/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [6/20], Step [140/2541], D Loss: 0.6504, G Loss: 2.0842\n",
      "Epoch [6/20], Step [141/2541], D Loss: 0.6502, G Loss: 2.1357\n",
      "Epoch [6/20], Step [142/2541], D Loss: 0.6505, G Loss: 2.0927\n",
      "Epoch [6/20], Step [143/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [6/20], Step [144/2541], D Loss: 0.6502, G Loss: 2.0569\n",
      "Epoch [6/20], Step [145/2541], D Loss: 0.6502, G Loss: 2.0929\n",
      "Epoch [6/20], Step [146/2541], D Loss: 0.6503, G Loss: 2.1154\n",
      "Epoch [6/20], Step [147/2541], D Loss: 0.6503, G Loss: 2.1097\n",
      "Epoch [6/20], Step [148/2541], D Loss: 0.6503, G Loss: 2.0568\n",
      "Epoch [6/20], Step [149/2541], D Loss: 0.6502, G Loss: 2.0628\n",
      "Epoch [6/20], Step [150/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [6/20], Step [151/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [6/20], Step [152/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [6/20], Step [153/2541], D Loss: 0.6503, G Loss: 2.1066\n",
      "Epoch [6/20], Step [154/2541], D Loss: 0.6507, G Loss: 2.0523\n",
      "Epoch [6/20], Step [155/2541], D Loss: 0.6505, G Loss: 2.0695\n",
      "Epoch [6/20], Step [156/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [6/20], Step [157/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [6/20], Step [158/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [6/20], Step [159/2541], D Loss: 0.6504, G Loss: 2.0688\n",
      "Epoch [6/20], Step [160/2541], D Loss: 0.6505, G Loss: 2.0931\n",
      "Epoch [6/20], Step [161/2541], D Loss: 0.6503, G Loss: 2.0801\n",
      "Epoch [6/20], Step [162/2541], D Loss: 0.6506, G Loss: 2.0864\n",
      "Epoch [6/20], Step [163/2541], D Loss: 0.6503, G Loss: 2.0800\n",
      "Epoch [6/20], Step [164/2541], D Loss: 0.6503, G Loss: 2.1033\n",
      "Epoch [6/20], Step [165/2541], D Loss: 0.6514, G Loss: 2.0670\n",
      "Epoch [6/20], Step [166/2541], D Loss: 0.6506, G Loss: 2.0746\n",
      "Epoch [6/20], Step [167/2541], D Loss: 0.6505, G Loss: 2.0738\n",
      "Epoch [6/20], Step [168/2541], D Loss: 0.6502, G Loss: 2.0976\n",
      "Epoch [6/20], Step [169/2541], D Loss: 0.6502, G Loss: 2.1107\n",
      "Epoch [6/20], Step [170/2541], D Loss: 0.6503, G Loss: 2.0672\n",
      "Epoch [6/20], Step [171/2541], D Loss: 0.6503, G Loss: 2.0681\n",
      "Epoch [6/20], Step [172/2541], D Loss: 0.6504, G Loss: 2.1031\n",
      "Epoch [6/20], Step [173/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [174/2541], D Loss: 0.6504, G Loss: 2.0731\n",
      "Epoch [6/20], Step [175/2541], D Loss: 0.6503, G Loss: 2.1002\n",
      "Epoch [6/20], Step [176/2541], D Loss: 0.6502, G Loss: 2.1178\n",
      "Epoch [6/20], Step [177/2541], D Loss: 0.6504, G Loss: 2.0930\n",
      "Epoch [6/20], Step [178/2541], D Loss: 0.6504, G Loss: 2.0873\n",
      "Epoch [6/20], Step [179/2541], D Loss: 0.6503, G Loss: 2.0891\n",
      "Epoch [6/20], Step [180/2541], D Loss: 0.6502, G Loss: 2.0688\n",
      "Epoch [6/20], Step [181/2541], D Loss: 0.6502, G Loss: 2.0740\n",
      "Epoch [6/20], Step [182/2541], D Loss: 0.6506, G Loss: 2.1014\n",
      "Epoch [6/20], Step [183/2541], D Loss: 0.6505, G Loss: 2.0873\n",
      "Epoch [6/20], Step [184/2541], D Loss: 0.6503, G Loss: 2.0663\n",
      "Epoch [6/20], Step [185/2541], D Loss: 0.6503, G Loss: 2.0845\n",
      "Epoch [6/20], Step [186/2541], D Loss: 0.6502, G Loss: 2.0593\n",
      "Epoch [6/20], Step [187/2541], D Loss: 0.6503, G Loss: 2.0983\n",
      "Epoch [6/20], Step [188/2541], D Loss: 0.6502, G Loss: 2.1239\n",
      "Epoch [6/20], Step [189/2541], D Loss: 0.6502, G Loss: 2.0997\n",
      "Epoch [6/20], Step [190/2541], D Loss: 0.6502, G Loss: 2.0706\n",
      "Epoch [6/20], Step [191/2541], D Loss: 0.6502, G Loss: 2.0679\n",
      "Epoch [6/20], Step [192/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [6/20], Step [193/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [194/2541], D Loss: 0.6502, G Loss: 2.1015\n",
      "Epoch [6/20], Step [195/2541], D Loss: 0.6503, G Loss: 2.0948\n",
      "Epoch [6/20], Step [196/2541], D Loss: 0.6502, G Loss: 2.0686\n",
      "Epoch [6/20], Step [197/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [6/20], Step [198/2541], D Loss: 0.6502, G Loss: 2.0627\n",
      "Epoch [6/20], Step [199/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [6/20], Step [200/2541], D Loss: 0.6502, G Loss: 2.0712\n",
      "Epoch [6/20], Step [201/2541], D Loss: 0.6502, G Loss: 2.0311\n",
      "Epoch [6/20], Step [202/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [6/20], Step [203/2541], D Loss: 0.6502, G Loss: 2.0658\n",
      "Epoch [6/20], Step [204/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [205/2541], D Loss: 0.6502, G Loss: 2.1054\n",
      "Epoch [6/20], Step [206/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [207/2541], D Loss: 0.6502, G Loss: 2.0666\n",
      "Epoch [6/20], Step [208/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [6/20], Step [209/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [6/20], Step [210/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [211/2541], D Loss: 0.6502, G Loss: 2.0571\n",
      "Epoch [6/20], Step [212/2541], D Loss: 0.6503, G Loss: 2.0937\n",
      "Epoch [6/20], Step [213/2541], D Loss: 0.6502, G Loss: 2.1048\n",
      "Epoch [6/20], Step [214/2541], D Loss: 0.6502, G Loss: 2.0636\n",
      "Epoch [6/20], Step [215/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [6/20], Step [216/2541], D Loss: 0.6502, G Loss: 2.0378\n",
      "Epoch [6/20], Step [217/2541], D Loss: 0.6503, G Loss: 2.0797\n",
      "Epoch [6/20], Step [218/2541], D Loss: 0.6504, G Loss: 2.0692\n",
      "Epoch [6/20], Step [219/2541], D Loss: 0.6503, G Loss: 2.0879\n",
      "Epoch [6/20], Step [220/2541], D Loss: 0.6503, G Loss: 2.0746\n",
      "Epoch [6/20], Step [221/2541], D Loss: 0.6502, G Loss: 2.0612\n",
      "Epoch [6/20], Step [222/2541], D Loss: 0.6502, G Loss: 2.0643\n",
      "Epoch [6/20], Step [223/2541], D Loss: 0.6502, G Loss: 2.0910\n",
      "Epoch [6/20], Step [224/2541], D Loss: 0.6503, G Loss: 2.0806\n",
      "Epoch [6/20], Step [225/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [6/20], Step [226/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [227/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [6/20], Step [228/2541], D Loss: 0.6502, G Loss: 2.1062\n",
      "Epoch [6/20], Step [229/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [6/20], Step [230/2541], D Loss: 0.6502, G Loss: 2.0686\n",
      "Epoch [6/20], Step [231/2541], D Loss: 0.6504, G Loss: 2.0972\n",
      "Epoch [6/20], Step [232/2541], D Loss: 0.6502, G Loss: 2.1001\n",
      "Epoch [6/20], Step [233/2541], D Loss: 0.6502, G Loss: 2.0644\n",
      "Epoch [6/20], Step [234/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [6/20], Step [235/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [6/20], Step [236/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [6/20], Step [237/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [6/20], Step [238/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [6/20], Step [239/2541], D Loss: 0.6502, G Loss: 2.0909\n",
      "Epoch [6/20], Step [240/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [6/20], Step [241/2541], D Loss: 0.6508, G Loss: 2.1105\n",
      "Epoch [6/20], Step [242/2541], D Loss: 0.6503, G Loss: 2.0788\n",
      "Epoch [6/20], Step [243/2541], D Loss: 0.6502, G Loss: 2.0548\n",
      "Epoch [6/20], Step [244/2541], D Loss: 0.6502, G Loss: 2.0937\n",
      "Epoch [6/20], Step [245/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [246/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [247/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [6/20], Step [248/2541], D Loss: 0.6505, G Loss: 2.1024\n",
      "Epoch [6/20], Step [249/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [250/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [6/20], Step [251/2541], D Loss: 0.6502, G Loss: 2.0968\n",
      "Epoch [6/20], Step [252/2541], D Loss: 0.6502, G Loss: 2.0986\n",
      "Epoch [6/20], Step [253/2541], D Loss: 0.6502, G Loss: 2.0612\n",
      "Epoch [6/20], Step [254/2541], D Loss: 0.6502, G Loss: 2.0644\n",
      "Epoch [6/20], Step [255/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [6/20], Step [256/2541], D Loss: 0.6502, G Loss: 2.0681\n",
      "Epoch [6/20], Step [257/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [6/20], Step [258/2541], D Loss: 0.6502, G Loss: 2.0941\n",
      "Epoch [6/20], Step [259/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [6/20], Step [260/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [6/20], Step [261/2541], D Loss: 0.6503, G Loss: 2.0888\n",
      "Epoch [6/20], Step [262/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [263/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [6/20], Step [264/2541], D Loss: 0.6504, G Loss: 2.0815\n",
      "Epoch [6/20], Step [265/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [266/2541], D Loss: 0.6502, G Loss: 2.0686\n",
      "Epoch [6/20], Step [267/2541], D Loss: 0.6502, G Loss: 2.0590\n",
      "Epoch [6/20], Step [268/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [6/20], Step [269/2541], D Loss: 0.6502, G Loss: 2.0978\n",
      "Epoch [6/20], Step [270/2541], D Loss: 0.6502, G Loss: 2.0929\n",
      "Epoch [6/20], Step [271/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [272/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [6/20], Step [273/2541], D Loss: 0.6502, G Loss: 2.1042\n",
      "Epoch [6/20], Step [274/2541], D Loss: 0.6503, G Loss: 2.0776\n",
      "Epoch [6/20], Step [275/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [6/20], Step [276/2541], D Loss: 0.6502, G Loss: 2.0719\n",
      "Epoch [6/20], Step [277/2541], D Loss: 0.6502, G Loss: 2.1003\n",
      "Epoch [6/20], Step [278/2541], D Loss: 0.6502, G Loss: 2.0980\n",
      "Epoch [6/20], Step [279/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [6/20], Step [280/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [6/20], Step [281/2541], D Loss: 0.6502, G Loss: 2.0950\n",
      "Epoch [6/20], Step [282/2541], D Loss: 0.6502, G Loss: 2.1003\n",
      "Epoch [6/20], Step [283/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [6/20], Step [284/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [6/20], Step [285/2541], D Loss: 0.6502, G Loss: 2.1016\n",
      "Epoch [6/20], Step [286/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [6/20], Step [287/2541], D Loss: 0.6502, G Loss: 2.0598\n",
      "Epoch [6/20], Step [288/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [289/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [6/20], Step [290/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [6/20], Step [291/2541], D Loss: 0.6505, G Loss: 2.1087\n",
      "Epoch [6/20], Step [292/2541], D Loss: 0.6506, G Loss: 2.0447\n",
      "Epoch [6/20], Step [293/2541], D Loss: 0.6503, G Loss: 2.0596\n",
      "Epoch [6/20], Step [294/2541], D Loss: 0.6503, G Loss: 2.1238\n",
      "Epoch [6/20], Step [295/2541], D Loss: 0.6502, G Loss: 2.1097\n",
      "Epoch [6/20], Step [296/2541], D Loss: 0.6503, G Loss: 2.0713\n",
      "Epoch [6/20], Step [297/2541], D Loss: 0.6502, G Loss: 2.0697\n",
      "Epoch [6/20], Step [298/2541], D Loss: 0.6502, G Loss: 2.0632\n",
      "Epoch [6/20], Step [299/2541], D Loss: 0.6503, G Loss: 2.0913\n",
      "Epoch [6/20], Step [300/2541], D Loss: 0.6502, G Loss: 2.1078\n",
      "Epoch [6/20], Step [301/2541], D Loss: 0.6502, G Loss: 2.0484\n",
      "Epoch [6/20], Step [302/2541], D Loss: 0.6502, G Loss: 2.1048\n",
      "Epoch [6/20], Step [303/2541], D Loss: 0.6502, G Loss: 2.0640\n",
      "Epoch [6/20], Step [304/2541], D Loss: 0.6502, G Loss: 2.0685\n",
      "Epoch [6/20], Step [305/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [6/20], Step [306/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [6/20], Step [307/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [6/20], Step [308/2541], D Loss: 0.6502, G Loss: 2.0458\n",
      "Epoch [6/20], Step [309/2541], D Loss: 0.6503, G Loss: 2.1044\n",
      "Epoch [6/20], Step [310/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [6/20], Step [311/2541], D Loss: 0.6502, G Loss: 2.0714\n",
      "Epoch [6/20], Step [312/2541], D Loss: 0.6502, G Loss: 2.0544\n",
      "Epoch [6/20], Step [313/2541], D Loss: 0.6503, G Loss: 2.1019\n",
      "Epoch [6/20], Step [314/2541], D Loss: 0.6502, G Loss: 2.1069\n",
      "Epoch [6/20], Step [315/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [6/20], Step [316/2541], D Loss: 0.6502, G Loss: 2.0677\n",
      "Epoch [6/20], Step [317/2541], D Loss: 0.6503, G Loss: 2.0868\n",
      "Epoch [6/20], Step [318/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [6/20], Step [319/2541], D Loss: 0.6502, G Loss: 2.1047\n",
      "Epoch [6/20], Step [320/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [6/20], Step [321/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [6/20], Step [322/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [6/20], Step [323/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [6/20], Step [324/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [6/20], Step [325/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [6/20], Step [326/2541], D Loss: 0.6502, G Loss: 2.1045\n",
      "Epoch [6/20], Step [327/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [6/20], Step [328/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [6/20], Step [329/2541], D Loss: 0.6502, G Loss: 2.0678\n",
      "Epoch [6/20], Step [330/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [6/20], Step [331/2541], D Loss: 0.6502, G Loss: 2.1006\n",
      "Epoch [6/20], Step [332/2541], D Loss: 0.6503, G Loss: 2.0931\n",
      "Epoch [6/20], Step [333/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [6/20], Step [334/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [6/20], Step [335/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [6/20], Step [336/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [337/2541], D Loss: 0.6503, G Loss: 2.0874\n",
      "Epoch [6/20], Step [338/2541], D Loss: 0.6502, G Loss: 2.0442\n",
      "Epoch [6/20], Step [339/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [340/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [6/20], Step [341/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [6/20], Step [342/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [6/20], Step [343/2541], D Loss: 0.6502, G Loss: 2.0686\n",
      "Epoch [6/20], Step [344/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [6/20], Step [345/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [6/20], Step [346/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [6/20], Step [347/2541], D Loss: 0.6506, G Loss: 2.0735\n",
      "Epoch [6/20], Step [348/2541], D Loss: 0.6503, G Loss: 2.0548\n",
      "Epoch [6/20], Step [349/2541], D Loss: 0.6503, G Loss: 2.0519\n",
      "Epoch [6/20], Step [350/2541], D Loss: 0.6502, G Loss: 2.1069\n",
      "Epoch [6/20], Step [351/2541], D Loss: 0.6503, G Loss: 2.0833\n",
      "Epoch [6/20], Step [352/2541], D Loss: 0.6502, G Loss: 2.0740\n",
      "Epoch [6/20], Step [353/2541], D Loss: 0.6504, G Loss: 2.0660\n",
      "Epoch [6/20], Step [354/2541], D Loss: 0.6503, G Loss: 2.0983\n",
      "Epoch [6/20], Step [355/2541], D Loss: 0.6502, G Loss: 2.1097\n",
      "Epoch [6/20], Step [356/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [6/20], Step [357/2541], D Loss: 0.6503, G Loss: 2.0500\n",
      "Epoch [6/20], Step [358/2541], D Loss: 0.6502, G Loss: 2.1006\n",
      "Epoch [6/20], Step [359/2541], D Loss: 0.6502, G Loss: 2.0965\n",
      "Epoch [6/20], Step [360/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [361/2541], D Loss: 0.6502, G Loss: 2.0689\n",
      "Epoch [6/20], Step [362/2541], D Loss: 0.6502, G Loss: 2.1016\n",
      "Epoch [6/20], Step [363/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [6/20], Step [364/2541], D Loss: 0.6502, G Loss: 2.0729\n",
      "Epoch [6/20], Step [365/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [6/20], Step [366/2541], D Loss: 0.6502, G Loss: 2.0680\n",
      "Epoch [6/20], Step [367/2541], D Loss: 0.6510, G Loss: 2.0580\n",
      "Epoch [6/20], Step [368/2541], D Loss: 0.6505, G Loss: 2.0921\n",
      "Epoch [6/20], Step [369/2541], D Loss: 0.6506, G Loss: 2.0824\n",
      "Epoch [6/20], Step [370/2541], D Loss: 0.6502, G Loss: 2.0702\n",
      "Epoch [6/20], Step [371/2541], D Loss: 0.6503, G Loss: 2.0838\n",
      "Epoch [6/20], Step [372/2541], D Loss: 0.6502, G Loss: 2.0978\n",
      "Epoch [6/20], Step [373/2541], D Loss: 0.6502, G Loss: 2.0901\n",
      "Epoch [6/20], Step [374/2541], D Loss: 0.6502, G Loss: 2.1053\n",
      "Epoch [6/20], Step [375/2541], D Loss: 0.6504, G Loss: 2.0583\n",
      "Epoch [6/20], Step [376/2541], D Loss: 0.6503, G Loss: 2.0795\n",
      "Epoch [6/20], Step [377/2541], D Loss: 0.6502, G Loss: 2.1054\n",
      "Epoch [6/20], Step [378/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [6/20], Step [379/2541], D Loss: 0.6503, G Loss: 2.0713\n",
      "Epoch [6/20], Step [380/2541], D Loss: 0.6503, G Loss: 2.0911\n",
      "Epoch [6/20], Step [381/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [6/20], Step [382/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [6/20], Step [383/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [6/20], Step [384/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [6/20], Step [385/2541], D Loss: 0.6503, G Loss: 2.1100\n",
      "Epoch [6/20], Step [386/2541], D Loss: 0.6503, G Loss: 2.0776\n",
      "Epoch [6/20], Step [387/2541], D Loss: 0.6502, G Loss: 2.0534\n",
      "Epoch [6/20], Step [388/2541], D Loss: 0.6502, G Loss: 2.1082\n",
      "Epoch [6/20], Step [389/2541], D Loss: 0.6503, G Loss: 2.1066\n",
      "Epoch [6/20], Step [390/2541], D Loss: 0.6502, G Loss: 2.1136\n",
      "Epoch [6/20], Step [391/2541], D Loss: 0.6503, G Loss: 2.0924\n",
      "Epoch [6/20], Step [392/2541], D Loss: 0.6511, G Loss: 2.0731\n",
      "Epoch [6/20], Step [393/2541], D Loss: 0.6505, G Loss: 2.0816\n",
      "Epoch [6/20], Step [394/2541], D Loss: 0.6505, G Loss: 2.0983\n",
      "Epoch [6/20], Step [395/2541], D Loss: 0.6504, G Loss: 2.1201\n",
      "Epoch [6/20], Step [396/2541], D Loss: 0.6503, G Loss: 2.0857\n",
      "Epoch [6/20], Step [397/2541], D Loss: 0.6502, G Loss: 2.0602\n",
      "Epoch [6/20], Step [398/2541], D Loss: 0.6502, G Loss: 2.0569\n",
      "Epoch [6/20], Step [399/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [6/20], Step [400/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [6/20], Step [401/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [6/20], Step [402/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [6/20], Step [403/2541], D Loss: 0.6503, G Loss: 2.0707\n",
      "Epoch [6/20], Step [404/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [6/20], Step [405/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [6/20], Step [406/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [6/20], Step [407/2541], D Loss: 0.6502, G Loss: 2.0924\n",
      "Epoch [6/20], Step [408/2541], D Loss: 0.6502, G Loss: 2.0763\n",
      "Epoch [6/20], Step [409/2541], D Loss: 0.6502, G Loss: 2.0763\n",
      "Epoch [6/20], Step [410/2541], D Loss: 0.6503, G Loss: 2.0759\n",
      "Epoch [6/20], Step [411/2541], D Loss: 0.6503, G Loss: 2.0896\n",
      "Epoch [6/20], Step [412/2541], D Loss: 0.6502, G Loss: 2.0950\n",
      "Epoch [6/20], Step [413/2541], D Loss: 0.6502, G Loss: 2.0968\n",
      "Epoch [6/20], Step [414/2541], D Loss: 0.6502, G Loss: 2.0567\n",
      "Epoch [6/20], Step [415/2541], D Loss: 0.6502, G Loss: 2.0693\n",
      "Epoch [6/20], Step [416/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [6/20], Step [417/2541], D Loss: 0.6502, G Loss: 2.0893\n",
      "Epoch [6/20], Step [418/2541], D Loss: 0.6502, G Loss: 2.0939\n",
      "Epoch [6/20], Step [419/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [6/20], Step [420/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [6/20], Step [421/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [6/20], Step [422/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [423/2541], D Loss: 0.6502, G Loss: 2.0517\n",
      "Epoch [6/20], Step [424/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [6/20], Step [425/2541], D Loss: 0.6502, G Loss: 2.0565\n",
      "Epoch [6/20], Step [426/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [427/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [428/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [6/20], Step [429/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [6/20], Step [430/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [6/20], Step [431/2541], D Loss: 0.6502, G Loss: 2.0955\n",
      "Epoch [6/20], Step [432/2541], D Loss: 0.6502, G Loss: 2.0620\n",
      "Epoch [6/20], Step [433/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [6/20], Step [434/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [6/20], Step [435/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [6/20], Step [436/2541], D Loss: 0.6502, G Loss: 2.0639\n",
      "Epoch [6/20], Step [437/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [6/20], Step [438/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [6/20], Step [439/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [6/20], Step [440/2541], D Loss: 0.6502, G Loss: 2.0949\n",
      "Epoch [6/20], Step [441/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [6/20], Step [442/2541], D Loss: 0.6502, G Loss: 2.0527\n",
      "Epoch [6/20], Step [443/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [6/20], Step [444/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [6/20], Step [445/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [6/20], Step [446/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [6/20], Step [447/2541], D Loss: 0.6503, G Loss: 2.0759\n",
      "Epoch [6/20], Step [448/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [449/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [6/20], Step [450/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [6/20], Step [451/2541], D Loss: 0.6502, G Loss: 2.1017\n",
      "Epoch [6/20], Step [452/2541], D Loss: 0.6502, G Loss: 2.0719\n",
      "Epoch [6/20], Step [453/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [6/20], Step [454/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [6/20], Step [455/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [6/20], Step [456/2541], D Loss: 0.6502, G Loss: 2.1125\n",
      "Epoch [6/20], Step [457/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [6/20], Step [458/2541], D Loss: 0.6502, G Loss: 2.0696\n",
      "Epoch [6/20], Step [459/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [6/20], Step [460/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [6/20], Step [461/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [462/2541], D Loss: 0.6502, G Loss: 2.0750\n",
      "Epoch [6/20], Step [463/2541], D Loss: 0.6502, G Loss: 2.1007\n",
      "Epoch [6/20], Step [464/2541], D Loss: 0.6502, G Loss: 2.0969\n",
      "Epoch [6/20], Step [465/2541], D Loss: 0.6502, G Loss: 2.1066\n",
      "Epoch [6/20], Step [466/2541], D Loss: 0.6502, G Loss: 2.0683\n",
      "Epoch [6/20], Step [467/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [6/20], Step [468/2541], D Loss: 0.6508, G Loss: 2.1017\n",
      "Epoch [6/20], Step [469/2541], D Loss: 0.6521, G Loss: 2.0771\n",
      "Epoch [6/20], Step [470/2541], D Loss: 0.6503, G Loss: 2.0946\n",
      "Epoch [6/20], Step [471/2541], D Loss: 0.6512, G Loss: 2.1234\n",
      "Epoch [6/20], Step [472/2541], D Loss: 0.6503, G Loss: 2.0995\n",
      "Epoch [6/20], Step [473/2541], D Loss: 0.6505, G Loss: 2.0584\n",
      "Epoch [6/20], Step [474/2541], D Loss: 0.6504, G Loss: 2.0604\n",
      "Epoch [6/20], Step [475/2541], D Loss: 0.6502, G Loss: 2.1070\n",
      "Epoch [6/20], Step [476/2541], D Loss: 0.6503, G Loss: 2.0804\n",
      "Epoch [6/20], Step [477/2541], D Loss: 0.6503, G Loss: 2.0747\n",
      "Epoch [6/20], Step [478/2541], D Loss: 0.6503, G Loss: 2.0678\n",
      "Epoch [6/20], Step [479/2541], D Loss: 0.6502, G Loss: 2.0646\n",
      "Epoch [6/20], Step [480/2541], D Loss: 0.6503, G Loss: 2.1192\n",
      "Epoch [6/20], Step [481/2541], D Loss: 0.6504, G Loss: 2.0643\n",
      "Epoch [6/20], Step [482/2541], D Loss: 0.6502, G Loss: 2.1070\n",
      "Epoch [6/20], Step [483/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [6/20], Step [484/2541], D Loss: 0.6502, G Loss: 2.0703\n",
      "Epoch [6/20], Step [485/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [6/20], Step [486/2541], D Loss: 0.6502, G Loss: 2.0643\n",
      "Epoch [6/20], Step [487/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [6/20], Step [488/2541], D Loss: 0.6503, G Loss: 2.1178\n",
      "Epoch [6/20], Step [489/2541], D Loss: 0.6503, G Loss: 2.1094\n",
      "Epoch [6/20], Step [490/2541], D Loss: 0.6503, G Loss: 2.0707\n",
      "Epoch [6/20], Step [491/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [6/20], Step [492/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [6/20], Step [493/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [6/20], Step [494/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [6/20], Step [495/2541], D Loss: 0.6503, G Loss: 2.0861\n",
      "Epoch [6/20], Step [496/2541], D Loss: 0.6503, G Loss: 2.0825\n",
      "Epoch [6/20], Step [497/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [6/20], Step [498/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [6/20], Step [499/2541], D Loss: 0.6502, G Loss: 2.1064\n",
      "Epoch [6/20], Step [500/2541], D Loss: 0.6503, G Loss: 2.0730\n",
      "Epoch [6/20], Step [501/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [6/20], Step [502/2541], D Loss: 0.6503, G Loss: 2.0902\n",
      "Epoch [6/20], Step [503/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [6/20], Step [504/2541], D Loss: 0.6503, G Loss: 2.1110\n",
      "Epoch [6/20], Step [505/2541], D Loss: 0.6503, G Loss: 2.0786\n",
      "Epoch [6/20], Step [506/2541], D Loss: 0.6502, G Loss: 2.0620\n",
      "Epoch [6/20], Step [507/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [508/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [6/20], Step [509/2541], D Loss: 0.6503, G Loss: 2.1167\n",
      "Epoch [6/20], Step [510/2541], D Loss: 0.6503, G Loss: 2.0907\n",
      "Epoch [6/20], Step [511/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [6/20], Step [512/2541], D Loss: 0.6502, G Loss: 2.0701\n",
      "Epoch [6/20], Step [513/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [6/20], Step [514/2541], D Loss: 0.6502, G Loss: 2.0932\n",
      "Epoch [6/20], Step [515/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [6/20], Step [516/2541], D Loss: 0.6502, G Loss: 2.0537\n",
      "Epoch [6/20], Step [517/2541], D Loss: 0.6502, G Loss: 2.0675\n",
      "Epoch [6/20], Step [518/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [519/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [6/20], Step [520/2541], D Loss: 0.6502, G Loss: 2.1122\n",
      "Epoch [6/20], Step [521/2541], D Loss: 0.6503, G Loss: 2.0681\n",
      "Epoch [6/20], Step [522/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [6/20], Step [523/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [6/20], Step [524/2541], D Loss: 0.6502, G Loss: 2.0676\n",
      "Epoch [6/20], Step [525/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [6/20], Step [526/2541], D Loss: 0.6503, G Loss: 2.1259\n",
      "Epoch [6/20], Step [527/2541], D Loss: 0.6503, G Loss: 2.0924\n",
      "Epoch [6/20], Step [528/2541], D Loss: 0.6502, G Loss: 2.0614\n",
      "Epoch [6/20], Step [529/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [530/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [6/20], Step [531/2541], D Loss: 0.6502, G Loss: 2.0925\n",
      "Epoch [6/20], Step [532/2541], D Loss: 0.6502, G Loss: 2.1131\n",
      "Epoch [6/20], Step [533/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [6/20], Step [534/2541], D Loss: 0.6507, G Loss: 2.0763\n",
      "Epoch [6/20], Step [535/2541], D Loss: 0.6508, G Loss: 2.0980\n",
      "Epoch [6/20], Step [536/2541], D Loss: 0.6504, G Loss: 2.0745\n",
      "Epoch [6/20], Step [537/2541], D Loss: 0.6503, G Loss: 2.1066\n",
      "Epoch [6/20], Step [538/2541], D Loss: 0.6503, G Loss: 2.0771\n",
      "Epoch [6/20], Step [539/2541], D Loss: 0.6502, G Loss: 2.0750\n",
      "Epoch [6/20], Step [540/2541], D Loss: 0.6503, G Loss: 2.0518\n",
      "Epoch [6/20], Step [541/2541], D Loss: 0.6503, G Loss: 2.0733\n",
      "Epoch [6/20], Step [542/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [6/20], Step [543/2541], D Loss: 0.6502, G Loss: 2.1055\n",
      "Epoch [6/20], Step [544/2541], D Loss: 0.6502, G Loss: 2.1181\n",
      "Epoch [6/20], Step [545/2541], D Loss: 0.6503, G Loss: 2.0976\n",
      "Epoch [6/20], Step [546/2541], D Loss: 0.6503, G Loss: 2.0823\n",
      "Epoch [6/20], Step [547/2541], D Loss: 0.6503, G Loss: 2.0765\n",
      "Epoch [6/20], Step [548/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [6/20], Step [549/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [6/20], Step [550/2541], D Loss: 0.6502, G Loss: 2.0719\n",
      "Epoch [6/20], Step [551/2541], D Loss: 0.6502, G Loss: 2.0876\n",
      "Epoch [6/20], Step [552/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [6/20], Step [553/2541], D Loss: 0.6502, G Loss: 2.1156\n",
      "Epoch [6/20], Step [554/2541], D Loss: 0.6502, G Loss: 2.1100\n",
      "Epoch [6/20], Step [555/2541], D Loss: 0.6503, G Loss: 2.1056\n",
      "Epoch [6/20], Step [556/2541], D Loss: 0.6502, G Loss: 2.0984\n",
      "Epoch [6/20], Step [557/2541], D Loss: 0.6502, G Loss: 2.0566\n",
      "Epoch [6/20], Step [558/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [6/20], Step [559/2541], D Loss: 0.6502, G Loss: 2.0508\n",
      "Epoch [6/20], Step [560/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [6/20], Step [561/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [562/2541], D Loss: 0.6502, G Loss: 2.0678\n",
      "Epoch [6/20], Step [563/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [6/20], Step [564/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [6/20], Step [565/2541], D Loss: 0.6503, G Loss: 2.0914\n",
      "Epoch [6/20], Step [566/2541], D Loss: 0.6502, G Loss: 2.0410\n",
      "Epoch [6/20], Step [567/2541], D Loss: 0.6502, G Loss: 2.0626\n",
      "Epoch [6/20], Step [568/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [6/20], Step [569/2541], D Loss: 0.6502, G Loss: 2.0975\n",
      "Epoch [6/20], Step [570/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [6/20], Step [571/2541], D Loss: 0.6504, G Loss: 2.0955\n",
      "Epoch [6/20], Step [572/2541], D Loss: 0.6506, G Loss: 2.1080\n",
      "Epoch [6/20], Step [573/2541], D Loss: 0.6504, G Loss: 2.0705\n",
      "Epoch [6/20], Step [574/2541], D Loss: 0.6503, G Loss: 2.0700\n",
      "Epoch [6/20], Step [575/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [6/20], Step [576/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [6/20], Step [577/2541], D Loss: 0.6503, G Loss: 2.0811\n",
      "Epoch [6/20], Step [578/2541], D Loss: 0.6504, G Loss: 2.0984\n",
      "Epoch [6/20], Step [579/2541], D Loss: 0.6503, G Loss: 2.0599\n",
      "Epoch [6/20], Step [580/2541], D Loss: 0.6502, G Loss: 2.0627\n",
      "Epoch [6/20], Step [581/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [6/20], Step [582/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [6/20], Step [583/2541], D Loss: 0.6502, G Loss: 2.0672\n",
      "Epoch [6/20], Step [584/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [6/20], Step [585/2541], D Loss: 0.6502, G Loss: 2.1068\n",
      "Epoch [6/20], Step [586/2541], D Loss: 0.6503, G Loss: 2.0814\n",
      "Epoch [6/20], Step [587/2541], D Loss: 0.6502, G Loss: 2.1030\n",
      "Epoch [6/20], Step [588/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [589/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [6/20], Step [590/2541], D Loss: 0.6502, G Loss: 2.0656\n",
      "Epoch [6/20], Step [591/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [6/20], Step [592/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [6/20], Step [593/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [6/20], Step [594/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [6/20], Step [595/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [596/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [6/20], Step [597/2541], D Loss: 0.6502, G Loss: 2.0922\n",
      "Epoch [6/20], Step [598/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [599/2541], D Loss: 0.6502, G Loss: 2.0670\n",
      "Epoch [6/20], Step [600/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [6/20], Step [601/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [602/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [6/20], Step [603/2541], D Loss: 0.6502, G Loss: 2.0435\n",
      "Epoch [6/20], Step [604/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [6/20], Step [605/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [606/2541], D Loss: 0.6502, G Loss: 2.0703\n",
      "Epoch [6/20], Step [607/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [6/20], Step [608/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [6/20], Step [609/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [6/20], Step [610/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [6/20], Step [611/2541], D Loss: 0.6502, G Loss: 2.0662\n",
      "Epoch [6/20], Step [612/2541], D Loss: 0.6503, G Loss: 2.0711\n",
      "Epoch [6/20], Step [613/2541], D Loss: 0.6505, G Loss: 2.0824\n",
      "Epoch [6/20], Step [614/2541], D Loss: 0.6503, G Loss: 2.0896\n",
      "Epoch [6/20], Step [615/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [6/20], Step [616/2541], D Loss: 0.6503, G Loss: 2.0636\n",
      "Epoch [6/20], Step [617/2541], D Loss: 0.6503, G Loss: 2.0717\n",
      "Epoch [6/20], Step [618/2541], D Loss: 0.6502, G Loss: 2.1013\n",
      "Epoch [6/20], Step [619/2541], D Loss: 0.6502, G Loss: 2.1002\n",
      "Epoch [6/20], Step [620/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [621/2541], D Loss: 0.6502, G Loss: 2.0972\n",
      "Epoch [6/20], Step [622/2541], D Loss: 0.6503, G Loss: 2.0844\n",
      "Epoch [6/20], Step [623/2541], D Loss: 0.6503, G Loss: 2.0949\n",
      "Epoch [6/20], Step [624/2541], D Loss: 0.6503, G Loss: 2.0974\n",
      "Epoch [6/20], Step [625/2541], D Loss: 0.6503, G Loss: 2.0857\n",
      "Epoch [6/20], Step [626/2541], D Loss: 0.6503, G Loss: 2.0819\n",
      "Epoch [6/20], Step [627/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [6/20], Step [628/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [6/20], Step [629/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [630/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [6/20], Step [631/2541], D Loss: 0.6502, G Loss: 2.0938\n",
      "Epoch [6/20], Step [632/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [6/20], Step [633/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [634/2541], D Loss: 0.6502, G Loss: 2.0940\n",
      "Epoch [6/20], Step [635/2541], D Loss: 0.6502, G Loss: 2.0722\n",
      "Epoch [6/20], Step [636/2541], D Loss: 0.6502, G Loss: 2.0684\n",
      "Epoch [6/20], Step [637/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [6/20], Step [638/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [6/20], Step [639/2541], D Loss: 0.6503, G Loss: 2.0982\n",
      "Epoch [6/20], Step [640/2541], D Loss: 0.6503, G Loss: 2.0681\n",
      "Epoch [6/20], Step [641/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [642/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [6/20], Step [643/2541], D Loss: 0.6502, G Loss: 2.0599\n",
      "Epoch [6/20], Step [644/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [6/20], Step [645/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [646/2541], D Loss: 0.6502, G Loss: 2.0646\n",
      "Epoch [6/20], Step [647/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [6/20], Step [648/2541], D Loss: 0.6502, G Loss: 2.0734\n",
      "Epoch [6/20], Step [649/2541], D Loss: 0.6502, G Loss: 2.0907\n",
      "Epoch [6/20], Step [650/2541], D Loss: 0.6502, G Loss: 2.1025\n",
      "Epoch [6/20], Step [651/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [6/20], Step [652/2541], D Loss: 0.6503, G Loss: 2.0769\n",
      "Epoch [6/20], Step [653/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [6/20], Step [654/2541], D Loss: 0.6502, G Loss: 2.0904\n",
      "Epoch [6/20], Step [655/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [6/20], Step [656/2541], D Loss: 0.6502, G Loss: 2.1049\n",
      "Epoch [6/20], Step [657/2541], D Loss: 0.6502, G Loss: 2.1037\n",
      "Epoch [6/20], Step [658/2541], D Loss: 0.6503, G Loss: 2.0588\n",
      "Epoch [6/20], Step [659/2541], D Loss: 0.6503, G Loss: 2.0703\n",
      "Epoch [6/20], Step [660/2541], D Loss: 0.6502, G Loss: 2.0947\n",
      "Epoch [6/20], Step [661/2541], D Loss: 0.6502, G Loss: 2.0743\n",
      "Epoch [6/20], Step [662/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [6/20], Step [663/2541], D Loss: 0.6502, G Loss: 2.0976\n",
      "Epoch [6/20], Step [664/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [6/20], Step [665/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [6/20], Step [666/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [6/20], Step [667/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [668/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [6/20], Step [669/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [6/20], Step [670/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [6/20], Step [671/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [6/20], Step [672/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [6/20], Step [673/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [6/20], Step [674/2541], D Loss: 0.6502, G Loss: 2.0954\n",
      "Epoch [6/20], Step [675/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [6/20], Step [676/2541], D Loss: 0.6502, G Loss: 2.0734\n",
      "Epoch [6/20], Step [677/2541], D Loss: 0.6502, G Loss: 2.0639\n",
      "Epoch [6/20], Step [678/2541], D Loss: 0.6502, G Loss: 2.0692\n",
      "Epoch [6/20], Step [679/2541], D Loss: 0.6502, G Loss: 2.0904\n",
      "Epoch [6/20], Step [680/2541], D Loss: 0.6502, G Loss: 2.1021\n",
      "Epoch [6/20], Step [681/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [6/20], Step [682/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [6/20], Step [683/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [6/20], Step [684/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [6/20], Step [685/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [6/20], Step [686/2541], D Loss: 0.6502, G Loss: 2.0944\n",
      "Epoch [6/20], Step [687/2541], D Loss: 0.6502, G Loss: 2.0887\n",
      "Epoch [6/20], Step [688/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [6/20], Step [689/2541], D Loss: 0.6502, G Loss: 2.0909\n",
      "Epoch [6/20], Step [690/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [6/20], Step [691/2541], D Loss: 0.6502, G Loss: 2.0573\n",
      "Epoch [6/20], Step [692/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [6/20], Step [693/2541], D Loss: 0.6503, G Loss: 2.0933\n",
      "Epoch [6/20], Step [694/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [695/2541], D Loss: 0.6502, G Loss: 2.0600\n",
      "Epoch [6/20], Step [696/2541], D Loss: 0.6502, G Loss: 2.0667\n",
      "Epoch [6/20], Step [697/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [698/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [6/20], Step [699/2541], D Loss: 0.6502, G Loss: 2.1004\n",
      "Epoch [6/20], Step [700/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [701/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [702/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [6/20], Step [703/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [6/20], Step [704/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [6/20], Step [705/2541], D Loss: 0.6502, G Loss: 2.0956\n",
      "Epoch [6/20], Step [706/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [6/20], Step [707/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [6/20], Step [708/2541], D Loss: 0.6502, G Loss: 2.0690\n",
      "Epoch [6/20], Step [709/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [710/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [6/20], Step [711/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [6/20], Step [712/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [6/20], Step [713/2541], D Loss: 0.6502, G Loss: 2.0511\n",
      "Epoch [6/20], Step [714/2541], D Loss: 0.6502, G Loss: 2.0901\n",
      "Epoch [6/20], Step [715/2541], D Loss: 0.6502, G Loss: 2.1043\n",
      "Epoch [6/20], Step [716/2541], D Loss: 0.6502, G Loss: 2.0940\n",
      "Epoch [6/20], Step [717/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [6/20], Step [718/2541], D Loss: 0.6502, G Loss: 2.0743\n",
      "Epoch [6/20], Step [719/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [6/20], Step [720/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [6/20], Step [721/2541], D Loss: 0.6502, G Loss: 2.0626\n",
      "Epoch [6/20], Step [722/2541], D Loss: 0.6502, G Loss: 2.0968\n",
      "Epoch [6/20], Step [723/2541], D Loss: 0.6506, G Loss: 2.0822\n",
      "Epoch [6/20], Step [724/2541], D Loss: 0.6506, G Loss: 2.0749\n",
      "Epoch [6/20], Step [725/2541], D Loss: 0.6504, G Loss: 2.0596\n",
      "Epoch [6/20], Step [726/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [6/20], Step [727/2541], D Loss: 0.6502, G Loss: 2.0965\n",
      "Epoch [6/20], Step [728/2541], D Loss: 0.6502, G Loss: 2.0976\n",
      "Epoch [6/20], Step [729/2541], D Loss: 0.6504, G Loss: 2.0899\n",
      "Epoch [6/20], Step [730/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [6/20], Step [731/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [6/20], Step [732/2541], D Loss: 0.6502, G Loss: 2.0625\n",
      "Epoch [6/20], Step [733/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [6/20], Step [734/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [6/20], Step [735/2541], D Loss: 0.6503, G Loss: 2.1241\n",
      "Epoch [6/20], Step [736/2541], D Loss: 0.6504, G Loss: 2.1050\n",
      "Epoch [6/20], Step [737/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [6/20], Step [738/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [6/20], Step [739/2541], D Loss: 0.6502, G Loss: 2.1058\n",
      "Epoch [6/20], Step [740/2541], D Loss: 0.6502, G Loss: 2.0938\n",
      "Epoch [6/20], Step [741/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [6/20], Step [742/2541], D Loss: 0.6502, G Loss: 2.1103\n",
      "Epoch [6/20], Step [743/2541], D Loss: 0.6503, G Loss: 2.0671\n",
      "Epoch [6/20], Step [744/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [6/20], Step [745/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [6/20], Step [746/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [6/20], Step [747/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [748/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [6/20], Step [749/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [6/20], Step [750/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [6/20], Step [751/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [6/20], Step [752/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [6/20], Step [753/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [754/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [6/20], Step [755/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [6/20], Step [756/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [6/20], Step [757/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [6/20], Step [758/2541], D Loss: 0.6502, G Loss: 2.1208\n",
      "Epoch [6/20], Step [759/2541], D Loss: 0.6503, G Loss: 2.0716\n",
      "Epoch [6/20], Step [760/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [6/20], Step [761/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [6/20], Step [762/2541], D Loss: 0.6502, G Loss: 2.0729\n",
      "Epoch [6/20], Step [763/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [6/20], Step [764/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [6/20], Step [765/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [6/20], Step [766/2541], D Loss: 0.6502, G Loss: 2.0714\n",
      "Epoch [6/20], Step [767/2541], D Loss: 0.6502, G Loss: 2.0951\n",
      "Epoch [6/20], Step [768/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [6/20], Step [769/2541], D Loss: 0.6502, G Loss: 2.0473\n",
      "Epoch [6/20], Step [770/2541], D Loss: 0.6503, G Loss: 2.0853\n",
      "Epoch [6/20], Step [771/2541], D Loss: 0.6502, G Loss: 2.0656\n",
      "Epoch [6/20], Step [772/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [6/20], Step [773/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [6/20], Step [774/2541], D Loss: 0.6502, G Loss: 2.0472\n",
      "Epoch [6/20], Step [775/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [776/2541], D Loss: 0.6502, G Loss: 2.0940\n",
      "Epoch [6/20], Step [777/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [6/20], Step [778/2541], D Loss: 0.6502, G Loss: 2.0650\n",
      "Epoch [6/20], Step [779/2541], D Loss: 0.6502, G Loss: 2.0876\n",
      "Epoch [6/20], Step [780/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [6/20], Step [781/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [6/20], Step [782/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [6/20], Step [783/2541], D Loss: 0.6502, G Loss: 2.0706\n",
      "Epoch [6/20], Step [784/2541], D Loss: 0.6502, G Loss: 2.0981\n",
      "Epoch [6/20], Step [785/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [6/20], Step [786/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [787/2541], D Loss: 0.6502, G Loss: 2.1013\n",
      "Epoch [6/20], Step [788/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [6/20], Step [789/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [6/20], Step [790/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [6/20], Step [791/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [6/20], Step [792/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [6/20], Step [793/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [6/20], Step [794/2541], D Loss: 0.6503, G Loss: 2.0891\n",
      "Epoch [6/20], Step [795/2541], D Loss: 0.6503, G Loss: 2.0859\n",
      "Epoch [6/20], Step [796/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [6/20], Step [797/2541], D Loss: 0.6502, G Loss: 2.0966\n",
      "Epoch [6/20], Step [798/2541], D Loss: 0.6502, G Loss: 2.0709\n",
      "Epoch [6/20], Step [799/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [6/20], Step [800/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [6/20], Step [801/2541], D Loss: 0.6502, G Loss: 2.0725\n",
      "Epoch [6/20], Step [802/2541], D Loss: 0.6502, G Loss: 2.1108\n",
      "Epoch [6/20], Step [803/2541], D Loss: 0.6502, G Loss: 2.0564\n",
      "Epoch [6/20], Step [804/2541], D Loss: 0.6502, G Loss: 2.0711\n",
      "Epoch [6/20], Step [805/2541], D Loss: 0.6503, G Loss: 2.1083\n",
      "Epoch [6/20], Step [806/2541], D Loss: 0.6503, G Loss: 2.0742\n",
      "Epoch [6/20], Step [807/2541], D Loss: 0.6503, G Loss: 2.1126\n",
      "Epoch [6/20], Step [808/2541], D Loss: 0.6502, G Loss: 2.0910\n",
      "Epoch [6/20], Step [809/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [6/20], Step [810/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [6/20], Step [811/2541], D Loss: 0.6505, G Loss: 2.0577\n",
      "Epoch [6/20], Step [812/2541], D Loss: 0.6504, G Loss: 2.0896\n",
      "Epoch [6/20], Step [813/2541], D Loss: 0.6503, G Loss: 2.0793\n",
      "Epoch [6/20], Step [814/2541], D Loss: 0.6503, G Loss: 2.0706\n",
      "Epoch [6/20], Step [815/2541], D Loss: 0.6503, G Loss: 2.0826\n",
      "Epoch [6/20], Step [816/2541], D Loss: 0.6502, G Loss: 2.0697\n",
      "Epoch [6/20], Step [817/2541], D Loss: 0.6502, G Loss: 2.0716\n",
      "Epoch [6/20], Step [818/2541], D Loss: 0.6502, G Loss: 2.0720\n",
      "Epoch [6/20], Step [819/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [6/20], Step [820/2541], D Loss: 0.6502, G Loss: 2.1045\n",
      "Epoch [6/20], Step [821/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [6/20], Step [822/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [6/20], Step [823/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [824/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [6/20], Step [825/2541], D Loss: 0.6502, G Loss: 2.1062\n",
      "Epoch [6/20], Step [826/2541], D Loss: 0.6503, G Loss: 2.0833\n",
      "Epoch [6/20], Step [827/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [6/20], Step [828/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [6/20], Step [829/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [6/20], Step [830/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [831/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [6/20], Step [832/2541], D Loss: 0.6502, G Loss: 2.0729\n",
      "Epoch [6/20], Step [833/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [6/20], Step [834/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [6/20], Step [835/2541], D Loss: 0.6502, G Loss: 2.0679\n",
      "Epoch [6/20], Step [836/2541], D Loss: 0.6503, G Loss: 2.0853\n",
      "Epoch [6/20], Step [837/2541], D Loss: 0.6502, G Loss: 2.0968\n",
      "Epoch [6/20], Step [838/2541], D Loss: 0.6503, G Loss: 2.0913\n",
      "Epoch [6/20], Step [839/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [6/20], Step [840/2541], D Loss: 0.6502, G Loss: 2.0662\n",
      "Epoch [6/20], Step [841/2541], D Loss: 0.6502, G Loss: 2.0923\n",
      "Epoch [6/20], Step [842/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [6/20], Step [843/2541], D Loss: 0.6502, G Loss: 2.0971\n",
      "Epoch [6/20], Step [844/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [6/20], Step [845/2541], D Loss: 0.6502, G Loss: 2.0617\n",
      "Epoch [6/20], Step [846/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [6/20], Step [847/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [6/20], Step [848/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [849/2541], D Loss: 0.6502, G Loss: 2.0951\n",
      "Epoch [6/20], Step [850/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [6/20], Step [851/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [6/20], Step [852/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [853/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [6/20], Step [854/2541], D Loss: 0.6502, G Loss: 2.0693\n",
      "Epoch [6/20], Step [855/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [6/20], Step [856/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [6/20], Step [857/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [6/20], Step [858/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [6/20], Step [859/2541], D Loss: 0.6502, G Loss: 2.0996\n",
      "Epoch [6/20], Step [860/2541], D Loss: 0.6502, G Loss: 2.0978\n",
      "Epoch [6/20], Step [861/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [6/20], Step [862/2541], D Loss: 0.6502, G Loss: 2.0655\n",
      "Epoch [6/20], Step [863/2541], D Loss: 0.6502, G Loss: 2.0750\n",
      "Epoch [6/20], Step [864/2541], D Loss: 0.6502, G Loss: 2.0740\n",
      "Epoch [6/20], Step [865/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [6/20], Step [866/2541], D Loss: 0.6502, G Loss: 2.1158\n",
      "Epoch [6/20], Step [867/2541], D Loss: 0.6503, G Loss: 2.0662\n",
      "Epoch [6/20], Step [868/2541], D Loss: 0.6502, G Loss: 2.0694\n",
      "Epoch [6/20], Step [869/2541], D Loss: 0.6502, G Loss: 2.1096\n",
      "Epoch [6/20], Step [870/2541], D Loss: 0.6503, G Loss: 2.0824\n",
      "Epoch [6/20], Step [871/2541], D Loss: 0.6502, G Loss: 2.0693\n",
      "Epoch [6/20], Step [872/2541], D Loss: 0.6506, G Loss: 2.0815\n",
      "Epoch [6/20], Step [873/2541], D Loss: 0.6507, G Loss: 2.0781\n",
      "Epoch [6/20], Step [874/2541], D Loss: 0.6503, G Loss: 2.0826\n",
      "Epoch [6/20], Step [875/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [876/2541], D Loss: 0.6502, G Loss: 2.1067\n",
      "Epoch [6/20], Step [877/2541], D Loss: 0.6503, G Loss: 2.0720\n",
      "Epoch [6/20], Step [878/2541], D Loss: 0.6502, G Loss: 2.0671\n",
      "Epoch [6/20], Step [879/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [6/20], Step [880/2541], D Loss: 0.6503, G Loss: 2.0903\n",
      "Epoch [6/20], Step [881/2541], D Loss: 0.6504, G Loss: 2.0775\n",
      "Epoch [6/20], Step [882/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [6/20], Step [883/2541], D Loss: 0.6502, G Loss: 2.0670\n",
      "Epoch [6/20], Step [884/2541], D Loss: 0.6505, G Loss: 2.0744\n",
      "Epoch [6/20], Step [885/2541], D Loss: 0.6503, G Loss: 2.0853\n",
      "Epoch [6/20], Step [886/2541], D Loss: 0.6505, G Loss: 2.0783\n",
      "Epoch [6/20], Step [887/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [888/2541], D Loss: 0.6502, G Loss: 2.0932\n",
      "Epoch [6/20], Step [889/2541], D Loss: 0.6503, G Loss: 2.0909\n",
      "Epoch [6/20], Step [890/2541], D Loss: 0.6502, G Loss: 2.0645\n",
      "Epoch [6/20], Step [891/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [892/2541], D Loss: 0.6503, G Loss: 2.0673\n",
      "Epoch [6/20], Step [893/2541], D Loss: 0.6503, G Loss: 2.0966\n",
      "Epoch [6/20], Step [894/2541], D Loss: 0.6505, G Loss: 2.0832\n",
      "Epoch [6/20], Step [895/2541], D Loss: 0.6504, G Loss: 2.0700\n",
      "Epoch [6/20], Step [896/2541], D Loss: 0.6504, G Loss: 2.0939\n",
      "Epoch [6/20], Step [897/2541], D Loss: 0.6503, G Loss: 2.0859\n",
      "Epoch [6/20], Step [898/2541], D Loss: 0.6504, G Loss: 2.0951\n",
      "Epoch [6/20], Step [899/2541], D Loss: 0.6503, G Loss: 2.0972\n",
      "Epoch [6/20], Step [900/2541], D Loss: 0.6503, G Loss: 2.0576\n",
      "Epoch [6/20], Step [901/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [6/20], Step [902/2541], D Loss: 0.6504, G Loss: 2.1139\n",
      "Epoch [6/20], Step [903/2541], D Loss: 0.6504, G Loss: 2.0754\n",
      "Epoch [6/20], Step [904/2541], D Loss: 0.6502, G Loss: 2.0947\n",
      "Epoch [6/20], Step [905/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [6/20], Step [906/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [6/20], Step [907/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [6/20], Step [908/2541], D Loss: 0.6502, G Loss: 2.0698\n",
      "Epoch [6/20], Step [909/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [910/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [6/20], Step [911/2541], D Loss: 0.6503, G Loss: 2.1042\n",
      "Epoch [6/20], Step [912/2541], D Loss: 0.6503, G Loss: 2.0860\n",
      "Epoch [6/20], Step [913/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [6/20], Step [914/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [6/20], Step [915/2541], D Loss: 0.6502, G Loss: 2.1109\n",
      "Epoch [6/20], Step [916/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [6/20], Step [917/2541], D Loss: 0.6502, G Loss: 2.0731\n",
      "Epoch [6/20], Step [918/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [6/20], Step [919/2541], D Loss: 0.6503, G Loss: 2.0637\n",
      "Epoch [6/20], Step [920/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [6/20], Step [921/2541], D Loss: 0.6502, G Loss: 2.0696\n",
      "Epoch [6/20], Step [922/2541], D Loss: 0.6502, G Loss: 2.0989\n",
      "Epoch [6/20], Step [923/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [6/20], Step [924/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [6/20], Step [925/2541], D Loss: 0.6503, G Loss: 2.0903\n",
      "Epoch [6/20], Step [926/2541], D Loss: 0.6502, G Loss: 2.0907\n",
      "Epoch [6/20], Step [927/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [6/20], Step [928/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [929/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [6/20], Step [930/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [6/20], Step [931/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [6/20], Step [932/2541], D Loss: 0.6504, G Loss: 2.0771\n",
      "Epoch [6/20], Step [933/2541], D Loss: 0.6503, G Loss: 2.0856\n",
      "Epoch [6/20], Step [934/2541], D Loss: 0.6503, G Loss: 2.0877\n",
      "Epoch [6/20], Step [935/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [6/20], Step [936/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [937/2541], D Loss: 0.6502, G Loss: 2.0724\n",
      "Epoch [6/20], Step [938/2541], D Loss: 0.6502, G Loss: 2.0811\n",
      "Epoch [6/20], Step [939/2541], D Loss: 0.6502, G Loss: 2.0976\n",
      "Epoch [6/20], Step [940/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [6/20], Step [941/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [6/20], Step [942/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [6/20], Step [943/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [6/20], Step [944/2541], D Loss: 0.6502, G Loss: 2.0743\n",
      "Epoch [6/20], Step [945/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [6/20], Step [946/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [6/20], Step [947/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [6/20], Step [948/2541], D Loss: 0.6502, G Loss: 2.0675\n",
      "Epoch [6/20], Step [949/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [6/20], Step [950/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [6/20], Step [951/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [6/20], Step [952/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [6/20], Step [953/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [6/20], Step [954/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [6/20], Step [955/2541], D Loss: 0.6503, G Loss: 2.0916\n",
      "Epoch [6/20], Step [956/2541], D Loss: 0.6502, G Loss: 2.0965\n",
      "Epoch [6/20], Step [957/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [6/20], Step [958/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [6/20], Step [959/2541], D Loss: 0.6503, G Loss: 2.0990\n",
      "Epoch [6/20], Step [960/2541], D Loss: 0.6503, G Loss: 2.0860\n",
      "Epoch [6/20], Step [961/2541], D Loss: 0.6502, G Loss: 2.0724\n",
      "Epoch [6/20], Step [962/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [963/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [6/20], Step [964/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [6/20], Step [965/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [6/20], Step [966/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [6/20], Step [967/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [6/20], Step [968/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [6/20], Step [969/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [6/20], Step [970/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [971/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [972/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [6/20], Step [973/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [6/20], Step [974/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [6/20], Step [975/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [976/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [977/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [6/20], Step [978/2541], D Loss: 0.6502, G Loss: 2.0717\n",
      "Epoch [6/20], Step [979/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [980/2541], D Loss: 0.6502, G Loss: 2.0703\n",
      "Epoch [6/20], Step [981/2541], D Loss: 0.6502, G Loss: 2.1004\n",
      "Epoch [6/20], Step [982/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [6/20], Step [983/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [6/20], Step [984/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [6/20], Step [985/2541], D Loss: 0.6502, G Loss: 2.0997\n",
      "Epoch [6/20], Step [986/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [6/20], Step [987/2541], D Loss: 0.6503, G Loss: 2.1275\n",
      "Epoch [6/20], Step [988/2541], D Loss: 0.6504, G Loss: 2.0788\n",
      "Epoch [6/20], Step [989/2541], D Loss: 0.6504, G Loss: 2.0892\n",
      "Epoch [6/20], Step [990/2541], D Loss: 0.6504, G Loss: 2.0667\n",
      "Epoch [6/20], Step [991/2541], D Loss: 0.6504, G Loss: 2.0732\n",
      "Epoch [6/20], Step [992/2541], D Loss: 0.6503, G Loss: 2.0869\n",
      "Epoch [6/20], Step [993/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [6/20], Step [994/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [6/20], Step [995/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [6/20], Step [996/2541], D Loss: 0.6502, G Loss: 2.0711\n",
      "Epoch [6/20], Step [997/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [6/20], Step [998/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [6/20], Step [999/2541], D Loss: 0.6502, G Loss: 2.1002\n",
      "Epoch [6/20], Step [1000/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [1001/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [6/20], Step [1002/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [6/20], Step [1003/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [6/20], Step [1004/2541], D Loss: 0.6502, G Loss: 2.0816\n",
      "Epoch [6/20], Step [1005/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [6/20], Step [1006/2541], D Loss: 0.6502, G Loss: 2.0811\n",
      "Epoch [6/20], Step [1007/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [6/20], Step [1008/2541], D Loss: 0.6502, G Loss: 2.0887\n",
      "Epoch [6/20], Step [1009/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [6/20], Step [1010/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [6/20], Step [1011/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [6/20], Step [1012/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [6/20], Step [1013/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [6/20], Step [1014/2541], D Loss: 0.6502, G Loss: 2.0731\n",
      "Epoch [6/20], Step [1015/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [1016/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [6/20], Step [1017/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [6/20], Step [1018/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [1019/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [6/20], Step [1020/2541], D Loss: 0.6502, G Loss: 2.0945\n",
      "Epoch [6/20], Step [1021/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [6/20], Step [1022/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [6/20], Step [1023/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [6/20], Step [1024/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [6/20], Step [1025/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [6/20], Step [1026/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [1027/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [6/20], Step [1028/2541], D Loss: 0.6502, G Loss: 2.0816\n",
      "Epoch [6/20], Step [1029/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [6/20], Step [1030/2541], D Loss: 0.6503, G Loss: 2.0795\n",
      "Epoch [6/20], Step [1031/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [1032/2541], D Loss: 0.6502, G Loss: 2.0731\n",
      "Epoch [6/20], Step [1033/2541], D Loss: 0.6502, G Loss: 2.0722\n",
      "Epoch [6/20], Step [1034/2541], D Loss: 0.6502, G Loss: 2.0614\n",
      "Epoch [6/20], Step [1035/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [6/20], Step [1036/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [1037/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [6/20], Step [1038/2541], D Loss: 0.6502, G Loss: 2.0681\n",
      "Epoch [6/20], Step [1039/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [6/20], Step [1040/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [6/20], Step [1041/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [6/20], Step [1042/2541], D Loss: 0.6503, G Loss: 2.0743\n",
      "Epoch [6/20], Step [1043/2541], D Loss: 0.6503, G Loss: 2.0925\n",
      "Epoch [6/20], Step [1044/2541], D Loss: 0.6502, G Loss: 2.1120\n",
      "Epoch [6/20], Step [1045/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [6/20], Step [1046/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [6/20], Step [1047/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [6/20], Step [1048/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [1049/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [6/20], Step [1050/2541], D Loss: 0.6503, G Loss: 2.0803\n",
      "Epoch [6/20], Step [1051/2541], D Loss: 0.6502, G Loss: 2.0953\n",
      "Epoch [6/20], Step [1052/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [6/20], Step [1053/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [6/20], Step [1054/2541], D Loss: 0.6502, G Loss: 2.0722\n",
      "Epoch [6/20], Step [1055/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [6/20], Step [1056/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [6/20], Step [1057/2541], D Loss: 0.6502, G Loss: 2.0692\n",
      "Epoch [6/20], Step [1058/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [6/20], Step [1059/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [1060/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1061/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [6/20], Step [1062/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [6/20], Step [1063/2541], D Loss: 0.6502, G Loss: 2.0639\n",
      "Epoch [6/20], Step [1064/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [6/20], Step [1065/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [6/20], Step [1066/2541], D Loss: 0.6503, G Loss: 2.0776\n",
      "Epoch [6/20], Step [1067/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [6/20], Step [1068/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [6/20], Step [1069/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [6/20], Step [1070/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [6/20], Step [1071/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1072/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [6/20], Step [1073/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [1074/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [1075/2541], D Loss: 0.6502, G Loss: 2.0973\n",
      "Epoch [6/20], Step [1076/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [6/20], Step [1077/2541], D Loss: 0.6502, G Loss: 2.0711\n",
      "Epoch [6/20], Step [1078/2541], D Loss: 0.6503, G Loss: 2.0955\n",
      "Epoch [6/20], Step [1079/2541], D Loss: 0.6502, G Loss: 2.0994\n",
      "Epoch [6/20], Step [1080/2541], D Loss: 0.6504, G Loss: 2.0754\n",
      "Epoch [6/20], Step [1081/2541], D Loss: 0.6504, G Loss: 2.0747\n",
      "Epoch [6/20], Step [1082/2541], D Loss: 0.6505, G Loss: 2.0983\n",
      "Epoch [6/20], Step [1083/2541], D Loss: 0.6504, G Loss: 2.0796\n",
      "Epoch [6/20], Step [1084/2541], D Loss: 0.6502, G Loss: 2.0704\n",
      "Epoch [6/20], Step [1085/2541], D Loss: 0.6502, G Loss: 2.0480\n",
      "Epoch [6/20], Step [1086/2541], D Loss: 0.6503, G Loss: 2.0880\n",
      "Epoch [6/20], Step [1087/2541], D Loss: 0.6503, G Loss: 2.0768\n",
      "Epoch [6/20], Step [1088/2541], D Loss: 0.6502, G Loss: 2.0530\n",
      "Epoch [6/20], Step [1089/2541], D Loss: 0.6503, G Loss: 2.0987\n",
      "Epoch [6/20], Step [1090/2541], D Loss: 0.6503, G Loss: 2.0759\n",
      "Epoch [6/20], Step [1091/2541], D Loss: 0.6502, G Loss: 2.0669\n",
      "Epoch [6/20], Step [1092/2541], D Loss: 0.6504, G Loss: 2.1080\n",
      "Epoch [6/20], Step [1093/2541], D Loss: 0.6503, G Loss: 2.0740\n",
      "Epoch [6/20], Step [1094/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [6/20], Step [1095/2541], D Loss: 0.6502, G Loss: 2.0944\n",
      "Epoch [6/20], Step [1096/2541], D Loss: 0.6503, G Loss: 2.0736\n",
      "Epoch [6/20], Step [1097/2541], D Loss: 0.6503, G Loss: 2.0874\n",
      "Epoch [6/20], Step [1098/2541], D Loss: 0.6502, G Loss: 2.1123\n",
      "Epoch [6/20], Step [1099/2541], D Loss: 0.6504, G Loss: 2.0671\n",
      "Epoch [6/20], Step [1100/2541], D Loss: 0.6505, G Loss: 2.0949\n",
      "Epoch [6/20], Step [1101/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [1102/2541], D Loss: 0.6502, G Loss: 2.0701\n",
      "Epoch [6/20], Step [1103/2541], D Loss: 0.6503, G Loss: 2.1039\n",
      "Epoch [6/20], Step [1104/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [6/20], Step [1105/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [6/20], Step [1106/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [6/20], Step [1107/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [6/20], Step [1108/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [6/20], Step [1109/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [6/20], Step [1110/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [6/20], Step [1111/2541], D Loss: 0.6503, G Loss: 2.0929\n",
      "Epoch [6/20], Step [1112/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [6/20], Step [1113/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [6/20], Step [1114/2541], D Loss: 0.6503, G Loss: 2.0808\n",
      "Epoch [6/20], Step [1115/2541], D Loss: 0.6503, G Loss: 2.0946\n",
      "Epoch [6/20], Step [1116/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [6/20], Step [1117/2541], D Loss: 0.6502, G Loss: 2.0666\n",
      "Epoch [6/20], Step [1118/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [6/20], Step [1119/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [6/20], Step [1120/2541], D Loss: 0.6504, G Loss: 2.0776\n",
      "Epoch [6/20], Step [1121/2541], D Loss: 0.6504, G Loss: 2.0799\n",
      "Epoch [6/20], Step [1122/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [6/20], Step [1123/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [6/20], Step [1124/2541], D Loss: 0.6502, G Loss: 2.0543\n",
      "Epoch [6/20], Step [1125/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1126/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [6/20], Step [1127/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [6/20], Step [1128/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [6/20], Step [1129/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [1130/2541], D Loss: 0.6502, G Loss: 2.0943\n",
      "Epoch [6/20], Step [1131/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [6/20], Step [1132/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [1133/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [6/20], Step [1134/2541], D Loss: 0.6502, G Loss: 2.1022\n",
      "Epoch [6/20], Step [1135/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [6/20], Step [1136/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1137/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [6/20], Step [1138/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [6/20], Step [1139/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [6/20], Step [1140/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [6/20], Step [1141/2541], D Loss: 0.6504, G Loss: 2.0815\n",
      "Epoch [6/20], Step [1142/2541], D Loss: 0.6503, G Loss: 2.0880\n",
      "Epoch [6/20], Step [1143/2541], D Loss: 0.6504, G Loss: 2.0909\n",
      "Epoch [6/20], Step [1144/2541], D Loss: 0.6503, G Loss: 2.0839\n",
      "Epoch [6/20], Step [1145/2541], D Loss: 0.6503, G Loss: 2.0828\n",
      "Epoch [6/20], Step [1146/2541], D Loss: 0.6503, G Loss: 2.0824\n",
      "Epoch [6/20], Step [1147/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [1148/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [6/20], Step [1149/2541], D Loss: 0.6502, G Loss: 2.0973\n",
      "Epoch [6/20], Step [1150/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [6/20], Step [1151/2541], D Loss: 0.6502, G Loss: 2.0702\n",
      "Epoch [6/20], Step [1152/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [6/20], Step [1153/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [6/20], Step [1154/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [6/20], Step [1155/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [6/20], Step [1156/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [6/20], Step [1157/2541], D Loss: 0.6502, G Loss: 2.0991\n",
      "Epoch [6/20], Step [1158/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [6/20], Step [1159/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [6/20], Step [1160/2541], D Loss: 0.6502, G Loss: 2.0651\n",
      "Epoch [6/20], Step [1161/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [6/20], Step [1162/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [6/20], Step [1163/2541], D Loss: 0.6502, G Loss: 2.1086\n",
      "Epoch [6/20], Step [1164/2541], D Loss: 0.6503, G Loss: 2.0661\n",
      "Epoch [6/20], Step [1165/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [6/20], Step [1166/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [6/20], Step [1167/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [6/20], Step [1168/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [6/20], Step [1169/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [6/20], Step [1170/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [6/20], Step [1171/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [6/20], Step [1172/2541], D Loss: 0.6502, G Loss: 2.0955\n",
      "Epoch [6/20], Step [1173/2541], D Loss: 0.6502, G Loss: 2.1131\n",
      "Epoch [6/20], Step [1174/2541], D Loss: 0.6503, G Loss: 2.0714\n",
      "Epoch [6/20], Step [1175/2541], D Loss: 0.6504, G Loss: 2.0865\n",
      "Epoch [6/20], Step [1176/2541], D Loss: 0.6504, G Loss: 2.0847\n",
      "Epoch [6/20], Step [1177/2541], D Loss: 0.6504, G Loss: 2.0844\n",
      "Epoch [6/20], Step [1178/2541], D Loss: 0.6507, G Loss: 2.0883\n",
      "Epoch [6/20], Step [1179/2541], D Loss: 0.6509, G Loss: 2.0965\n",
      "Epoch [6/20], Step [1180/2541], D Loss: 0.6506, G Loss: 2.0901\n",
      "Epoch [6/20], Step [1181/2541], D Loss: 0.6503, G Loss: 2.0672\n",
      "Epoch [6/20], Step [1182/2541], D Loss: 0.6504, G Loss: 2.0977\n",
      "Epoch [6/20], Step [1183/2541], D Loss: 0.6503, G Loss: 2.0776\n",
      "Epoch [6/20], Step [1184/2541], D Loss: 0.6503, G Loss: 2.0803\n",
      "Epoch [6/20], Step [1185/2541], D Loss: 0.6502, G Loss: 2.0985\n",
      "Epoch [6/20], Step [1186/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [6/20], Step [1187/2541], D Loss: 0.6503, G Loss: 2.0815\n",
      "Epoch [6/20], Step [1188/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [6/20], Step [1189/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [6/20], Step [1190/2541], D Loss: 0.6502, G Loss: 2.0725\n",
      "Epoch [6/20], Step [1191/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [6/20], Step [1192/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [6/20], Step [1193/2541], D Loss: 0.6502, G Loss: 2.0610\n",
      "Epoch [6/20], Step [1194/2541], D Loss: 0.6503, G Loss: 2.0953\n",
      "Epoch [6/20], Step [1195/2541], D Loss: 0.6503, G Loss: 2.0854\n",
      "Epoch [6/20], Step [1196/2541], D Loss: 0.6504, G Loss: 2.0792\n",
      "Epoch [6/20], Step [1197/2541], D Loss: 0.6503, G Loss: 2.1044\n",
      "Epoch [6/20], Step [1198/2541], D Loss: 0.6504, G Loss: 2.1011\n",
      "Epoch [6/20], Step [1199/2541], D Loss: 0.6508, G Loss: 2.0515\n",
      "Epoch [6/20], Step [1200/2541], D Loss: 0.6505, G Loss: 2.0928\n",
      "Epoch [6/20], Step [1201/2541], D Loss: 0.6503, G Loss: 2.0931\n",
      "Epoch [6/20], Step [1202/2541], D Loss: 0.6504, G Loss: 2.0750\n",
      "Epoch [6/20], Step [1203/2541], D Loss: 0.6503, G Loss: 2.0830\n",
      "Epoch [6/20], Step [1204/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [6/20], Step [1205/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [6/20], Step [1206/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [6/20], Step [1207/2541], D Loss: 0.6503, G Loss: 2.0981\n",
      "Epoch [6/20], Step [1208/2541], D Loss: 0.6503, G Loss: 2.0849\n",
      "Epoch [6/20], Step [1209/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [6/20], Step [1210/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [6/20], Step [1211/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [6/20], Step [1212/2541], D Loss: 0.6502, G Loss: 2.0729\n",
      "Epoch [6/20], Step [1213/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [6/20], Step [1214/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [6/20], Step [1215/2541], D Loss: 0.6502, G Loss: 2.0951\n",
      "Epoch [6/20], Step [1216/2541], D Loss: 0.6503, G Loss: 2.1074\n",
      "Epoch [6/20], Step [1217/2541], D Loss: 0.6504, G Loss: 2.0659\n",
      "Epoch [6/20], Step [1218/2541], D Loss: 0.6506, G Loss: 2.1444\n",
      "Epoch [6/20], Step [1219/2541], D Loss: 0.6506, G Loss: 2.0737\n",
      "Epoch [6/20], Step [1220/2541], D Loss: 0.6502, G Loss: 2.0511\n",
      "Epoch [6/20], Step [1221/2541], D Loss: 0.6504, G Loss: 2.1059\n",
      "Epoch [6/20], Step [1222/2541], D Loss: 0.6505, G Loss: 2.0401\n",
      "Epoch [6/20], Step [1223/2541], D Loss: 0.6504, G Loss: 2.0870\n",
      "Epoch [6/20], Step [1224/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [6/20], Step [1225/2541], D Loss: 0.6502, G Loss: 2.0323\n",
      "Epoch [6/20], Step [1226/2541], D Loss: 0.6503, G Loss: 2.0944\n",
      "Epoch [6/20], Step [1227/2541], D Loss: 0.6502, G Loss: 2.1223\n",
      "Epoch [6/20], Step [1228/2541], D Loss: 0.6504, G Loss: 2.0603\n",
      "Epoch [6/20], Step [1229/2541], D Loss: 0.6504, G Loss: 2.0822\n",
      "Epoch [6/20], Step [1230/2541], D Loss: 0.6503, G Loss: 2.0929\n",
      "Epoch [6/20], Step [1231/2541], D Loss: 0.6504, G Loss: 2.0512\n",
      "Epoch [6/20], Step [1232/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [6/20], Step [1233/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [6/20], Step [1234/2541], D Loss: 0.6502, G Loss: 2.0679\n",
      "Epoch [6/20], Step [1235/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [6/20], Step [1236/2541], D Loss: 0.6502, G Loss: 2.0922\n",
      "Epoch [6/20], Step [1237/2541], D Loss: 0.6502, G Loss: 2.0637\n",
      "Epoch [6/20], Step [1238/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [6/20], Step [1239/2541], D Loss: 0.6502, G Loss: 2.0583\n",
      "Epoch [6/20], Step [1240/2541], D Loss: 0.6503, G Loss: 2.0800\n",
      "Epoch [6/20], Step [1241/2541], D Loss: 0.6506, G Loss: 2.0884\n",
      "Epoch [6/20], Step [1242/2541], D Loss: 0.6505, G Loss: 2.0669\n",
      "Epoch [6/20], Step [1243/2541], D Loss: 0.6503, G Loss: 2.0893\n",
      "Epoch [6/20], Step [1244/2541], D Loss: 0.6503, G Loss: 2.0703\n",
      "Epoch [6/20], Step [1245/2541], D Loss: 0.6503, G Loss: 2.1070\n",
      "Epoch [6/20], Step [1246/2541], D Loss: 0.6503, G Loss: 2.0890\n",
      "Epoch [6/20], Step [1247/2541], D Loss: 0.6502, G Loss: 2.0686\n",
      "Epoch [6/20], Step [1248/2541], D Loss: 0.6503, G Loss: 2.0967\n",
      "Epoch [6/20], Step [1249/2541], D Loss: 0.6502, G Loss: 2.0938\n",
      "Epoch [6/20], Step [1250/2541], D Loss: 0.6503, G Loss: 2.0883\n",
      "Epoch [6/20], Step [1251/2541], D Loss: 0.6503, G Loss: 2.0728\n",
      "Epoch [6/20], Step [1252/2541], D Loss: 0.6503, G Loss: 2.1008\n",
      "Epoch [6/20], Step [1253/2541], D Loss: 0.6502, G Loss: 2.0959\n",
      "Epoch [6/20], Step [1254/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [6/20], Step [1255/2541], D Loss: 0.6503, G Loss: 2.0777\n",
      "Epoch [6/20], Step [1256/2541], D Loss: 0.6504, G Loss: 2.0638\n",
      "Epoch [6/20], Step [1257/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [6/20], Step [1258/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [6/20], Step [1259/2541], D Loss: 0.6502, G Loss: 2.0904\n",
      "Epoch [6/20], Step [1260/2541], D Loss: 0.6503, G Loss: 2.1112\n",
      "Epoch [6/20], Step [1261/2541], D Loss: 0.6503, G Loss: 2.0744\n",
      "Epoch [6/20], Step [1262/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [6/20], Step [1263/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [6/20], Step [1264/2541], D Loss: 0.6502, G Loss: 2.0981\n",
      "Epoch [6/20], Step [1265/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [6/20], Step [1266/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [6/20], Step [1267/2541], D Loss: 0.6503, G Loss: 2.0878\n",
      "Epoch [6/20], Step [1268/2541], D Loss: 0.6503, G Loss: 2.0961\n",
      "Epoch [6/20], Step [1269/2541], D Loss: 0.6503, G Loss: 2.0710\n",
      "Epoch [6/20], Step [1270/2541], D Loss: 0.6503, G Loss: 2.0951\n",
      "Epoch [6/20], Step [1271/2541], D Loss: 0.6504, G Loss: 2.0711\n",
      "Epoch [6/20], Step [1272/2541], D Loss: 0.6503, G Loss: 2.0938\n",
      "Epoch [6/20], Step [1273/2541], D Loss: 0.6505, G Loss: 2.0956\n",
      "Epoch [6/20], Step [1274/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [6/20], Step [1275/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [6/20], Step [1276/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [1277/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [6/20], Step [1278/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [6/20], Step [1279/2541], D Loss: 0.6502, G Loss: 2.0663\n",
      "Epoch [6/20], Step [1280/2541], D Loss: 0.6504, G Loss: 2.0887\n",
      "Epoch [6/20], Step [1281/2541], D Loss: 0.6509, G Loss: 2.0636\n",
      "Epoch [6/20], Step [1282/2541], D Loss: 0.6510, G Loss: 2.0966\n",
      "Epoch [6/20], Step [1283/2541], D Loss: 0.6503, G Loss: 2.0724\n",
      "Epoch [6/20], Step [1284/2541], D Loss: 0.6502, G Loss: 2.0628\n",
      "Epoch [6/20], Step [1285/2541], D Loss: 0.6507, G Loss: 2.0803\n",
      "Epoch [6/20], Step [1286/2541], D Loss: 0.6511, G Loss: 2.0897\n",
      "Epoch [6/20], Step [1287/2541], D Loss: 0.6504, G Loss: 2.0865\n",
      "Epoch [6/20], Step [1288/2541], D Loss: 0.6503, G Loss: 2.0856\n",
      "Epoch [6/20], Step [1289/2541], D Loss: 0.6504, G Loss: 2.0836\n",
      "Epoch [6/20], Step [1290/2541], D Loss: 0.6502, G Loss: 2.1018\n",
      "Epoch [6/20], Step [1291/2541], D Loss: 0.6503, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1292/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [6/20], Step [1293/2541], D Loss: 0.6503, G Loss: 2.0826\n",
      "Epoch [6/20], Step [1294/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [6/20], Step [1295/2541], D Loss: 0.6503, G Loss: 2.0723\n",
      "Epoch [6/20], Step [1296/2541], D Loss: 0.6504, G Loss: 2.0955\n",
      "Epoch [6/20], Step [1297/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [1298/2541], D Loss: 0.6502, G Loss: 2.0668\n",
      "Epoch [6/20], Step [1299/2541], D Loss: 0.6502, G Loss: 2.0988\n",
      "Epoch [6/20], Step [1300/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [6/20], Step [1301/2541], D Loss: 0.6502, G Loss: 2.0720\n",
      "Epoch [6/20], Step [1302/2541], D Loss: 0.6503, G Loss: 2.0745\n",
      "Epoch [6/20], Step [1303/2541], D Loss: 0.6502, G Loss: 2.0954\n",
      "Epoch [6/20], Step [1304/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [6/20], Step [1305/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [1306/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [6/20], Step [1307/2541], D Loss: 0.6503, G Loss: 2.0822\n",
      "Epoch [6/20], Step [1308/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [6/20], Step [1309/2541], D Loss: 0.6502, G Loss: 2.0712\n",
      "Epoch [6/20], Step [1310/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [6/20], Step [1311/2541], D Loss: 0.6502, G Loss: 2.0942\n",
      "Epoch [6/20], Step [1312/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [1313/2541], D Loss: 0.6502, G Loss: 2.0677\n",
      "Epoch [6/20], Step [1314/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [6/20], Step [1315/2541], D Loss: 0.6503, G Loss: 2.0988\n",
      "Epoch [6/20], Step [1316/2541], D Loss: 0.6505, G Loss: 2.0744\n",
      "Epoch [6/20], Step [1317/2541], D Loss: 0.6503, G Loss: 2.0806\n",
      "Epoch [6/20], Step [1318/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [6/20], Step [1319/2541], D Loss: 0.6503, G Loss: 2.0673\n",
      "Epoch [6/20], Step [1320/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [6/20], Step [1321/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [6/20], Step [1322/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [6/20], Step [1323/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [6/20], Step [1324/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [1325/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [6/20], Step [1326/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [6/20], Step [1327/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [6/20], Step [1328/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [6/20], Step [1329/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [6/20], Step [1330/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [6/20], Step [1331/2541], D Loss: 0.6506, G Loss: 2.0855\n",
      "Epoch [6/20], Step [1332/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [6/20], Step [1333/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [6/20], Step [1334/2541], D Loss: 0.6502, G Loss: 2.0958\n",
      "Epoch [6/20], Step [1335/2541], D Loss: 0.6502, G Loss: 2.0647\n",
      "Epoch [6/20], Step [1336/2541], D Loss: 0.6502, G Loss: 2.0976\n",
      "Epoch [6/20], Step [1337/2541], D Loss: 0.6502, G Loss: 2.1205\n",
      "Epoch [6/20], Step [1338/2541], D Loss: 0.6503, G Loss: 2.0687\n",
      "Epoch [6/20], Step [1339/2541], D Loss: 0.6502, G Loss: 2.0724\n",
      "Epoch [6/20], Step [1340/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [1341/2541], D Loss: 0.6502, G Loss: 2.0982\n",
      "Epoch [6/20], Step [1342/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [6/20], Step [1343/2541], D Loss: 0.6502, G Loss: 2.0701\n",
      "Epoch [6/20], Step [1344/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [6/20], Step [1345/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [1346/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [6/20], Step [1347/2541], D Loss: 0.6502, G Loss: 2.0910\n",
      "Epoch [6/20], Step [1348/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [6/20], Step [1349/2541], D Loss: 0.6502, G Loss: 2.0692\n",
      "Epoch [6/20], Step [1350/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [6/20], Step [1351/2541], D Loss: 0.6502, G Loss: 2.0942\n",
      "Epoch [6/20], Step [1352/2541], D Loss: 0.6502, G Loss: 2.0676\n",
      "Epoch [6/20], Step [1353/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [6/20], Step [1354/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [6/20], Step [1355/2541], D Loss: 0.6502, G Loss: 2.1103\n",
      "Epoch [6/20], Step [1356/2541], D Loss: 0.6503, G Loss: 2.0600\n",
      "Epoch [6/20], Step [1357/2541], D Loss: 0.6503, G Loss: 2.0864\n",
      "Epoch [6/20], Step [1358/2541], D Loss: 0.6502, G Loss: 2.0910\n",
      "Epoch [6/20], Step [1359/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1360/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [1361/2541], D Loss: 0.6502, G Loss: 2.0937\n",
      "Epoch [6/20], Step [1362/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [6/20], Step [1363/2541], D Loss: 0.6503, G Loss: 2.0836\n",
      "Epoch [6/20], Step [1364/2541], D Loss: 0.6503, G Loss: 2.0914\n",
      "Epoch [6/20], Step [1365/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1366/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [6/20], Step [1367/2541], D Loss: 0.6503, G Loss: 2.0716\n",
      "Epoch [6/20], Step [1368/2541], D Loss: 0.6502, G Loss: 2.0901\n",
      "Epoch [6/20], Step [1369/2541], D Loss: 0.6503, G Loss: 2.0879\n",
      "Epoch [6/20], Step [1370/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [6/20], Step [1371/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [6/20], Step [1372/2541], D Loss: 0.6511, G Loss: 2.0789\n",
      "Epoch [6/20], Step [1373/2541], D Loss: 0.6508, G Loss: 2.0753\n",
      "Epoch [6/20], Step [1374/2541], D Loss: 0.6506, G Loss: 2.0897\n",
      "Epoch [6/20], Step [1375/2541], D Loss: 0.6502, G Loss: 2.0993\n",
      "Epoch [6/20], Step [1376/2541], D Loss: 0.6503, G Loss: 2.0726\n",
      "Epoch [6/20], Step [1377/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [6/20], Step [1378/2541], D Loss: 0.6503, G Loss: 2.0727\n",
      "Epoch [6/20], Step [1379/2541], D Loss: 0.6505, G Loss: 2.1119\n",
      "Epoch [6/20], Step [1380/2541], D Loss: 0.6505, G Loss: 2.0860\n",
      "Epoch [6/20], Step [1381/2541], D Loss: 0.6504, G Loss: 2.0816\n",
      "Epoch [6/20], Step [1382/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [6/20], Step [1383/2541], D Loss: 0.6503, G Loss: 2.0829\n",
      "Epoch [6/20], Step [1384/2541], D Loss: 0.6502, G Loss: 2.1003\n",
      "Epoch [6/20], Step [1385/2541], D Loss: 0.6502, G Loss: 2.1088\n",
      "Epoch [6/20], Step [1386/2541], D Loss: 0.6503, G Loss: 2.0850\n",
      "Epoch [6/20], Step [1387/2541], D Loss: 0.6503, G Loss: 2.0828\n",
      "Epoch [6/20], Step [1388/2541], D Loss: 0.6503, G Loss: 2.0742\n",
      "Epoch [6/20], Step [1389/2541], D Loss: 0.6503, G Loss: 2.0956\n",
      "Epoch [6/20], Step [1390/2541], D Loss: 0.6502, G Loss: 2.0694\n",
      "Epoch [6/20], Step [1391/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [6/20], Step [1392/2541], D Loss: 0.6502, G Loss: 2.1083\n",
      "Epoch [6/20], Step [1393/2541], D Loss: 0.6502, G Loss: 2.0656\n",
      "Epoch [6/20], Step [1394/2541], D Loss: 0.6502, G Loss: 2.0936\n",
      "Epoch [6/20], Step [1395/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [6/20], Step [1396/2541], D Loss: 0.6502, G Loss: 2.0717\n",
      "Epoch [6/20], Step [1397/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [6/20], Step [1398/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [6/20], Step [1399/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [6/20], Step [1400/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [6/20], Step [1401/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [6/20], Step [1402/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [6/20], Step [1403/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [1404/2541], D Loss: 0.6502, G Loss: 2.0904\n",
      "Epoch [6/20], Step [1405/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [6/20], Step [1406/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [6/20], Step [1407/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [6/20], Step [1408/2541], D Loss: 0.6502, G Loss: 2.0922\n",
      "Epoch [6/20], Step [1409/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [6/20], Step [1410/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [1411/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [1412/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [6/20], Step [1413/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [6/20], Step [1414/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [6/20], Step [1415/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [6/20], Step [1416/2541], D Loss: 0.6502, G Loss: 2.0979\n",
      "Epoch [6/20], Step [1417/2541], D Loss: 0.6504, G Loss: 2.0813\n",
      "Epoch [6/20], Step [1418/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [6/20], Step [1419/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [6/20], Step [1420/2541], D Loss: 0.6502, G Loss: 2.0932\n",
      "Epoch [6/20], Step [1421/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [6/20], Step [1422/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [6/20], Step [1423/2541], D Loss: 0.6503, G Loss: 2.0819\n",
      "Epoch [6/20], Step [1424/2541], D Loss: 0.6503, G Loss: 2.0877\n",
      "Epoch [6/20], Step [1425/2541], D Loss: 0.6502, G Loss: 2.0636\n",
      "Epoch [6/20], Step [1426/2541], D Loss: 0.6502, G Loss: 2.0921\n",
      "Epoch [6/20], Step [1427/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [6/20], Step [1428/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [6/20], Step [1429/2541], D Loss: 0.6502, G Loss: 2.0927\n",
      "Epoch [6/20], Step [1430/2541], D Loss: 0.6502, G Loss: 2.0664\n",
      "Epoch [6/20], Step [1431/2541], D Loss: 0.6503, G Loss: 2.0750\n",
      "Epoch [6/20], Step [1432/2541], D Loss: 0.6502, G Loss: 2.1014\n",
      "Epoch [6/20], Step [1433/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [6/20], Step [1434/2541], D Loss: 0.6503, G Loss: 2.0684\n",
      "Epoch [6/20], Step [1435/2541], D Loss: 0.6503, G Loss: 2.0850\n",
      "Epoch [6/20], Step [1436/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1437/2541], D Loss: 0.6502, G Loss: 2.0755\n",
      "Epoch [6/20], Step [1438/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [6/20], Step [1439/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [6/20], Step [1440/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [6/20], Step [1441/2541], D Loss: 0.6503, G Loss: 2.0813\n",
      "Epoch [6/20], Step [1442/2541], D Loss: 0.6502, G Loss: 2.0743\n",
      "Epoch [6/20], Step [1443/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [6/20], Step [1444/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [6/20], Step [1445/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [1446/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [6/20], Step [1447/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [6/20], Step [1448/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [6/20], Step [1449/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [6/20], Step [1450/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [6/20], Step [1451/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [6/20], Step [1452/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [6/20], Step [1453/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [6/20], Step [1454/2541], D Loss: 0.6502, G Loss: 2.0750\n",
      "Epoch [6/20], Step [1455/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [6/20], Step [1456/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [6/20], Step [1457/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [6/20], Step [1458/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [1459/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [6/20], Step [1460/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [6/20], Step [1461/2541], D Loss: 0.6502, G Loss: 2.0961\n",
      "Epoch [6/20], Step [1462/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [6/20], Step [1463/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1464/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [6/20], Step [1465/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [6/20], Step [1466/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [6/20], Step [1467/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [6/20], Step [1468/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [6/20], Step [1469/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [1470/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [6/20], Step [1471/2541], D Loss: 0.6502, G Loss: 2.0731\n",
      "Epoch [6/20], Step [1472/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [6/20], Step [1473/2541], D Loss: 0.6502, G Loss: 2.1057\n",
      "Epoch [6/20], Step [1474/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [6/20], Step [1475/2541], D Loss: 0.6502, G Loss: 2.0631\n",
      "Epoch [6/20], Step [1476/2541], D Loss: 0.6502, G Loss: 2.0942\n",
      "Epoch [6/20], Step [1477/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [6/20], Step [1478/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [6/20], Step [1479/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [6/20], Step [1480/2541], D Loss: 0.6502, G Loss: 2.1030\n",
      "Epoch [6/20], Step [1481/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [6/20], Step [1482/2541], D Loss: 0.6502, G Loss: 2.0616\n",
      "Epoch [6/20], Step [1483/2541], D Loss: 0.6503, G Loss: 2.0944\n",
      "Epoch [6/20], Step [1484/2541], D Loss: 0.6503, G Loss: 2.0748\n",
      "Epoch [6/20], Step [1485/2541], D Loss: 0.6502, G Loss: 2.0961\n",
      "Epoch [6/20], Step [1486/2541], D Loss: 0.6502, G Loss: 2.0721\n",
      "Epoch [6/20], Step [1487/2541], D Loss: 0.6506, G Loss: 2.1272\n",
      "Epoch [6/20], Step [1488/2541], D Loss: 0.6510, G Loss: 2.0582\n",
      "Epoch [6/20], Step [1489/2541], D Loss: 0.6508, G Loss: 2.1119\n",
      "Epoch [6/20], Step [1490/2541], D Loss: 0.6506, G Loss: 2.0804\n",
      "Epoch [6/20], Step [1491/2541], D Loss: 0.6505, G Loss: 2.0933\n",
      "Epoch [6/20], Step [1492/2541], D Loss: 0.6503, G Loss: 2.0824\n",
      "Epoch [6/20], Step [1493/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [1494/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [6/20], Step [1495/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [6/20], Step [1496/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [6/20], Step [1497/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [6/20], Step [1498/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [1499/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [6/20], Step [1500/2541], D Loss: 0.6504, G Loss: 2.0715\n",
      "Epoch [6/20], Step [1501/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [6/20], Step [1502/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [6/20], Step [1503/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [6/20], Step [1504/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [6/20], Step [1505/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [6/20], Step [1506/2541], D Loss: 0.6503, G Loss: 2.0972\n",
      "Epoch [6/20], Step [1507/2541], D Loss: 0.6503, G Loss: 2.0969\n",
      "Epoch [6/20], Step [1508/2541], D Loss: 0.6503, G Loss: 2.0687\n",
      "Epoch [6/20], Step [1509/2541], D Loss: 0.6503, G Loss: 2.0954\n",
      "Epoch [6/20], Step [1510/2541], D Loss: 0.6503, G Loss: 2.0688\n",
      "Epoch [6/20], Step [1511/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [6/20], Step [1512/2541], D Loss: 0.6505, G Loss: 2.1771\n",
      "Epoch [6/20], Step [1513/2541], D Loss: 0.6520, G Loss: 2.0187\n",
      "Epoch [6/20], Step [1514/2541], D Loss: 0.6512, G Loss: 2.0871\n",
      "Epoch [6/20], Step [1515/2541], D Loss: 0.6504, G Loss: 2.1209\n",
      "Epoch [6/20], Step [1516/2541], D Loss: 0.6506, G Loss: 2.0903\n",
      "Epoch [6/20], Step [1517/2541], D Loss: 0.6503, G Loss: 2.0524\n",
      "Epoch [6/20], Step [1518/2541], D Loss: 0.6507, G Loss: 2.1062\n",
      "Epoch [6/20], Step [1519/2541], D Loss: 0.6504, G Loss: 2.0958\n",
      "Epoch [6/20], Step [1520/2541], D Loss: 0.6503, G Loss: 2.0568\n",
      "Epoch [6/20], Step [1521/2541], D Loss: 0.6506, G Loss: 2.1052\n",
      "Epoch [6/20], Step [1522/2541], D Loss: 0.6505, G Loss: 2.0571\n",
      "Epoch [6/20], Step [1523/2541], D Loss: 0.6507, G Loss: 2.0972\n",
      "Epoch [6/20], Step [1524/2541], D Loss: 0.6505, G Loss: 2.0945\n",
      "Epoch [6/20], Step [1525/2541], D Loss: 0.6504, G Loss: 2.0798\n",
      "Epoch [6/20], Step [1526/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [6/20], Step [1527/2541], D Loss: 0.6504, G Loss: 2.0721\n",
      "Epoch [6/20], Step [1528/2541], D Loss: 0.6503, G Loss: 2.0800\n",
      "Epoch [6/20], Step [1529/2541], D Loss: 0.6502, G Loss: 2.0989\n",
      "Epoch [6/20], Step [1530/2541], D Loss: 0.6502, G Loss: 2.0725\n",
      "Epoch [6/20], Step [1531/2541], D Loss: 0.6502, G Loss: 2.0686\n",
      "Epoch [6/20], Step [1532/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [6/20], Step [1533/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [6/20], Step [1534/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [6/20], Step [1535/2541], D Loss: 0.6503, G Loss: 2.0860\n",
      "Epoch [6/20], Step [1536/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1537/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [6/20], Step [1538/2541], D Loss: 0.6503, G Loss: 2.1019\n",
      "Epoch [6/20], Step [1539/2541], D Loss: 0.6503, G Loss: 2.0977\n",
      "Epoch [6/20], Step [1540/2541], D Loss: 0.6502, G Loss: 2.1060\n",
      "Epoch [6/20], Step [1541/2541], D Loss: 0.6503, G Loss: 2.1022\n",
      "Epoch [6/20], Step [1542/2541], D Loss: 0.6503, G Loss: 2.0605\n",
      "Epoch [6/20], Step [1543/2541], D Loss: 0.6503, G Loss: 2.0869\n",
      "Epoch [6/20], Step [1544/2541], D Loss: 0.6503, G Loss: 2.1008\n",
      "Epoch [6/20], Step [1545/2541], D Loss: 0.6504, G Loss: 2.0655\n",
      "Epoch [6/20], Step [1546/2541], D Loss: 0.6504, G Loss: 2.0919\n",
      "Epoch [6/20], Step [1547/2541], D Loss: 0.6503, G Loss: 2.0814\n",
      "Epoch [6/20], Step [1548/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [6/20], Step [1549/2541], D Loss: 0.6502, G Loss: 2.0690\n",
      "Epoch [6/20], Step [1550/2541], D Loss: 0.6503, G Loss: 2.0821\n",
      "Epoch [6/20], Step [1551/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [1552/2541], D Loss: 0.6502, G Loss: 2.1100\n",
      "Epoch [6/20], Step [1553/2541], D Loss: 0.6504, G Loss: 2.0597\n",
      "Epoch [6/20], Step [1554/2541], D Loss: 0.6503, G Loss: 2.0862\n",
      "Epoch [6/20], Step [1555/2541], D Loss: 0.6502, G Loss: 2.1143\n",
      "Epoch [6/20], Step [1556/2541], D Loss: 0.6502, G Loss: 2.0970\n",
      "Epoch [6/20], Step [1557/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [6/20], Step [1558/2541], D Loss: 0.6502, G Loss: 2.0719\n",
      "Epoch [6/20], Step [1559/2541], D Loss: 0.6502, G Loss: 2.0991\n",
      "Epoch [6/20], Step [1560/2541], D Loss: 0.6502, G Loss: 2.1063\n",
      "Epoch [6/20], Step [1561/2541], D Loss: 0.6502, G Loss: 2.0554\n",
      "Epoch [6/20], Step [1562/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [1563/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [6/20], Step [1564/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [6/20], Step [1565/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [6/20], Step [1566/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [1567/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [6/20], Step [1568/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [1569/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [6/20], Step [1570/2541], D Loss: 0.6502, G Loss: 2.1059\n",
      "Epoch [6/20], Step [1571/2541], D Loss: 0.6502, G Loss: 2.0734\n",
      "Epoch [6/20], Step [1572/2541], D Loss: 0.6502, G Loss: 2.0926\n",
      "Epoch [6/20], Step [1573/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [6/20], Step [1574/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [6/20], Step [1575/2541], D Loss: 0.6503, G Loss: 2.0980\n",
      "Epoch [6/20], Step [1576/2541], D Loss: 0.6504, G Loss: 2.0652\n",
      "Epoch [6/20], Step [1577/2541], D Loss: 0.6504, G Loss: 2.0764\n",
      "Epoch [6/20], Step [1578/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [6/20], Step [1579/2541], D Loss: 0.6503, G Loss: 2.0703\n",
      "Epoch [6/20], Step [1580/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [6/20], Step [1581/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [6/20], Step [1582/2541], D Loss: 0.6503, G Loss: 2.0860\n",
      "Epoch [6/20], Step [1583/2541], D Loss: 0.6503, G Loss: 2.0692\n",
      "Epoch [6/20], Step [1584/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [6/20], Step [1585/2541], D Loss: 0.6502, G Loss: 2.1011\n",
      "Epoch [6/20], Step [1586/2541], D Loss: 0.6502, G Loss: 2.0704\n",
      "Epoch [6/20], Step [1587/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [6/20], Step [1588/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [6/20], Step [1589/2541], D Loss: 0.6502, G Loss: 2.0942\n",
      "Epoch [6/20], Step [1590/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [6/20], Step [1591/2541], D Loss: 0.6502, G Loss: 2.1111\n",
      "Epoch [6/20], Step [1592/2541], D Loss: 0.6503, G Loss: 2.0655\n",
      "Epoch [6/20], Step [1593/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [1594/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [6/20], Step [1595/2541], D Loss: 0.6502, G Loss: 2.0949\n",
      "Epoch [6/20], Step [1596/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [6/20], Step [1597/2541], D Loss: 0.6502, G Loss: 2.0665\n",
      "Epoch [6/20], Step [1598/2541], D Loss: 0.6502, G Loss: 2.1058\n",
      "Epoch [6/20], Step [1599/2541], D Loss: 0.6502, G Loss: 2.0907\n",
      "Epoch [6/20], Step [1600/2541], D Loss: 0.6502, G Loss: 2.0697\n",
      "Epoch [6/20], Step [1601/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1602/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [6/20], Step [1603/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [6/20], Step [1604/2541], D Loss: 0.6502, G Loss: 2.0689\n",
      "Epoch [6/20], Step [1605/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [6/20], Step [1606/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [1607/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [6/20], Step [1608/2541], D Loss: 0.6502, G Loss: 2.0981\n",
      "Epoch [6/20], Step [1609/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [6/20], Step [1610/2541], D Loss: 0.6502, G Loss: 2.0603\n",
      "Epoch [6/20], Step [1611/2541], D Loss: 0.6502, G Loss: 2.0988\n",
      "Epoch [6/20], Step [1612/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [6/20], Step [1613/2541], D Loss: 0.6502, G Loss: 2.0609\n",
      "Epoch [6/20], Step [1614/2541], D Loss: 0.6502, G Loss: 2.0989\n",
      "Epoch [6/20], Step [1615/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [6/20], Step [1616/2541], D Loss: 0.6502, G Loss: 2.0657\n",
      "Epoch [6/20], Step [1617/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [6/20], Step [1618/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [6/20], Step [1619/2541], D Loss: 0.6502, G Loss: 2.0974\n",
      "Epoch [6/20], Step [1620/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [6/20], Step [1621/2541], D Loss: 0.6507, G Loss: 2.0781\n",
      "Epoch [6/20], Step [1622/2541], D Loss: 0.6509, G Loss: 2.0846\n",
      "Epoch [6/20], Step [1623/2541], D Loss: 0.6504, G Loss: 2.1018\n",
      "Epoch [6/20], Step [1624/2541], D Loss: 0.6503, G Loss: 2.0818\n",
      "Epoch [6/20], Step [1625/2541], D Loss: 0.6505, G Loss: 2.0843\n",
      "Epoch [6/20], Step [1626/2541], D Loss: 0.6502, G Loss: 2.0755\n",
      "Epoch [6/20], Step [1627/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [6/20], Step [1628/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [6/20], Step [1629/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [6/20], Step [1630/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [1631/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [6/20], Step [1632/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [6/20], Step [1633/2541], D Loss: 0.6502, G Loss: 2.1065\n",
      "Epoch [6/20], Step [1634/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [6/20], Step [1635/2541], D Loss: 0.6502, G Loss: 2.0730\n",
      "Epoch [6/20], Step [1636/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [6/20], Step [1637/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [6/20], Step [1638/2541], D Loss: 0.6502, G Loss: 2.0929\n",
      "Epoch [6/20], Step [1639/2541], D Loss: 0.6502, G Loss: 2.1056\n",
      "Epoch [6/20], Step [1640/2541], D Loss: 0.6503, G Loss: 2.0908\n",
      "Epoch [6/20], Step [1641/2541], D Loss: 0.6502, G Loss: 2.0493\n",
      "Epoch [6/20], Step [1642/2541], D Loss: 0.6511, G Loss: 2.1935\n",
      "Epoch [6/20], Step [1643/2541], D Loss: 0.6541, G Loss: 2.0166\n",
      "Epoch [6/20], Step [1644/2541], D Loss: 0.6514, G Loss: 2.1037\n",
      "Epoch [6/20], Step [1645/2541], D Loss: 0.6504, G Loss: 2.1057\n",
      "Epoch [6/20], Step [1646/2541], D Loss: 0.6505, G Loss: 2.0881\n",
      "Epoch [6/20], Step [1647/2541], D Loss: 0.6503, G Loss: 2.0709\n",
      "Epoch [6/20], Step [1648/2541], D Loss: 0.6502, G Loss: 2.1385\n",
      "Epoch [6/20], Step [1649/2541], D Loss: 0.6506, G Loss: 2.0189\n",
      "Epoch [6/20], Step [1650/2541], D Loss: 0.6509, G Loss: 2.0927\n",
      "Epoch [6/20], Step [1651/2541], D Loss: 0.6504, G Loss: 2.0776\n",
      "Epoch [6/20], Step [1652/2541], D Loss: 0.6503, G Loss: 2.0815\n",
      "Epoch [6/20], Step [1653/2541], D Loss: 0.6505, G Loss: 2.1096\n",
      "Epoch [6/20], Step [1654/2541], D Loss: 0.6507, G Loss: 2.0580\n",
      "Epoch [6/20], Step [1655/2541], D Loss: 0.6504, G Loss: 2.0737\n",
      "Epoch [6/20], Step [1656/2541], D Loss: 0.6503, G Loss: 2.0944\n",
      "Epoch [6/20], Step [1657/2541], D Loss: 0.6504, G Loss: 2.0562\n",
      "Epoch [6/20], Step [1658/2541], D Loss: 0.6503, G Loss: 2.0998\n",
      "Epoch [6/20], Step [1659/2541], D Loss: 0.6503, G Loss: 2.0920\n",
      "Epoch [6/20], Step [1660/2541], D Loss: 0.6503, G Loss: 2.0738\n",
      "Epoch [6/20], Step [1661/2541], D Loss: 0.6502, G Loss: 2.0887\n",
      "Epoch [6/20], Step [1662/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [6/20], Step [1663/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [6/20], Step [1664/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [1665/2541], D Loss: 0.6502, G Loss: 2.0693\n",
      "Epoch [6/20], Step [1666/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [6/20], Step [1667/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [6/20], Step [1668/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [6/20], Step [1669/2541], D Loss: 0.6502, G Loss: 2.0898\n",
      "Epoch [6/20], Step [1670/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [6/20], Step [1671/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [6/20], Step [1672/2541], D Loss: 0.6503, G Loss: 2.0757\n",
      "Epoch [6/20], Step [1673/2541], D Loss: 0.6503, G Loss: 2.0964\n",
      "Epoch [6/20], Step [1674/2541], D Loss: 0.6502, G Loss: 2.0811\n",
      "Epoch [6/20], Step [1675/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [6/20], Step [1676/2541], D Loss: 0.6503, G Loss: 2.0817\n",
      "Epoch [6/20], Step [1677/2541], D Loss: 0.6504, G Loss: 2.0733\n",
      "Epoch [6/20], Step [1678/2541], D Loss: 0.6503, G Loss: 2.0736\n",
      "Epoch [6/20], Step [1679/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [6/20], Step [1680/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [1681/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [6/20], Step [1682/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [1683/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [1684/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [6/20], Step [1685/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [6/20], Step [1686/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [1687/2541], D Loss: 0.6502, G Loss: 2.0683\n",
      "Epoch [6/20], Step [1688/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [1689/2541], D Loss: 0.6503, G Loss: 2.0754\n",
      "Epoch [6/20], Step [1690/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [6/20], Step [1691/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [6/20], Step [1692/2541], D Loss: 0.6502, G Loss: 2.0642\n",
      "Epoch [6/20], Step [1693/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [1694/2541], D Loss: 0.6502, G Loss: 2.0924\n",
      "Epoch [6/20], Step [1695/2541], D Loss: 0.6502, G Loss: 2.0650\n",
      "Epoch [6/20], Step [1696/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [6/20], Step [1697/2541], D Loss: 0.6502, G Loss: 2.0990\n",
      "Epoch [6/20], Step [1698/2541], D Loss: 0.6502, G Loss: 2.0646\n",
      "Epoch [6/20], Step [1699/2541], D Loss: 0.6503, G Loss: 2.0961\n",
      "Epoch [6/20], Step [1700/2541], D Loss: 0.6503, G Loss: 2.1107\n",
      "Epoch [6/20], Step [1701/2541], D Loss: 0.6503, G Loss: 2.0639\n",
      "Epoch [6/20], Step [1702/2541], D Loss: 0.6503, G Loss: 2.0810\n",
      "Epoch [6/20], Step [1703/2541], D Loss: 0.6503, G Loss: 2.0922\n",
      "Epoch [6/20], Step [1704/2541], D Loss: 0.6503, G Loss: 2.0941\n",
      "Epoch [6/20], Step [1705/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [6/20], Step [1706/2541], D Loss: 0.6502, G Loss: 2.0948\n",
      "Epoch [6/20], Step [1707/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [6/20], Step [1708/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [6/20], Step [1709/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [6/20], Step [1710/2541], D Loss: 0.6502, G Loss: 2.1059\n",
      "Epoch [6/20], Step [1711/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [6/20], Step [1712/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [6/20], Step [1713/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [6/20], Step [1714/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [1715/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [1716/2541], D Loss: 0.6503, G Loss: 2.0913\n",
      "Epoch [6/20], Step [1717/2541], D Loss: 0.6503, G Loss: 2.0829\n",
      "Epoch [6/20], Step [1718/2541], D Loss: 0.6503, G Loss: 2.0806\n",
      "Epoch [6/20], Step [1719/2541], D Loss: 0.6503, G Loss: 2.0995\n",
      "Epoch [6/20], Step [1720/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [6/20], Step [1721/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [6/20], Step [1722/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1723/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [6/20], Step [1724/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [6/20], Step [1725/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [6/20], Step [1726/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [6/20], Step [1727/2541], D Loss: 0.6502, G Loss: 2.0939\n",
      "Epoch [6/20], Step [1728/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [6/20], Step [1729/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [6/20], Step [1730/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [1731/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1732/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [6/20], Step [1733/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [6/20], Step [1734/2541], D Loss: 0.6506, G Loss: 2.0697\n",
      "Epoch [6/20], Step [1735/2541], D Loss: 0.6503, G Loss: 2.0882\n",
      "Epoch [6/20], Step [1736/2541], D Loss: 0.6503, G Loss: 2.0755\n",
      "Epoch [6/20], Step [1737/2541], D Loss: 0.6502, G Loss: 2.0876\n",
      "Epoch [6/20], Step [1738/2541], D Loss: 0.6518, G Loss: 2.0881\n",
      "Epoch [6/20], Step [1739/2541], D Loss: 0.6521, G Loss: 2.0627\n",
      "Epoch [6/20], Step [1740/2541], D Loss: 0.6513, G Loss: 2.1013\n",
      "Epoch [6/20], Step [1741/2541], D Loss: 0.6504, G Loss: 2.0789\n",
      "Epoch [6/20], Step [1742/2541], D Loss: 0.6513, G Loss: 2.0620\n",
      "Epoch [6/20], Step [1743/2541], D Loss: 0.6527, G Loss: 2.1248\n",
      "Epoch [6/20], Step [1744/2541], D Loss: 0.6510, G Loss: 2.0758\n",
      "Epoch [6/20], Step [1745/2541], D Loss: 0.6505, G Loss: 2.0966\n",
      "Epoch [6/20], Step [1746/2541], D Loss: 0.6503, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1747/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [6/20], Step [1748/2541], D Loss: 0.6502, G Loss: 2.0938\n",
      "Epoch [6/20], Step [1749/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [6/20], Step [1750/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [1751/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [1752/2541], D Loss: 0.6503, G Loss: 2.0884\n",
      "Epoch [6/20], Step [1753/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [6/20], Step [1754/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [6/20], Step [1755/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [6/20], Step [1756/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [6/20], Step [1757/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [6/20], Step [1758/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [1759/2541], D Loss: 0.6503, G Loss: 2.0790\n",
      "Epoch [6/20], Step [1760/2541], D Loss: 0.6506, G Loss: 2.0924\n",
      "Epoch [6/20], Step [1761/2541], D Loss: 0.6505, G Loss: 2.0640\n",
      "Epoch [6/20], Step [1762/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [6/20], Step [1763/2541], D Loss: 0.6503, G Loss: 2.0912\n",
      "Epoch [6/20], Step [1764/2541], D Loss: 0.6503, G Loss: 2.0750\n",
      "Epoch [6/20], Step [1765/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [1766/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [6/20], Step [1767/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [6/20], Step [1768/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [6/20], Step [1769/2541], D Loss: 0.6503, G Loss: 2.0819\n",
      "Epoch [6/20], Step [1770/2541], D Loss: 0.6503, G Loss: 2.1072\n",
      "Epoch [6/20], Step [1771/2541], D Loss: 0.6503, G Loss: 2.0992\n",
      "Epoch [6/20], Step [1772/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [6/20], Step [1773/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [6/20], Step [1774/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [1775/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [6/20], Step [1776/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [6/20], Step [1777/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [1778/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1779/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [6/20], Step [1780/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [6/20], Step [1781/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [1782/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [6/20], Step [1783/2541], D Loss: 0.6502, G Loss: 2.0944\n",
      "Epoch [6/20], Step [1784/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [6/20], Step [1785/2541], D Loss: 0.6502, G Loss: 2.0929\n",
      "Epoch [6/20], Step [1786/2541], D Loss: 0.6502, G Loss: 2.0922\n",
      "Epoch [6/20], Step [1787/2541], D Loss: 0.6503, G Loss: 2.0746\n",
      "Epoch [6/20], Step [1788/2541], D Loss: 0.6503, G Loss: 2.0940\n",
      "Epoch [6/20], Step [1789/2541], D Loss: 0.6503, G Loss: 2.0718\n",
      "Epoch [6/20], Step [1790/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [6/20], Step [1791/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [6/20], Step [1792/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [1793/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [1794/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [6/20], Step [1795/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [6/20], Step [1796/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [6/20], Step [1797/2541], D Loss: 0.6502, G Loss: 2.0711\n",
      "Epoch [6/20], Step [1798/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [6/20], Step [1799/2541], D Loss: 0.6502, G Loss: 2.0941\n",
      "Epoch [6/20], Step [1800/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [6/20], Step [1801/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [6/20], Step [1802/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [6/20], Step [1803/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [6/20], Step [1804/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [1805/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [6/20], Step [1806/2541], D Loss: 0.6502, G Loss: 2.1092\n",
      "Epoch [6/20], Step [1807/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [6/20], Step [1808/2541], D Loss: 0.6502, G Loss: 2.0590\n",
      "Epoch [6/20], Step [1809/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [6/20], Step [1810/2541], D Loss: 0.6502, G Loss: 2.0964\n",
      "Epoch [6/20], Step [1811/2541], D Loss: 0.6502, G Loss: 2.1140\n",
      "Epoch [6/20], Step [1812/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [6/20], Step [1813/2541], D Loss: 0.6502, G Loss: 2.0611\n",
      "Epoch [6/20], Step [1814/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [6/20], Step [1815/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [6/20], Step [1816/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [6/20], Step [1817/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [6/20], Step [1818/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [6/20], Step [1819/2541], D Loss: 0.6502, G Loss: 2.1073\n",
      "Epoch [6/20], Step [1820/2541], D Loss: 0.6503, G Loss: 2.0692\n",
      "Epoch [6/20], Step [1821/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [1822/2541], D Loss: 0.6502, G Loss: 2.0960\n",
      "Epoch [6/20], Step [1823/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [6/20], Step [1824/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [6/20], Step [1825/2541], D Loss: 0.6502, G Loss: 2.0887\n",
      "Epoch [6/20], Step [1826/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [6/20], Step [1827/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [6/20], Step [1828/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [6/20], Step [1829/2541], D Loss: 0.6502, G Loss: 2.0532\n",
      "Epoch [6/20], Step [1830/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [6/20], Step [1831/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [6/20], Step [1832/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [6/20], Step [1833/2541], D Loss: 0.6502, G Loss: 2.0763\n",
      "Epoch [6/20], Step [1834/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [6/20], Step [1835/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [6/20], Step [1836/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [6/20], Step [1837/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [6/20], Step [1838/2541], D Loss: 0.6503, G Loss: 2.0621\n",
      "Epoch [6/20], Step [1839/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [6/20], Step [1840/2541], D Loss: 0.6502, G Loss: 2.0792\n",
      "Epoch [6/20], Step [1841/2541], D Loss: 0.6503, G Loss: 2.0990\n",
      "Epoch [6/20], Step [1842/2541], D Loss: 0.6503, G Loss: 2.0717\n",
      "Epoch [6/20], Step [1843/2541], D Loss: 0.6502, G Loss: 2.0650\n",
      "Epoch [6/20], Step [1844/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [6/20], Step [1845/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [6/20], Step [1846/2541], D Loss: 0.6502, G Loss: 2.0717\n",
      "Epoch [6/20], Step [1847/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [6/20], Step [1848/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [1849/2541], D Loss: 0.6502, G Loss: 2.0986\n",
      "Epoch [6/20], Step [1850/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [6/20], Step [1851/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [6/20], Step [1852/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [6/20], Step [1853/2541], D Loss: 0.6502, G Loss: 2.0984\n",
      "Epoch [6/20], Step [1854/2541], D Loss: 0.6502, G Loss: 2.0909\n",
      "Epoch [6/20], Step [1855/2541], D Loss: 0.6502, G Loss: 2.0792\n",
      "Epoch [6/20], Step [1856/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [6/20], Step [1857/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [1858/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [6/20], Step [1859/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [6/20], Step [1860/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [6/20], Step [1861/2541], D Loss: 0.6502, G Loss: 2.0929\n",
      "Epoch [6/20], Step [1862/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [1863/2541], D Loss: 0.6502, G Loss: 2.0684\n",
      "Epoch [6/20], Step [1864/2541], D Loss: 0.6503, G Loss: 2.0898\n",
      "Epoch [6/20], Step [1865/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [6/20], Step [1866/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [6/20], Step [1867/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [6/20], Step [1868/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [6/20], Step [1869/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [6/20], Step [1870/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [6/20], Step [1871/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [6/20], Step [1872/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [6/20], Step [1873/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [6/20], Step [1874/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [6/20], Step [1875/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [6/20], Step [1876/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [6/20], Step [1877/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [6/20], Step [1878/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [1879/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [6/20], Step [1880/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [6/20], Step [1881/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [6/20], Step [1882/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [6/20], Step [1883/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [6/20], Step [1884/2541], D Loss: 0.6502, G Loss: 2.0938\n",
      "Epoch [6/20], Step [1885/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [6/20], Step [1886/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [6/20], Step [1887/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [6/20], Step [1888/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [6/20], Step [1889/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [1890/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [6/20], Step [1891/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [1892/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [1893/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [1894/2541], D Loss: 0.6503, G Loss: 2.1095\n",
      "Epoch [6/20], Step [1895/2541], D Loss: 0.6504, G Loss: 2.0729\n",
      "Epoch [6/20], Step [1896/2541], D Loss: 0.6504, G Loss: 2.0740\n",
      "Epoch [6/20], Step [1897/2541], D Loss: 0.6503, G Loss: 2.0936\n",
      "Epoch [6/20], Step [1898/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [1899/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [6/20], Step [1900/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [6/20], Step [1901/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [6/20], Step [1902/2541], D Loss: 0.6503, G Loss: 2.0818\n",
      "Epoch [6/20], Step [1903/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [6/20], Step [1904/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [6/20], Step [1905/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [1906/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [6/20], Step [1907/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [1908/2541], D Loss: 0.6502, G Loss: 2.0904\n",
      "Epoch [6/20], Step [1909/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [6/20], Step [1910/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [6/20], Step [1911/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [1912/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [6/20], Step [1913/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [6/20], Step [1914/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [6/20], Step [1915/2541], D Loss: 0.6503, G Loss: 2.0807\n",
      "Epoch [6/20], Step [1916/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [6/20], Step [1917/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [1918/2541], D Loss: 0.6502, G Loss: 2.0909\n",
      "Epoch [6/20], Step [1919/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [6/20], Step [1920/2541], D Loss: 0.6502, G Loss: 2.0590\n",
      "Epoch [6/20], Step [1921/2541], D Loss: 0.6502, G Loss: 2.0981\n",
      "Epoch [6/20], Step [1922/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [6/20], Step [1923/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [6/20], Step [1924/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [6/20], Step [1925/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [6/20], Step [1926/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [6/20], Step [1927/2541], D Loss: 0.6502, G Loss: 2.0924\n",
      "Epoch [6/20], Step [1928/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [1929/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [6/20], Step [1930/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [6/20], Step [1931/2541], D Loss: 0.6503, G Loss: 2.0926\n",
      "Epoch [6/20], Step [1932/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [1933/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [6/20], Step [1934/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [6/20], Step [1935/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [1936/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [6/20], Step [1937/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [6/20], Step [1938/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [6/20], Step [1939/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [6/20], Step [1940/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [6/20], Step [1941/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [1942/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [6/20], Step [1943/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [6/20], Step [1944/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1945/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [6/20], Step [1946/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [6/20], Step [1947/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [6/20], Step [1948/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [1949/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [6/20], Step [1950/2541], D Loss: 0.6503, G Loss: 2.0832\n",
      "Epoch [6/20], Step [1951/2541], D Loss: 0.6504, G Loss: 2.0764\n",
      "Epoch [6/20], Step [1952/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [6/20], Step [1953/2541], D Loss: 0.6502, G Loss: 2.0581\n",
      "Epoch [6/20], Step [1954/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [6/20], Step [1955/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [6/20], Step [1956/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [6/20], Step [1957/2541], D Loss: 0.6502, G Loss: 2.0802\n",
      "Epoch [6/20], Step [1958/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [6/20], Step [1959/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [6/20], Step [1960/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [6/20], Step [1961/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [6/20], Step [1962/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [1963/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [1964/2541], D Loss: 0.6502, G Loss: 2.0963\n",
      "Epoch [6/20], Step [1965/2541], D Loss: 0.6502, G Loss: 2.0802\n",
      "Epoch [6/20], Step [1966/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [6/20], Step [1967/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [6/20], Step [1968/2541], D Loss: 0.6502, G Loss: 2.0714\n",
      "Epoch [6/20], Step [1969/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [6/20], Step [1970/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [6/20], Step [1971/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [1972/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [6/20], Step [1973/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [6/20], Step [1974/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [6/20], Step [1975/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [6/20], Step [1976/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [6/20], Step [1977/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [6/20], Step [1978/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [6/20], Step [1979/2541], D Loss: 0.6502, G Loss: 2.0945\n",
      "Epoch [6/20], Step [1980/2541], D Loss: 0.6503, G Loss: 2.0651\n",
      "Epoch [6/20], Step [1981/2541], D Loss: 0.6503, G Loss: 2.0916\n",
      "Epoch [6/20], Step [1982/2541], D Loss: 0.6503, G Loss: 2.0940\n",
      "Epoch [6/20], Step [1983/2541], D Loss: 0.6503, G Loss: 2.0731\n",
      "Epoch [6/20], Step [1984/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [6/20], Step [1985/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [6/20], Step [1986/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [6/20], Step [1987/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [6/20], Step [1988/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [6/20], Step [1989/2541], D Loss: 0.6503, G Loss: 2.0777\n",
      "Epoch [6/20], Step [1990/2541], D Loss: 0.6508, G Loss: 2.0946\n",
      "Epoch [6/20], Step [1991/2541], D Loss: 0.6503, G Loss: 2.0695\n",
      "Epoch [6/20], Step [1992/2541], D Loss: 0.6503, G Loss: 2.1016\n",
      "Epoch [6/20], Step [1993/2541], D Loss: 0.6504, G Loss: 2.0700\n",
      "Epoch [6/20], Step [1994/2541], D Loss: 0.6504, G Loss: 2.0863\n",
      "Epoch [6/20], Step [1995/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [6/20], Step [1996/2541], D Loss: 0.6503, G Loss: 2.0894\n",
      "Epoch [6/20], Step [1997/2541], D Loss: 0.6503, G Loss: 2.0734\n",
      "Epoch [6/20], Step [1998/2541], D Loss: 0.6504, G Loss: 2.1161\n",
      "Epoch [6/20], Step [1999/2541], D Loss: 0.6509, G Loss: 2.0518\n",
      "Epoch [6/20], Step [2000/2541], D Loss: 0.6506, G Loss: 2.0903\n",
      "Epoch [6/20], Step [2001/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [6/20], Step [2002/2541], D Loss: 0.6503, G Loss: 2.0706\n",
      "Epoch [6/20], Step [2003/2541], D Loss: 0.6504, G Loss: 2.0848\n",
      "Epoch [6/20], Step [2004/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [2005/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [6/20], Step [2006/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [6/20], Step [2007/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [6/20], Step [2008/2541], D Loss: 0.6502, G Loss: 2.0929\n",
      "Epoch [6/20], Step [2009/2541], D Loss: 0.6503, G Loss: 2.0831\n",
      "Epoch [6/20], Step [2010/2541], D Loss: 0.6503, G Loss: 2.0824\n",
      "Epoch [6/20], Step [2011/2541], D Loss: 0.6503, G Loss: 2.0777\n",
      "Epoch [6/20], Step [2012/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [6/20], Step [2013/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [6/20], Step [2014/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [6/20], Step [2015/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [6/20], Step [2016/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [6/20], Step [2017/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [2018/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [6/20], Step [2019/2541], D Loss: 0.6502, G Loss: 2.0672\n",
      "Epoch [6/20], Step [2020/2541], D Loss: 0.6503, G Loss: 2.0878\n",
      "Epoch [6/20], Step [2021/2541], D Loss: 0.6503, G Loss: 2.0832\n",
      "Epoch [6/20], Step [2022/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [6/20], Step [2023/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [6/20], Step [2024/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [6/20], Step [2025/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [6/20], Step [2026/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [6/20], Step [2027/2541], D Loss: 0.6502, G Loss: 2.0925\n",
      "Epoch [6/20], Step [2028/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [6/20], Step [2029/2541], D Loss: 0.6507, G Loss: 2.1660\n",
      "Epoch [6/20], Step [2030/2541], D Loss: 0.6519, G Loss: 2.0262\n",
      "Epoch [6/20], Step [2031/2541], D Loss: 0.6511, G Loss: 2.0910\n",
      "Epoch [6/20], Step [2032/2541], D Loss: 0.6505, G Loss: 2.0946\n",
      "Epoch [6/20], Step [2033/2541], D Loss: 0.6506, G Loss: 2.0528\n",
      "Epoch [6/20], Step [2034/2541], D Loss: 0.6504, G Loss: 2.1042\n",
      "Epoch [6/20], Step [2035/2541], D Loss: 0.6503, G Loss: 2.0697\n",
      "Epoch [6/20], Step [2036/2541], D Loss: 0.6503, G Loss: 2.1025\n",
      "Epoch [6/20], Step [2037/2541], D Loss: 0.6505, G Loss: 2.0531\n",
      "Epoch [6/20], Step [2038/2541], D Loss: 0.6503, G Loss: 2.0677\n",
      "Epoch [6/20], Step [2039/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [6/20], Step [2040/2541], D Loss: 0.6502, G Loss: 2.0946\n",
      "Epoch [6/20], Step [2041/2541], D Loss: 0.6502, G Loss: 2.0623\n",
      "Epoch [6/20], Step [2042/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [6/20], Step [2043/2541], D Loss: 0.6505, G Loss: 2.0955\n",
      "Epoch [6/20], Step [2044/2541], D Loss: 0.6503, G Loss: 2.0746\n",
      "Epoch [6/20], Step [2045/2541], D Loss: 0.6502, G Loss: 2.0959\n",
      "Epoch [6/20], Step [2046/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [2047/2541], D Loss: 0.6530, G Loss: 2.0104\n",
      "Epoch [6/20], Step [2048/2541], D Loss: 0.6506, G Loss: 2.1121\n",
      "Epoch [6/20], Step [2049/2541], D Loss: 0.6504, G Loss: 2.0794\n",
      "Epoch [6/20], Step [2050/2541], D Loss: 0.6505, G Loss: 2.0514\n",
      "Epoch [6/20], Step [2051/2541], D Loss: 0.6554, G Loss: 2.2123\n",
      "Epoch [6/20], Step [2052/2541], D Loss: 0.6530, G Loss: 1.9741\n",
      "Epoch [6/20], Step [2053/2541], D Loss: 0.6548, G Loss: 2.1574\n",
      "Epoch [6/20], Step [2054/2541], D Loss: 0.6523, G Loss: 2.1278\n",
      "Epoch [6/20], Step [2055/2541], D Loss: 0.6528, G Loss: 1.9935\n",
      "Epoch [6/20], Step [2056/2541], D Loss: 0.6518, G Loss: 2.1442\n",
      "Epoch [6/20], Step [2057/2541], D Loss: 0.6557, G Loss: 2.0769\n",
      "Epoch [6/20], Step [2058/2541], D Loss: 0.6580, G Loss: 2.1858\n",
      "Epoch [6/20], Step [2059/2541], D Loss: 0.6562, G Loss: 2.1019\n",
      "Epoch [6/20], Step [2060/2541], D Loss: 0.6516, G Loss: 2.1059\n",
      "Epoch [6/20], Step [2061/2541], D Loss: 0.6561, G Loss: 2.0589\n",
      "Epoch [6/20], Step [2062/2541], D Loss: 0.6560, G Loss: 2.1697\n",
      "Epoch [6/20], Step [2063/2541], D Loss: 0.6563, G Loss: 2.0946\n",
      "Epoch [6/20], Step [2064/2541], D Loss: 0.6513, G Loss: 2.1335\n",
      "Epoch [6/20], Step [2065/2541], D Loss: 0.6522, G Loss: 2.1178\n",
      "Epoch [6/20], Step [2066/2541], D Loss: 0.6509, G Loss: 1.9721\n",
      "Epoch [6/20], Step [2067/2541], D Loss: 0.6512, G Loss: 2.0598\n",
      "Epoch [6/20], Step [2068/2541], D Loss: 0.6505, G Loss: 2.1316\n",
      "Epoch [6/20], Step [2069/2541], D Loss: 0.6512, G Loss: 2.0199\n",
      "Epoch [6/20], Step [2070/2541], D Loss: 0.6522, G Loss: 2.1821\n",
      "Epoch [6/20], Step [2071/2541], D Loss: 0.6518, G Loss: 2.0287\n",
      "Epoch [6/20], Step [2072/2541], D Loss: 0.6504, G Loss: 2.0760\n",
      "Epoch [6/20], Step [2073/2541], D Loss: 0.6502, G Loss: 2.0331\n",
      "Epoch [6/20], Step [2074/2541], D Loss: 0.6503, G Loss: 2.0168\n",
      "Epoch [6/20], Step [2075/2541], D Loss: 0.6515, G Loss: 2.2178\n",
      "Epoch [6/20], Step [2076/2541], D Loss: 0.6537, G Loss: 2.0612\n",
      "Epoch [6/20], Step [2077/2541], D Loss: 0.6512, G Loss: 2.0803\n",
      "Epoch [6/20], Step [2078/2541], D Loss: 0.6506, G Loss: 2.2030\n",
      "Epoch [6/20], Step [2079/2541], D Loss: 0.6512, G Loss: 2.0744\n",
      "Epoch [6/20], Step [2080/2541], D Loss: 0.6505, G Loss: 2.0486\n",
      "Epoch [6/20], Step [2081/2541], D Loss: 0.6535, G Loss: 2.0542\n",
      "Epoch [6/20], Step [2082/2541], D Loss: 0.6533, G Loss: 2.0945\n",
      "Epoch [6/20], Step [2083/2541], D Loss: 0.6507, G Loss: 2.0845\n",
      "Epoch [6/20], Step [2084/2541], D Loss: 0.6519, G Loss: 2.1170\n",
      "Epoch [6/20], Step [2085/2541], D Loss: 0.6509, G Loss: 2.1482\n",
      "Epoch [6/20], Step [2086/2541], D Loss: 0.6522, G Loss: 2.1018\n",
      "Epoch [6/20], Step [2087/2541], D Loss: 0.6515, G Loss: 2.0041\n",
      "Epoch [6/20], Step [2088/2541], D Loss: 0.6512, G Loss: 2.1278\n",
      "Epoch [6/20], Step [2089/2541], D Loss: 0.6505, G Loss: 2.0887\n",
      "Epoch [6/20], Step [2090/2541], D Loss: 0.6506, G Loss: 2.0938\n",
      "Epoch [6/20], Step [2091/2541], D Loss: 0.6517, G Loss: 2.1075\n",
      "Epoch [6/20], Step [2092/2541], D Loss: 0.6509, G Loss: 2.0413\n",
      "Epoch [6/20], Step [2093/2541], D Loss: 0.6505, G Loss: 2.1080\n",
      "Epoch [6/20], Step [2094/2541], D Loss: 0.6504, G Loss: 2.1021\n",
      "Epoch [6/20], Step [2095/2541], D Loss: 0.6507, G Loss: 2.0483\n",
      "Epoch [6/20], Step [2096/2541], D Loss: 0.6505, G Loss: 2.0857\n",
      "Epoch [6/20], Step [2097/2541], D Loss: 0.6505, G Loss: 2.0969\n",
      "Epoch [6/20], Step [2098/2541], D Loss: 0.6512, G Loss: 2.0645\n",
      "Epoch [6/20], Step [2099/2541], D Loss: 0.6527, G Loss: 2.1082\n",
      "Epoch [6/20], Step [2100/2541], D Loss: 0.6504, G Loss: 2.0965\n",
      "Epoch [6/20], Step [2101/2541], D Loss: 0.6523, G Loss: 2.0421\n",
      "Epoch [6/20], Step [2102/2541], D Loss: 0.6506, G Loss: 2.0509\n",
      "Epoch [6/20], Step [2103/2541], D Loss: 0.6510, G Loss: 2.0977\n",
      "Epoch [6/20], Step [2104/2541], D Loss: 0.6518, G Loss: 2.0539\n",
      "Epoch [6/20], Step [2105/2541], D Loss: 0.6508, G Loss: 2.0738\n",
      "Epoch [6/20], Step [2106/2541], D Loss: 0.6502, G Loss: 2.1127\n",
      "Epoch [6/20], Step [2107/2541], D Loss: 0.6505, G Loss: 2.0908\n",
      "Epoch [6/20], Step [2108/2541], D Loss: 0.6507, G Loss: 2.0823\n",
      "Epoch [6/20], Step [2109/2541], D Loss: 0.6502, G Loss: 2.1120\n",
      "Epoch [6/20], Step [2110/2541], D Loss: 0.6506, G Loss: 2.0675\n",
      "Epoch [6/20], Step [2111/2541], D Loss: 0.6507, G Loss: 2.0750\n",
      "Epoch [6/20], Step [2112/2541], D Loss: 0.6510, G Loss: 2.0273\n",
      "Epoch [6/20], Step [2113/2541], D Loss: 0.6523, G Loss: 2.0900\n",
      "Epoch [6/20], Step [2114/2541], D Loss: 0.6507, G Loss: 2.0865\n",
      "Epoch [6/20], Step [2115/2541], D Loss: 0.6506, G Loss: 2.1211\n",
      "Epoch [6/20], Step [2116/2541], D Loss: 0.6508, G Loss: 2.0531\n",
      "Epoch [6/20], Step [2117/2541], D Loss: 0.6506, G Loss: 2.0823\n",
      "Epoch [6/20], Step [2118/2541], D Loss: 0.6504, G Loss: 2.0899\n",
      "Epoch [6/20], Step [2119/2541], D Loss: 0.6505, G Loss: 2.0788\n",
      "Epoch [6/20], Step [2120/2541], D Loss: 0.6509, G Loss: 2.0755\n",
      "Epoch [6/20], Step [2121/2541], D Loss: 0.6508, G Loss: 2.0693\n",
      "Epoch [6/20], Step [2122/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [6/20], Step [2123/2541], D Loss: 0.6524, G Loss: 2.0659\n",
      "Epoch [6/20], Step [2124/2541], D Loss: 0.6513, G Loss: 2.0914\n",
      "Epoch [6/20], Step [2125/2541], D Loss: 0.6512, G Loss: 2.1281\n",
      "Epoch [6/20], Step [2126/2541], D Loss: 0.6504, G Loss: 2.0822\n",
      "Epoch [6/20], Step [2127/2541], D Loss: 0.6511, G Loss: 2.0296\n",
      "Epoch [6/20], Step [2128/2541], D Loss: 0.6507, G Loss: 2.1135\n",
      "Epoch [6/20], Step [2129/2541], D Loss: 0.6504, G Loss: 2.0680\n",
      "Epoch [6/20], Step [2130/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [6/20], Step [2131/2541], D Loss: 0.6508, G Loss: 2.0937\n",
      "Epoch [6/20], Step [2132/2541], D Loss: 0.6505, G Loss: 2.0547\n",
      "Epoch [6/20], Step [2133/2541], D Loss: 0.6510, G Loss: 2.0987\n",
      "Epoch [6/20], Step [2134/2541], D Loss: 0.6504, G Loss: 2.1428\n",
      "Epoch [6/20], Step [2135/2541], D Loss: 0.6508, G Loss: 2.0307\n",
      "Epoch [6/20], Step [2136/2541], D Loss: 0.6506, G Loss: 2.0614\n",
      "Epoch [6/20], Step [2137/2541], D Loss: 0.6519, G Loss: 2.0883\n",
      "Epoch [6/20], Step [2138/2541], D Loss: 0.6522, G Loss: 2.1130\n",
      "Epoch [6/20], Step [2139/2541], D Loss: 0.6508, G Loss: 2.0758\n",
      "Epoch [6/20], Step [2140/2541], D Loss: 0.6510, G Loss: 2.0576\n",
      "Epoch [6/20], Step [2141/2541], D Loss: 0.6505, G Loss: 2.0782\n",
      "Epoch [6/20], Step [2142/2541], D Loss: 0.6504, G Loss: 2.0932\n",
      "Epoch [6/20], Step [2143/2541], D Loss: 0.6502, G Loss: 2.1052\n",
      "Epoch [6/20], Step [2144/2541], D Loss: 0.6504, G Loss: 2.0450\n",
      "Epoch [6/20], Step [2145/2541], D Loss: 0.6511, G Loss: 2.0724\n",
      "Epoch [6/20], Step [2146/2541], D Loss: 0.6529, G Loss: 2.0771\n",
      "Epoch [6/20], Step [2147/2541], D Loss: 0.6505, G Loss: 2.0851\n",
      "Epoch [6/20], Step [2148/2541], D Loss: 0.6506, G Loss: 2.0968\n",
      "Epoch [6/20], Step [2149/2541], D Loss: 0.6503, G Loss: 2.0702\n",
      "Epoch [6/20], Step [2150/2541], D Loss: 0.6507, G Loss: 2.0422\n",
      "Epoch [6/20], Step [2151/2541], D Loss: 0.6503, G Loss: 2.1206\n",
      "Epoch [6/20], Step [2152/2541], D Loss: 0.6509, G Loss: 2.0720\n",
      "Epoch [6/20], Step [2153/2541], D Loss: 0.6556, G Loss: 2.0745\n",
      "Epoch [6/20], Step [2154/2541], D Loss: 0.6506, G Loss: 2.0641\n",
      "Epoch [6/20], Step [2155/2541], D Loss: 0.6504, G Loss: 2.1098\n",
      "Epoch [6/20], Step [2156/2541], D Loss: 0.6504, G Loss: 2.0963\n",
      "Epoch [6/20], Step [2157/2541], D Loss: 0.6506, G Loss: 2.0686\n",
      "Epoch [6/20], Step [2158/2541], D Loss: 0.6512, G Loss: 2.0724\n",
      "Epoch [6/20], Step [2159/2541], D Loss: 0.6504, G Loss: 2.0887\n",
      "Epoch [6/20], Step [2160/2541], D Loss: 0.6503, G Loss: 2.0953\n",
      "Epoch [6/20], Step [2161/2541], D Loss: 0.6504, G Loss: 2.0590\n",
      "Epoch [6/20], Step [2162/2541], D Loss: 0.6504, G Loss: 2.1059\n",
      "Epoch [6/20], Step [2163/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [6/20], Step [2164/2541], D Loss: 0.6508, G Loss: 2.1048\n",
      "Epoch [6/20], Step [2165/2541], D Loss: 0.6508, G Loss: 2.0619\n",
      "Epoch [6/20], Step [2166/2541], D Loss: 0.6508, G Loss: 2.0979\n",
      "Epoch [6/20], Step [2167/2541], D Loss: 0.6502, G Loss: 2.0959\n",
      "Epoch [6/20], Step [2168/2541], D Loss: 0.6506, G Loss: 2.0755\n",
      "Epoch [6/20], Step [2169/2541], D Loss: 0.6509, G Loss: 2.0840\n",
      "Epoch [6/20], Step [2170/2541], D Loss: 0.6505, G Loss: 2.0782\n",
      "Epoch [6/20], Step [2171/2541], D Loss: 0.6503, G Loss: 2.0408\n",
      "Epoch [6/20], Step [2172/2541], D Loss: 0.6507, G Loss: 2.1041\n",
      "Epoch [6/20], Step [2173/2541], D Loss: 0.6505, G Loss: 2.0966\n",
      "Epoch [6/20], Step [2174/2541], D Loss: 0.6505, G Loss: 2.0348\n",
      "Epoch [6/20], Step [2175/2541], D Loss: 0.6504, G Loss: 2.0485\n",
      "Epoch [6/20], Step [2176/2541], D Loss: 0.6503, G Loss: 2.1017\n",
      "Epoch [6/20], Step [2177/2541], D Loss: 0.6505, G Loss: 2.0844\n",
      "Epoch [6/20], Step [2178/2541], D Loss: 0.6504, G Loss: 2.0828\n",
      "Epoch [6/20], Step [2179/2541], D Loss: 0.6509, G Loss: 2.0486\n",
      "Epoch [6/20], Step [2180/2541], D Loss: 0.6511, G Loss: 2.0849\n",
      "Epoch [6/20], Step [2181/2541], D Loss: 0.6503, G Loss: 2.0967\n",
      "Epoch [6/20], Step [2182/2541], D Loss: 0.6505, G Loss: 2.0683\n",
      "Epoch [6/20], Step [2183/2541], D Loss: 0.6503, G Loss: 2.0771\n",
      "Epoch [6/20], Step [2184/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [6/20], Step [2185/2541], D Loss: 0.6503, G Loss: 2.0664\n",
      "Epoch [6/20], Step [2186/2541], D Loss: 0.6503, G Loss: 2.0981\n",
      "Epoch [6/20], Step [2187/2541], D Loss: 0.6503, G Loss: 2.0773\n",
      "Epoch [6/20], Step [2188/2541], D Loss: 0.6503, G Loss: 2.0733\n",
      "Epoch [6/20], Step [2189/2541], D Loss: 0.6503, G Loss: 2.0752\n",
      "Epoch [6/20], Step [2190/2541], D Loss: 0.6504, G Loss: 2.0598\n",
      "Epoch [6/20], Step [2191/2541], D Loss: 0.6505, G Loss: 2.0667\n",
      "Epoch [6/20], Step [2192/2541], D Loss: 0.6503, G Loss: 2.0986\n",
      "Epoch [6/20], Step [2193/2541], D Loss: 0.6503, G Loss: 2.0904\n",
      "Epoch [6/20], Step [2194/2541], D Loss: 0.6502, G Loss: 2.1048\n",
      "Epoch [6/20], Step [2195/2541], D Loss: 0.6505, G Loss: 2.0804\n",
      "Epoch [6/20], Step [2196/2541], D Loss: 0.6504, G Loss: 2.0800\n",
      "Epoch [6/20], Step [2197/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [6/20], Step [2198/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [6/20], Step [2199/2541], D Loss: 0.6502, G Loss: 2.1164\n",
      "Epoch [6/20], Step [2200/2541], D Loss: 0.6502, G Loss: 2.1185\n",
      "Epoch [6/20], Step [2201/2541], D Loss: 0.6503, G Loss: 2.0569\n",
      "Epoch [6/20], Step [2202/2541], D Loss: 0.6509, G Loss: 2.0990\n",
      "Epoch [6/20], Step [2203/2541], D Loss: 0.6504, G Loss: 2.0872\n",
      "Epoch [6/20], Step [2204/2541], D Loss: 0.6503, G Loss: 2.0900\n",
      "Epoch [6/20], Step [2205/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [6/20], Step [2206/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [6/20], Step [2207/2541], D Loss: 0.6503, G Loss: 2.0849\n",
      "Epoch [6/20], Step [2208/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [6/20], Step [2209/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [6/20], Step [2210/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [2211/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [6/20], Step [2212/2541], D Loss: 0.6504, G Loss: 2.0332\n",
      "Epoch [6/20], Step [2213/2541], D Loss: 0.6503, G Loss: 2.0967\n",
      "Epoch [6/20], Step [2214/2541], D Loss: 0.6503, G Loss: 2.0999\n",
      "Epoch [6/20], Step [2215/2541], D Loss: 0.6502, G Loss: 2.1202\n",
      "Epoch [6/20], Step [2216/2541], D Loss: 0.6505, G Loss: 2.0724\n",
      "Epoch [6/20], Step [2217/2541], D Loss: 0.6507, G Loss: 2.1137\n",
      "Epoch [6/20], Step [2218/2541], D Loss: 0.6505, G Loss: 2.1323\n",
      "Epoch [6/20], Step [2219/2541], D Loss: 0.6509, G Loss: 2.0259\n",
      "Epoch [6/20], Step [2220/2541], D Loss: 0.6509, G Loss: 2.0236\n",
      "Epoch [6/20], Step [2221/2541], D Loss: 0.6511, G Loss: 2.1466\n",
      "Epoch [6/20], Step [2222/2541], D Loss: 0.6511, G Loss: 2.0839\n",
      "Epoch [6/20], Step [2223/2541], D Loss: 0.6504, G Loss: 2.0722\n",
      "Epoch [6/20], Step [2224/2541], D Loss: 0.6506, G Loss: 2.0912\n",
      "Epoch [6/20], Step [2225/2541], D Loss: 0.6510, G Loss: 2.0932\n",
      "Epoch [6/20], Step [2226/2541], D Loss: 0.6503, G Loss: 2.0784\n",
      "Epoch [6/20], Step [2227/2541], D Loss: 0.6504, G Loss: 2.0662\n",
      "Epoch [6/20], Step [2228/2541], D Loss: 0.6505, G Loss: 2.1770\n",
      "Epoch [6/20], Step [2229/2541], D Loss: 0.6509, G Loss: 2.0666\n",
      "Epoch [6/20], Step [2230/2541], D Loss: 0.6503, G Loss: 2.0647\n",
      "Epoch [6/20], Step [2231/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [6/20], Step [2232/2541], D Loss: 0.6505, G Loss: 2.0305\n",
      "Epoch [6/20], Step [2233/2541], D Loss: 0.6506, G Loss: 2.0729\n",
      "Epoch [6/20], Step [2234/2541], D Loss: 0.6504, G Loss: 2.0747\n",
      "Epoch [6/20], Step [2235/2541], D Loss: 0.6503, G Loss: 2.0675\n",
      "Epoch [6/20], Step [2236/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [6/20], Step [2237/2541], D Loss: 0.6504, G Loss: 2.1253\n",
      "Epoch [6/20], Step [2238/2541], D Loss: 0.6504, G Loss: 2.0617\n",
      "Epoch [6/20], Step [2239/2541], D Loss: 0.6503, G Loss: 2.0728\n",
      "Epoch [6/20], Step [2240/2541], D Loss: 0.6503, G Loss: 2.1129\n",
      "Epoch [6/20], Step [2241/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [6/20], Step [2242/2541], D Loss: 0.6503, G Loss: 2.0551\n",
      "Epoch [6/20], Step [2243/2541], D Loss: 0.6503, G Loss: 2.0913\n",
      "Epoch [6/20], Step [2244/2541], D Loss: 0.6503, G Loss: 2.0653\n",
      "Epoch [6/20], Step [2245/2541], D Loss: 0.6504, G Loss: 2.0769\n",
      "Epoch [6/20], Step [2246/2541], D Loss: 0.6503, G Loss: 2.0923\n",
      "Epoch [6/20], Step [2247/2541], D Loss: 0.6502, G Loss: 2.0609\n",
      "Epoch [6/20], Step [2248/2541], D Loss: 0.6503, G Loss: 2.0906\n",
      "Epoch [6/20], Step [2249/2541], D Loss: 0.6504, G Loss: 2.0938\n",
      "Epoch [6/20], Step [2250/2541], D Loss: 0.6506, G Loss: 2.0478\n",
      "Epoch [6/20], Step [2251/2541], D Loss: 0.6504, G Loss: 2.0657\n",
      "Epoch [6/20], Step [2252/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [6/20], Step [2253/2541], D Loss: 0.6504, G Loss: 2.0564\n",
      "Epoch [6/20], Step [2254/2541], D Loss: 0.6504, G Loss: 2.0904\n",
      "Epoch [6/20], Step [2255/2541], D Loss: 0.6502, G Loss: 2.1122\n",
      "Epoch [6/20], Step [2256/2541], D Loss: 0.6506, G Loss: 2.0728\n",
      "Epoch [6/20], Step [2257/2541], D Loss: 0.6502, G Loss: 2.0563\n",
      "Epoch [6/20], Step [2258/2541], D Loss: 0.6503, G Loss: 2.0775\n",
      "Epoch [6/20], Step [2259/2541], D Loss: 0.6502, G Loss: 2.0792\n",
      "Epoch [6/20], Step [2260/2541], D Loss: 0.6502, G Loss: 2.1018\n",
      "Epoch [6/20], Step [2261/2541], D Loss: 0.6502, G Loss: 2.0717\n",
      "Epoch [6/20], Step [2262/2541], D Loss: 0.6504, G Loss: 2.0918\n",
      "Epoch [6/20], Step [2263/2541], D Loss: 0.6503, G Loss: 2.1087\n",
      "Epoch [6/20], Step [2264/2541], D Loss: 0.6503, G Loss: 2.0514\n",
      "Epoch [6/20], Step [2265/2541], D Loss: 0.6504, G Loss: 2.1045\n",
      "Epoch [6/20], Step [2266/2541], D Loss: 0.6503, G Loss: 2.0958\n",
      "Epoch [6/20], Step [2267/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [6/20], Step [2268/2541], D Loss: 0.6502, G Loss: 2.0725\n",
      "Epoch [6/20], Step [2269/2541], D Loss: 0.6503, G Loss: 2.0976\n",
      "Epoch [6/20], Step [2270/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [6/20], Step [2271/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [6/20], Step [2272/2541], D Loss: 0.6502, G Loss: 2.1036\n",
      "Epoch [6/20], Step [2273/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [6/20], Step [2274/2541], D Loss: 0.6503, G Loss: 2.0971\n",
      "Epoch [6/20], Step [2275/2541], D Loss: 0.6504, G Loss: 2.0783\n",
      "Epoch [6/20], Step [2276/2541], D Loss: 0.6510, G Loss: 2.0824\n",
      "Epoch [6/20], Step [2277/2541], D Loss: 0.6502, G Loss: 2.0649\n",
      "Epoch [6/20], Step [2278/2541], D Loss: 0.6504, G Loss: 2.0751\n",
      "Epoch [6/20], Step [2279/2541], D Loss: 0.6504, G Loss: 2.0759\n",
      "Epoch [6/20], Step [2280/2541], D Loss: 0.6504, G Loss: 2.0630\n",
      "Epoch [6/20], Step [2281/2541], D Loss: 0.6503, G Loss: 2.0758\n",
      "Epoch [6/20], Step [2282/2541], D Loss: 0.6502, G Loss: 2.0696\n",
      "Epoch [6/20], Step [2283/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [6/20], Step [2284/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [6/20], Step [2285/2541], D Loss: 0.6502, G Loss: 2.1057\n",
      "Epoch [6/20], Step [2286/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [6/20], Step [2287/2541], D Loss: 0.6503, G Loss: 2.0945\n",
      "Epoch [6/20], Step [2288/2541], D Loss: 0.6503, G Loss: 2.0580\n",
      "Epoch [6/20], Step [2289/2541], D Loss: 0.6503, G Loss: 2.1013\n",
      "Epoch [6/20], Step [2290/2541], D Loss: 0.6504, G Loss: 2.0986\n",
      "Epoch [6/20], Step [2291/2541], D Loss: 0.6502, G Loss: 2.0568\n",
      "Epoch [6/20], Step [2292/2541], D Loss: 0.6505, G Loss: 2.1395\n",
      "Epoch [6/20], Step [2293/2541], D Loss: 0.6505, G Loss: 2.0892\n",
      "Epoch [6/20], Step [2294/2541], D Loss: 0.6502, G Loss: 2.0591\n",
      "Epoch [6/20], Step [2295/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [6/20], Step [2296/2541], D Loss: 0.6502, G Loss: 2.0968\n",
      "Epoch [6/20], Step [2297/2541], D Loss: 0.6503, G Loss: 2.0852\n",
      "Epoch [6/20], Step [2298/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [6/20], Step [2299/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [6/20], Step [2300/2541], D Loss: 0.6502, G Loss: 2.0876\n",
      "Epoch [6/20], Step [2301/2541], D Loss: 0.6503, G Loss: 2.0878\n",
      "Epoch [6/20], Step [2302/2541], D Loss: 0.6504, G Loss: 2.0728\n",
      "Epoch [6/20], Step [2303/2541], D Loss: 0.6503, G Loss: 2.1030\n",
      "Epoch [6/20], Step [2304/2541], D Loss: 0.6503, G Loss: 2.0759\n",
      "Epoch [6/20], Step [2305/2541], D Loss: 0.6503, G Loss: 2.0806\n",
      "Epoch [6/20], Step [2306/2541], D Loss: 0.6502, G Loss: 2.0956\n",
      "Epoch [6/20], Step [2307/2541], D Loss: 0.6502, G Loss: 2.0946\n",
      "Epoch [6/20], Step [2308/2541], D Loss: 0.6504, G Loss: 2.0969\n",
      "Epoch [6/20], Step [2309/2541], D Loss: 0.6503, G Loss: 2.0849\n",
      "Epoch [6/20], Step [2310/2541], D Loss: 0.6502, G Loss: 2.0987\n",
      "Epoch [6/20], Step [2311/2541], D Loss: 0.6502, G Loss: 2.0661\n",
      "Epoch [6/20], Step [2312/2541], D Loss: 0.6502, G Loss: 2.1086\n",
      "Epoch [6/20], Step [2313/2541], D Loss: 0.6502, G Loss: 2.0703\n",
      "Epoch [6/20], Step [2314/2541], D Loss: 0.6504, G Loss: 2.0780\n",
      "Epoch [6/20], Step [2315/2541], D Loss: 0.6504, G Loss: 2.1043\n",
      "Epoch [6/20], Step [2316/2541], D Loss: 0.6502, G Loss: 2.0672\n",
      "Epoch [6/20], Step [2317/2541], D Loss: 0.6503, G Loss: 2.0981\n",
      "Epoch [6/20], Step [2318/2541], D Loss: 0.6502, G Loss: 2.0941\n",
      "Epoch [6/20], Step [2319/2541], D Loss: 0.6502, G Loss: 2.0733\n",
      "Epoch [6/20], Step [2320/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [6/20], Step [2321/2541], D Loss: 0.6502, G Loss: 2.0912\n",
      "Epoch [6/20], Step [2322/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [6/20], Step [2323/2541], D Loss: 0.6503, G Loss: 2.0829\n",
      "Epoch [6/20], Step [2324/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [6/20], Step [2325/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [6/20], Step [2326/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [2327/2541], D Loss: 0.6502, G Loss: 2.0981\n",
      "Epoch [6/20], Step [2328/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [6/20], Step [2329/2541], D Loss: 0.6506, G Loss: 2.0832\n",
      "Epoch [6/20], Step [2330/2541], D Loss: 0.6503, G Loss: 2.0800\n",
      "Epoch [6/20], Step [2331/2541], D Loss: 0.6504, G Loss: 2.0770\n",
      "Epoch [6/20], Step [2332/2541], D Loss: 0.6503, G Loss: 2.1001\n",
      "Epoch [6/20], Step [2333/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [2334/2541], D Loss: 0.6504, G Loss: 2.1122\n",
      "Epoch [6/20], Step [2335/2541], D Loss: 0.6504, G Loss: 2.0892\n",
      "Epoch [6/20], Step [2336/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [6/20], Step [2337/2541], D Loss: 0.6502, G Loss: 2.0645\n",
      "Epoch [6/20], Step [2338/2541], D Loss: 0.6503, G Loss: 2.0908\n",
      "Epoch [6/20], Step [2339/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [6/20], Step [2340/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [6/20], Step [2341/2541], D Loss: 0.6502, G Loss: 2.0985\n",
      "Epoch [6/20], Step [2342/2541], D Loss: 0.6507, G Loss: 2.0576\n",
      "Epoch [6/20], Step [2343/2541], D Loss: 0.6508, G Loss: 2.0803\n",
      "Epoch [6/20], Step [2344/2541], D Loss: 0.6502, G Loss: 2.0981\n",
      "Epoch [6/20], Step [2345/2541], D Loss: 0.6503, G Loss: 2.0547\n",
      "Epoch [6/20], Step [2346/2541], D Loss: 0.6502, G Loss: 2.0951\n",
      "Epoch [6/20], Step [2347/2541], D Loss: 0.6504, G Loss: 2.0809\n",
      "Epoch [6/20], Step [2348/2541], D Loss: 0.6503, G Loss: 2.0716\n",
      "Epoch [6/20], Step [2349/2541], D Loss: 0.6503, G Loss: 2.0756\n",
      "Epoch [6/20], Step [2350/2541], D Loss: 0.6510, G Loss: 2.1137\n",
      "Epoch [6/20], Step [2351/2541], D Loss: 0.6512, G Loss: 2.0635\n",
      "Epoch [6/20], Step [2352/2541], D Loss: 0.6506, G Loss: 2.0622\n",
      "Epoch [6/20], Step [2353/2541], D Loss: 0.6504, G Loss: 2.0876\n",
      "Epoch [6/20], Step [2354/2541], D Loss: 0.6504, G Loss: 2.0650\n",
      "Epoch [6/20], Step [2355/2541], D Loss: 0.6502, G Loss: 2.0634\n",
      "Epoch [6/20], Step [2356/2541], D Loss: 0.6503, G Loss: 2.1002\n",
      "Epoch [6/20], Step [2357/2541], D Loss: 0.6502, G Loss: 2.0729\n",
      "Epoch [6/20], Step [2358/2541], D Loss: 0.6502, G Loss: 2.0653\n",
      "Epoch [6/20], Step [2359/2541], D Loss: 0.6502, G Loss: 2.1245\n",
      "Epoch [6/20], Step [2360/2541], D Loss: 0.6503, G Loss: 2.0739\n",
      "Epoch [6/20], Step [2361/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [6/20], Step [2362/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [6/20], Step [2363/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [6/20], Step [2364/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [6/20], Step [2365/2541], D Loss: 0.6507, G Loss: 2.1010\n",
      "Epoch [6/20], Step [2366/2541], D Loss: 0.6504, G Loss: 2.0650\n",
      "Epoch [6/20], Step [2367/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [6/20], Step [2368/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [6/20], Step [2369/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [6/20], Step [2370/2541], D Loss: 0.6502, G Loss: 2.0733\n",
      "Epoch [6/20], Step [2371/2541], D Loss: 0.6502, G Loss: 2.0709\n",
      "Epoch [6/20], Step [2372/2541], D Loss: 0.6502, G Loss: 2.1152\n",
      "Epoch [6/20], Step [2373/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [6/20], Step [2374/2541], D Loss: 0.6502, G Loss: 2.0698\n",
      "Epoch [6/20], Step [2375/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [6/20], Step [2376/2541], D Loss: 0.6502, G Loss: 2.1139\n",
      "Epoch [6/20], Step [2377/2541], D Loss: 0.6503, G Loss: 2.0901\n",
      "Epoch [6/20], Step [2378/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [6/20], Step [2379/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [6/20], Step [2380/2541], D Loss: 0.6502, G Loss: 2.0651\n",
      "Epoch [6/20], Step [2381/2541], D Loss: 0.6502, G Loss: 2.0942\n",
      "Epoch [6/20], Step [2382/2541], D Loss: 0.6502, G Loss: 2.1201\n",
      "Epoch [6/20], Step [2383/2541], D Loss: 0.6503, G Loss: 2.0682\n",
      "Epoch [6/20], Step [2384/2541], D Loss: 0.6502, G Loss: 2.0643\n",
      "Epoch [6/20], Step [2385/2541], D Loss: 0.6503, G Loss: 2.0932\n",
      "Epoch [6/20], Step [2386/2541], D Loss: 0.6503, G Loss: 2.1001\n",
      "Epoch [6/20], Step [2387/2541], D Loss: 0.6503, G Loss: 2.0784\n",
      "Epoch [6/20], Step [2388/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [6/20], Step [2389/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [6/20], Step [2390/2541], D Loss: 0.6503, G Loss: 2.1030\n",
      "Epoch [6/20], Step [2391/2541], D Loss: 0.6503, G Loss: 2.0690\n",
      "Epoch [6/20], Step [2392/2541], D Loss: 0.6503, G Loss: 2.0819\n",
      "Epoch [6/20], Step [2393/2541], D Loss: 0.6505, G Loss: 2.1209\n",
      "Epoch [6/20], Step [2394/2541], D Loss: 0.6504, G Loss: 2.0812\n",
      "Epoch [6/20], Step [2395/2541], D Loss: 0.6502, G Loss: 2.0408\n",
      "Epoch [6/20], Step [2396/2541], D Loss: 0.6503, G Loss: 2.0550\n",
      "Epoch [6/20], Step [2397/2541], D Loss: 0.6503, G Loss: 2.0499\n",
      "Epoch [6/20], Step [2398/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [6/20], Step [2399/2541], D Loss: 0.6503, G Loss: 2.1086\n",
      "Epoch [6/20], Step [2400/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [6/20], Step [2401/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [6/20], Step [2402/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [6/20], Step [2403/2541], D Loss: 0.6502, G Loss: 2.0956\n",
      "Epoch [6/20], Step [2404/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [6/20], Step [2405/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [2406/2541], D Loss: 0.6502, G Loss: 2.0757\n",
      "Epoch [6/20], Step [2407/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [6/20], Step [2408/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [6/20], Step [2409/2541], D Loss: 0.6502, G Loss: 2.0678\n",
      "Epoch [6/20], Step [2410/2541], D Loss: 0.6502, G Loss: 2.0757\n",
      "Epoch [6/20], Step [2411/2541], D Loss: 0.6502, G Loss: 2.0595\n",
      "Epoch [6/20], Step [2412/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [6/20], Step [2413/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [6/20], Step [2414/2541], D Loss: 0.6502, G Loss: 2.1018\n",
      "Epoch [6/20], Step [2415/2541], D Loss: 0.6502, G Loss: 2.1083\n",
      "Epoch [6/20], Step [2416/2541], D Loss: 0.6502, G Loss: 2.0666\n",
      "Epoch [6/20], Step [2417/2541], D Loss: 0.6503, G Loss: 2.0755\n",
      "Epoch [6/20], Step [2418/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [6/20], Step [2419/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [2420/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [6/20], Step [2421/2541], D Loss: 0.6502, G Loss: 2.0996\n",
      "Epoch [6/20], Step [2422/2541], D Loss: 0.6503, G Loss: 2.1003\n",
      "Epoch [6/20], Step [2423/2541], D Loss: 0.6503, G Loss: 2.0692\n",
      "Epoch [6/20], Step [2424/2541], D Loss: 0.6505, G Loss: 2.0888\n",
      "Epoch [6/20], Step [2425/2541], D Loss: 0.6503, G Loss: 2.0899\n",
      "Epoch [6/20], Step [2426/2541], D Loss: 0.6503, G Loss: 2.1104\n",
      "Epoch [6/20], Step [2427/2541], D Loss: 0.6508, G Loss: 2.0522\n",
      "Epoch [6/20], Step [2428/2541], D Loss: 0.6503, G Loss: 2.1050\n",
      "Epoch [6/20], Step [2429/2541], D Loss: 0.6502, G Loss: 2.0714\n",
      "Epoch [6/20], Step [2430/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [6/20], Step [2431/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [6/20], Step [2432/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [6/20], Step [2433/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [6/20], Step [2434/2541], D Loss: 0.6502, G Loss: 2.0591\n",
      "Epoch [6/20], Step [2435/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [6/20], Step [2436/2541], D Loss: 0.6502, G Loss: 2.0968\n",
      "Epoch [6/20], Step [2437/2541], D Loss: 0.6507, G Loss: 2.0525\n",
      "Epoch [6/20], Step [2438/2541], D Loss: 0.6505, G Loss: 2.0796\n",
      "Epoch [6/20], Step [2439/2541], D Loss: 0.6503, G Loss: 2.0989\n",
      "Epoch [6/20], Step [2440/2541], D Loss: 0.6502, G Loss: 2.0901\n",
      "Epoch [6/20], Step [2441/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [6/20], Step [2442/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [6/20], Step [2443/2541], D Loss: 0.6502, G Loss: 2.0926\n",
      "Epoch [6/20], Step [2444/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [6/20], Step [2445/2541], D Loss: 0.6502, G Loss: 2.1080\n",
      "Epoch [6/20], Step [2446/2541], D Loss: 0.6502, G Loss: 2.1295\n",
      "Epoch [6/20], Step [2447/2541], D Loss: 0.6503, G Loss: 2.0595\n",
      "Epoch [6/20], Step [2448/2541], D Loss: 0.6502, G Loss: 2.0638\n",
      "Epoch [6/20], Step [2449/2541], D Loss: 0.6502, G Loss: 2.0941\n",
      "Epoch [6/20], Step [2450/2541], D Loss: 0.6503, G Loss: 2.1500\n",
      "Epoch [6/20], Step [2451/2541], D Loss: 0.6507, G Loss: 2.0639\n",
      "Epoch [6/20], Step [2452/2541], D Loss: 0.6503, G Loss: 2.0713\n",
      "Epoch [6/20], Step [2453/2541], D Loss: 0.6503, G Loss: 2.1064\n",
      "Epoch [6/20], Step [2454/2541], D Loss: 0.6503, G Loss: 2.0920\n",
      "Epoch [6/20], Step [2455/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [6/20], Step [2456/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [6/20], Step [2457/2541], D Loss: 0.6502, G Loss: 2.0929\n",
      "Epoch [6/20], Step [2458/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [2459/2541], D Loss: 0.6503, G Loss: 2.1092\n",
      "Epoch [6/20], Step [2460/2541], D Loss: 0.6505, G Loss: 2.0905\n",
      "Epoch [6/20], Step [2461/2541], D Loss: 0.6504, G Loss: 2.0889\n",
      "Epoch [6/20], Step [2462/2541], D Loss: 0.6504, G Loss: 2.1046\n",
      "Epoch [6/20], Step [2463/2541], D Loss: 0.6502, G Loss: 2.0640\n",
      "Epoch [6/20], Step [2464/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [6/20], Step [2465/2541], D Loss: 0.6503, G Loss: 2.0555\n",
      "Epoch [6/20], Step [2466/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [6/20], Step [2467/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [6/20], Step [2468/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [6/20], Step [2469/2541], D Loss: 0.6502, G Loss: 2.0974\n",
      "Epoch [6/20], Step [2470/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [6/20], Step [2471/2541], D Loss: 0.6503, G Loss: 2.0792\n",
      "Epoch [6/20], Step [2472/2541], D Loss: 0.6502, G Loss: 2.1006\n",
      "Epoch [6/20], Step [2473/2541], D Loss: 0.6502, G Loss: 2.0951\n",
      "Epoch [6/20], Step [2474/2541], D Loss: 0.6504, G Loss: 2.0564\n",
      "Epoch [6/20], Step [2475/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [6/20], Step [2476/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [6/20], Step [2477/2541], D Loss: 0.6502, G Loss: 2.0925\n",
      "Epoch [6/20], Step [2478/2541], D Loss: 0.6504, G Loss: 2.0576\n",
      "Epoch [6/20], Step [2479/2541], D Loss: 0.6504, G Loss: 2.0880\n",
      "Epoch [6/20], Step [2480/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [6/20], Step [2481/2541], D Loss: 0.6502, G Loss: 2.0755\n",
      "Epoch [6/20], Step [2482/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [6/20], Step [2483/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [6/20], Step [2484/2541], D Loss: 0.6502, G Loss: 2.0677\n",
      "Epoch [6/20], Step [2485/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [6/20], Step [2486/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [6/20], Step [2487/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [6/20], Step [2488/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [6/20], Step [2489/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [6/20], Step [2490/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [6/20], Step [2491/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [6/20], Step [2492/2541], D Loss: 0.6503, G Loss: 2.1103\n",
      "Epoch [6/20], Step [2493/2541], D Loss: 0.6503, G Loss: 2.0932\n",
      "Epoch [6/20], Step [2494/2541], D Loss: 0.6503, G Loss: 2.0774\n",
      "Epoch [6/20], Step [2495/2541], D Loss: 0.6504, G Loss: 2.0896\n",
      "Epoch [6/20], Step [2496/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [6/20], Step [2497/2541], D Loss: 0.6502, G Loss: 2.0581\n",
      "Epoch [6/20], Step [2498/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [6/20], Step [2499/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [6/20], Step [2500/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [6/20], Step [2501/2541], D Loss: 0.6502, G Loss: 2.1067\n",
      "Epoch [6/20], Step [2502/2541], D Loss: 0.6503, G Loss: 2.0669\n",
      "Epoch [6/20], Step [2503/2541], D Loss: 0.6503, G Loss: 2.0745\n",
      "Epoch [6/20], Step [2504/2541], D Loss: 0.6502, G Loss: 2.1386\n",
      "Epoch [6/20], Step [2505/2541], D Loss: 0.6504, G Loss: 2.0955\n",
      "Epoch [6/20], Step [2506/2541], D Loss: 0.6502, G Loss: 2.0593\n",
      "Epoch [6/20], Step [2507/2541], D Loss: 0.6502, G Loss: 2.0936\n",
      "Epoch [6/20], Step [2508/2541], D Loss: 0.6503, G Loss: 2.0877\n",
      "Epoch [6/20], Step [2509/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [6/20], Step [2510/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [6/20], Step [2511/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [6/20], Step [2512/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [6/20], Step [2513/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [6/20], Step [2514/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [6/20], Step [2515/2541], D Loss: 0.6502, G Loss: 2.0961\n",
      "Epoch [6/20], Step [2516/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [6/20], Step [2517/2541], D Loss: 0.6502, G Loss: 2.0675\n",
      "Epoch [6/20], Step [2518/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [6/20], Step [2519/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [6/20], Step [2520/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [6/20], Step [2521/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [6/20], Step [2522/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [6/20], Step [2523/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [6/20], Step [2524/2541], D Loss: 0.6502, G Loss: 2.0697\n",
      "Epoch [6/20], Step [2525/2541], D Loss: 0.6502, G Loss: 2.0684\n",
      "Epoch [6/20], Step [2526/2541], D Loss: 0.6502, G Loss: 2.0968\n",
      "Epoch [6/20], Step [2527/2541], D Loss: 0.6503, G Loss: 2.0756\n",
      "Epoch [6/20], Step [2528/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [6/20], Step [2529/2541], D Loss: 0.6502, G Loss: 2.0904\n",
      "Epoch [6/20], Step [2530/2541], D Loss: 0.6502, G Loss: 2.0982\n",
      "Epoch [6/20], Step [2531/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [6/20], Step [2532/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [6/20], Step [2533/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [6/20], Step [2534/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [6/20], Step [2535/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [6/20], Step [2536/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [6/20], Step [2537/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [6/20], Step [2538/2541], D Loss: 0.6503, G Loss: 2.0749\n",
      "Epoch [6/20], Step [2539/2541], D Loss: 0.6503, G Loss: 2.0798\n",
      "Epoch [6/20], Step [2540/2541], D Loss: 0.6505, G Loss: 2.0322\n",
      "Models saved after epoch 6\n",
      "Epoch [7/20], Step [0/2541], D Loss: 0.6510, G Loss: 2.1120\n",
      "Epoch [7/20], Step [1/2541], D Loss: 0.6504, G Loss: 2.0939\n",
      "Epoch [7/20], Step [2/2541], D Loss: 0.6504, G Loss: 2.0603\n",
      "Epoch [7/20], Step [3/2541], D Loss: 0.6504, G Loss: 2.1049\n",
      "Epoch [7/20], Step [4/2541], D Loss: 0.6503, G Loss: 2.0724\n",
      "Epoch [7/20], Step [5/2541], D Loss: 0.6503, G Loss: 2.0490\n",
      "Epoch [7/20], Step [6/2541], D Loss: 0.6503, G Loss: 2.0887\n",
      "Epoch [7/20], Step [7/2541], D Loss: 0.6502, G Loss: 2.1121\n",
      "Epoch [7/20], Step [8/2541], D Loss: 0.6504, G Loss: 2.0409\n",
      "Epoch [7/20], Step [9/2541], D Loss: 0.6503, G Loss: 2.0801\n",
      "Epoch [7/20], Step [10/2541], D Loss: 0.6503, G Loss: 2.0928\n",
      "Epoch [7/20], Step [11/2541], D Loss: 0.6502, G Loss: 2.0704\n",
      "Epoch [7/20], Step [12/2541], D Loss: 0.6503, G Loss: 2.0797\n",
      "Epoch [7/20], Step [13/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [14/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [7/20], Step [15/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [16/2541], D Loss: 0.6502, G Loss: 2.0712\n",
      "Epoch [7/20], Step [17/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [7/20], Step [18/2541], D Loss: 0.6502, G Loss: 2.0909\n",
      "Epoch [7/20], Step [19/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [7/20], Step [20/2541], D Loss: 0.6502, G Loss: 2.0661\n",
      "Epoch [7/20], Step [21/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [7/20], Step [22/2541], D Loss: 0.6502, G Loss: 2.1032\n",
      "Epoch [7/20], Step [23/2541], D Loss: 0.6503, G Loss: 2.0749\n",
      "Epoch [7/20], Step [24/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [7/20], Step [25/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [7/20], Step [26/2541], D Loss: 0.6502, G Loss: 2.0875\n",
      "Epoch [7/20], Step [27/2541], D Loss: 0.6502, G Loss: 2.0669\n",
      "Epoch [7/20], Step [28/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [29/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [7/20], Step [30/2541], D Loss: 0.6502, G Loss: 2.0938\n",
      "Epoch [7/20], Step [31/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [7/20], Step [32/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [7/20], Step [33/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [7/20], Step [34/2541], D Loss: 0.6502, G Loss: 2.1062\n",
      "Epoch [7/20], Step [35/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [7/20], Step [36/2541], D Loss: 0.6502, G Loss: 2.0698\n",
      "Epoch [7/20], Step [37/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [7/20], Step [38/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [7/20], Step [39/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [7/20], Step [40/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [7/20], Step [41/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [7/20], Step [42/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [7/20], Step [43/2541], D Loss: 0.6502, G Loss: 2.1141\n",
      "Epoch [7/20], Step [44/2541], D Loss: 0.6503, G Loss: 2.0667\n",
      "Epoch [7/20], Step [45/2541], D Loss: 0.6502, G Loss: 2.0659\n",
      "Epoch [7/20], Step [46/2541], D Loss: 0.6502, G Loss: 2.1001\n",
      "Epoch [7/20], Step [47/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [7/20], Step [48/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [49/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [7/20], Step [50/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [7/20], Step [51/2541], D Loss: 0.6505, G Loss: 2.0634\n",
      "Epoch [7/20], Step [52/2541], D Loss: 0.6503, G Loss: 2.1242\n",
      "Epoch [7/20], Step [53/2541], D Loss: 0.6503, G Loss: 2.0902\n",
      "Epoch [7/20], Step [54/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [7/20], Step [55/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [7/20], Step [56/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [7/20], Step [57/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [7/20], Step [58/2541], D Loss: 0.6502, G Loss: 2.0630\n",
      "Epoch [7/20], Step [59/2541], D Loss: 0.6503, G Loss: 2.0825\n",
      "Epoch [7/20], Step [60/2541], D Loss: 0.6504, G Loss: 2.0945\n",
      "Epoch [7/20], Step [61/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [7/20], Step [62/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [63/2541], D Loss: 0.6502, G Loss: 2.0485\n",
      "Epoch [7/20], Step [64/2541], D Loss: 0.6503, G Loss: 2.0899\n",
      "Epoch [7/20], Step [65/2541], D Loss: 0.6503, G Loss: 2.0698\n",
      "Epoch [7/20], Step [66/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [7/20], Step [67/2541], D Loss: 0.6504, G Loss: 2.0955\n",
      "Epoch [7/20], Step [68/2541], D Loss: 0.6504, G Loss: 2.0830\n",
      "Epoch [7/20], Step [69/2541], D Loss: 0.6502, G Loss: 2.0639\n",
      "Epoch [7/20], Step [70/2541], D Loss: 0.6503, G Loss: 2.0978\n",
      "Epoch [7/20], Step [71/2541], D Loss: 0.6503, G Loss: 2.0771\n",
      "Epoch [7/20], Step [72/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [7/20], Step [73/2541], D Loss: 0.6502, G Loss: 2.0912\n",
      "Epoch [7/20], Step [74/2541], D Loss: 0.6502, G Loss: 2.0594\n",
      "Epoch [7/20], Step [75/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [7/20], Step [76/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [7/20], Step [77/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [7/20], Step [78/2541], D Loss: 0.6502, G Loss: 2.0679\n",
      "Epoch [7/20], Step [79/2541], D Loss: 0.6503, G Loss: 2.0917\n",
      "Epoch [7/20], Step [80/2541], D Loss: 0.6506, G Loss: 2.0690\n",
      "Epoch [7/20], Step [81/2541], D Loss: 0.6502, G Loss: 2.0721\n",
      "Epoch [7/20], Step [82/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [7/20], Step [83/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [84/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [7/20], Step [85/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [86/2541], D Loss: 0.6502, G Loss: 2.0924\n",
      "Epoch [7/20], Step [87/2541], D Loss: 0.6502, G Loss: 2.0615\n",
      "Epoch [7/20], Step [88/2541], D Loss: 0.6502, G Loss: 2.1035\n",
      "Epoch [7/20], Step [89/2541], D Loss: 0.6502, G Loss: 2.0990\n",
      "Epoch [7/20], Step [90/2541], D Loss: 0.6503, G Loss: 2.0808\n",
      "Epoch [7/20], Step [91/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [7/20], Step [92/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [7/20], Step [93/2541], D Loss: 0.6502, G Loss: 2.0959\n",
      "Epoch [7/20], Step [94/2541], D Loss: 0.6502, G Loss: 2.0656\n",
      "Epoch [7/20], Step [95/2541], D Loss: 0.6503, G Loss: 2.0812\n",
      "Epoch [7/20], Step [96/2541], D Loss: 0.6502, G Loss: 2.0960\n",
      "Epoch [7/20], Step [97/2541], D Loss: 0.6502, G Loss: 2.0951\n",
      "Epoch [7/20], Step [98/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [7/20], Step [99/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [7/20], Step [100/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [7/20], Step [101/2541], D Loss: 0.6502, G Loss: 2.1027\n",
      "Epoch [7/20], Step [102/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [7/20], Step [103/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [7/20], Step [104/2541], D Loss: 0.6502, G Loss: 2.0974\n",
      "Epoch [7/20], Step [105/2541], D Loss: 0.6502, G Loss: 2.0927\n",
      "Epoch [7/20], Step [106/2541], D Loss: 0.6502, G Loss: 2.0979\n",
      "Epoch [7/20], Step [107/2541], D Loss: 0.6502, G Loss: 2.0650\n",
      "Epoch [7/20], Step [108/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [7/20], Step [109/2541], D Loss: 0.6502, G Loss: 2.0757\n",
      "Epoch [7/20], Step [110/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [111/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [7/20], Step [112/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [113/2541], D Loss: 0.6502, G Loss: 2.0937\n",
      "Epoch [7/20], Step [114/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [7/20], Step [115/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [7/20], Step [116/2541], D Loss: 0.6502, G Loss: 2.0965\n",
      "Epoch [7/20], Step [117/2541], D Loss: 0.6502, G Loss: 2.0706\n",
      "Epoch [7/20], Step [118/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [7/20], Step [119/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [7/20], Step [120/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [7/20], Step [121/2541], D Loss: 0.6502, G Loss: 2.0451\n",
      "Epoch [7/20], Step [122/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [7/20], Step [123/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [7/20], Step [124/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [7/20], Step [125/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [7/20], Step [126/2541], D Loss: 0.6502, G Loss: 2.1047\n",
      "Epoch [7/20], Step [127/2541], D Loss: 0.6502, G Loss: 2.0689\n",
      "Epoch [7/20], Step [128/2541], D Loss: 0.6504, G Loss: 2.0739\n",
      "Epoch [7/20], Step [129/2541], D Loss: 0.6503, G Loss: 2.1049\n",
      "Epoch [7/20], Step [130/2541], D Loss: 0.6504, G Loss: 2.0771\n",
      "Epoch [7/20], Step [131/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [7/20], Step [132/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [7/20], Step [133/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [7/20], Step [134/2541], D Loss: 0.6504, G Loss: 2.0888\n",
      "Epoch [7/20], Step [135/2541], D Loss: 0.6503, G Loss: 2.0846\n",
      "Epoch [7/20], Step [136/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [7/20], Step [137/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [7/20], Step [138/2541], D Loss: 0.6503, G Loss: 2.0686\n",
      "Epoch [7/20], Step [139/2541], D Loss: 0.6502, G Loss: 2.0978\n",
      "Epoch [7/20], Step [140/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [7/20], Step [141/2541], D Loss: 0.6502, G Loss: 2.1063\n",
      "Epoch [7/20], Step [142/2541], D Loss: 0.6502, G Loss: 2.0702\n",
      "Epoch [7/20], Step [143/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [144/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [7/20], Step [145/2541], D Loss: 0.6502, G Loss: 2.0625\n",
      "Epoch [7/20], Step [146/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [7/20], Step [147/2541], D Loss: 0.6502, G Loss: 2.1031\n",
      "Epoch [7/20], Step [148/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [7/20], Step [149/2541], D Loss: 0.6502, G Loss: 2.0591\n",
      "Epoch [7/20], Step [150/2541], D Loss: 0.6502, G Loss: 2.1008\n",
      "Epoch [7/20], Step [151/2541], D Loss: 0.6503, G Loss: 2.1635\n",
      "Epoch [7/20], Step [152/2541], D Loss: 0.6504, G Loss: 2.1128\n",
      "Epoch [7/20], Step [153/2541], D Loss: 0.6505, G Loss: 2.0442\n",
      "Epoch [7/20], Step [154/2541], D Loss: 0.6504, G Loss: 2.0871\n",
      "Epoch [7/20], Step [155/2541], D Loss: 0.6503, G Loss: 2.0738\n",
      "Epoch [7/20], Step [156/2541], D Loss: 0.6505, G Loss: 2.1176\n",
      "Epoch [7/20], Step [157/2541], D Loss: 0.6503, G Loss: 2.0857\n",
      "Epoch [7/20], Step [158/2541], D Loss: 0.6502, G Loss: 2.0452\n",
      "Epoch [7/20], Step [159/2541], D Loss: 0.6503, G Loss: 2.0756\n",
      "Epoch [7/20], Step [160/2541], D Loss: 0.6503, G Loss: 2.0817\n",
      "Epoch [7/20], Step [161/2541], D Loss: 0.6504, G Loss: 2.0808\n",
      "Epoch [7/20], Step [162/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [7/20], Step [163/2541], D Loss: 0.6502, G Loss: 2.0757\n",
      "Epoch [7/20], Step [164/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [7/20], Step [165/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [7/20], Step [166/2541], D Loss: 0.6502, G Loss: 2.0910\n",
      "Epoch [7/20], Step [167/2541], D Loss: 0.6502, G Loss: 2.0944\n",
      "Epoch [7/20], Step [168/2541], D Loss: 0.6503, G Loss: 2.0620\n",
      "Epoch [7/20], Step [169/2541], D Loss: 0.6502, G Loss: 2.0932\n",
      "Epoch [7/20], Step [170/2541], D Loss: 0.6502, G Loss: 2.0941\n",
      "Epoch [7/20], Step [171/2541], D Loss: 0.6502, G Loss: 2.0545\n",
      "Epoch [7/20], Step [172/2541], D Loss: 0.6503, G Loss: 2.0982\n",
      "Epoch [7/20], Step [173/2541], D Loss: 0.6503, G Loss: 2.1024\n",
      "Epoch [7/20], Step [174/2541], D Loss: 0.6503, G Loss: 2.0811\n",
      "Epoch [7/20], Step [175/2541], D Loss: 0.6503, G Loss: 2.0728\n",
      "Epoch [7/20], Step [176/2541], D Loss: 0.6502, G Loss: 2.1188\n",
      "Epoch [7/20], Step [177/2541], D Loss: 0.6503, G Loss: 2.0774\n",
      "Epoch [7/20], Step [178/2541], D Loss: 0.6502, G Loss: 2.0594\n",
      "Epoch [7/20], Step [179/2541], D Loss: 0.6502, G Loss: 2.0816\n",
      "Epoch [7/20], Step [180/2541], D Loss: 0.6502, G Loss: 2.0665\n",
      "Epoch [7/20], Step [181/2541], D Loss: 0.6505, G Loss: 2.0817\n",
      "Epoch [7/20], Step [182/2541], D Loss: 0.6503, G Loss: 2.0799\n",
      "Epoch [7/20], Step [183/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [7/20], Step [184/2541], D Loss: 0.6504, G Loss: 2.1276\n",
      "Epoch [7/20], Step [185/2541], D Loss: 0.6502, G Loss: 2.1007\n",
      "Epoch [7/20], Step [186/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [7/20], Step [187/2541], D Loss: 0.6503, G Loss: 2.0830\n",
      "Epoch [7/20], Step [188/2541], D Loss: 0.6502, G Loss: 2.0455\n",
      "Epoch [7/20], Step [189/2541], D Loss: 0.6503, G Loss: 2.0999\n",
      "Epoch [7/20], Step [190/2541], D Loss: 0.6503, G Loss: 2.0797\n",
      "Epoch [7/20], Step [191/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [7/20], Step [192/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [7/20], Step [193/2541], D Loss: 0.6503, G Loss: 2.1015\n",
      "Epoch [7/20], Step [194/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [7/20], Step [195/2541], D Loss: 0.6502, G Loss: 2.0468\n",
      "Epoch [7/20], Step [196/2541], D Loss: 0.6503, G Loss: 2.0918\n",
      "Epoch [7/20], Step [197/2541], D Loss: 0.6504, G Loss: 2.0789\n",
      "Epoch [7/20], Step [198/2541], D Loss: 0.6503, G Loss: 2.0629\n",
      "Epoch [7/20], Step [199/2541], D Loss: 0.6503, G Loss: 2.0794\n",
      "Epoch [7/20], Step [200/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [7/20], Step [201/2541], D Loss: 0.6502, G Loss: 2.1010\n",
      "Epoch [7/20], Step [202/2541], D Loss: 0.6502, G Loss: 2.1157\n",
      "Epoch [7/20], Step [203/2541], D Loss: 0.6502, G Loss: 2.0684\n",
      "Epoch [7/20], Step [204/2541], D Loss: 0.6503, G Loss: 2.1099\n",
      "Epoch [7/20], Step [205/2541], D Loss: 0.6503, G Loss: 2.0814\n",
      "Epoch [7/20], Step [206/2541], D Loss: 0.6502, G Loss: 2.0662\n",
      "Epoch [7/20], Step [207/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [7/20], Step [208/2541], D Loss: 0.6502, G Loss: 2.0970\n",
      "Epoch [7/20], Step [209/2541], D Loss: 0.6502, G Loss: 2.0763\n",
      "Epoch [7/20], Step [210/2541], D Loss: 0.6502, G Loss: 2.1048\n",
      "Epoch [7/20], Step [211/2541], D Loss: 0.6502, G Loss: 2.0675\n",
      "Epoch [7/20], Step [212/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [7/20], Step [213/2541], D Loss: 0.6502, G Loss: 2.1075\n",
      "Epoch [7/20], Step [214/2541], D Loss: 0.6503, G Loss: 2.0779\n",
      "Epoch [7/20], Step [215/2541], D Loss: 0.6502, G Loss: 2.0699\n",
      "Epoch [7/20], Step [216/2541], D Loss: 0.6502, G Loss: 2.0936\n",
      "Epoch [7/20], Step [217/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [7/20], Step [218/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [219/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [220/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [7/20], Step [221/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [222/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [7/20], Step [223/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [7/20], Step [224/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [225/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [7/20], Step [226/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [7/20], Step [227/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [7/20], Step [228/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [7/20], Step [229/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [7/20], Step [230/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [7/20], Step [231/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [7/20], Step [232/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [7/20], Step [233/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [7/20], Step [234/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [7/20], Step [235/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [7/20], Step [236/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [7/20], Step [237/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [7/20], Step [238/2541], D Loss: 0.6502, G Loss: 2.0683\n",
      "Epoch [7/20], Step [239/2541], D Loss: 0.6502, G Loss: 2.0697\n",
      "Epoch [7/20], Step [240/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [7/20], Step [241/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [7/20], Step [242/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [7/20], Step [243/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [7/20], Step [244/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [7/20], Step [245/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [7/20], Step [246/2541], D Loss: 0.6502, G Loss: 2.0925\n",
      "Epoch [7/20], Step [247/2541], D Loss: 0.6503, G Loss: 2.0649\n",
      "Epoch [7/20], Step [248/2541], D Loss: 0.6503, G Loss: 2.0998\n",
      "Epoch [7/20], Step [249/2541], D Loss: 0.6503, G Loss: 2.0765\n",
      "Epoch [7/20], Step [250/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [7/20], Step [251/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [7/20], Step [252/2541], D Loss: 0.6502, G Loss: 2.0947\n",
      "Epoch [7/20], Step [253/2541], D Loss: 0.6502, G Loss: 2.0960\n",
      "Epoch [7/20], Step [254/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [7/20], Step [255/2541], D Loss: 0.6504, G Loss: 2.0878\n",
      "Epoch [7/20], Step [256/2541], D Loss: 0.6502, G Loss: 2.0694\n",
      "Epoch [7/20], Step [257/2541], D Loss: 0.6502, G Loss: 2.0816\n",
      "Epoch [7/20], Step [258/2541], D Loss: 0.6503, G Loss: 2.0808\n",
      "Epoch [7/20], Step [259/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [7/20], Step [260/2541], D Loss: 0.6502, G Loss: 2.0611\n",
      "Epoch [7/20], Step [261/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [7/20], Step [262/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [263/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [7/20], Step [264/2541], D Loss: 0.6502, G Loss: 2.0639\n",
      "Epoch [7/20], Step [265/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [7/20], Step [266/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [7/20], Step [267/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [7/20], Step [268/2541], D Loss: 0.6502, G Loss: 2.0636\n",
      "Epoch [7/20], Step [269/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [7/20], Step [270/2541], D Loss: 0.6502, G Loss: 2.0943\n",
      "Epoch [7/20], Step [271/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [7/20], Step [272/2541], D Loss: 0.6502, G Loss: 2.0921\n",
      "Epoch [7/20], Step [273/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [7/20], Step [274/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [7/20], Step [275/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [7/20], Step [276/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [277/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [7/20], Step [278/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [279/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [7/20], Step [280/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [7/20], Step [281/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [7/20], Step [282/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [7/20], Step [283/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [7/20], Step [284/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [7/20], Step [285/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [7/20], Step [286/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [7/20], Step [287/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [7/20], Step [288/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [7/20], Step [289/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [7/20], Step [290/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [291/2541], D Loss: 0.6502, G Loss: 2.1223\n",
      "Epoch [7/20], Step [292/2541], D Loss: 0.6503, G Loss: 2.0680\n",
      "Epoch [7/20], Step [293/2541], D Loss: 0.6502, G Loss: 2.0674\n",
      "Epoch [7/20], Step [294/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [7/20], Step [295/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [7/20], Step [296/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [297/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [7/20], Step [298/2541], D Loss: 0.6502, G Loss: 2.1005\n",
      "Epoch [7/20], Step [299/2541], D Loss: 0.6503, G Loss: 2.0666\n",
      "Epoch [7/20], Step [300/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [7/20], Step [301/2541], D Loss: 0.6502, G Loss: 2.1005\n",
      "Epoch [7/20], Step [302/2541], D Loss: 0.6503, G Loss: 2.0706\n",
      "Epoch [7/20], Step [303/2541], D Loss: 0.6502, G Loss: 2.0706\n",
      "Epoch [7/20], Step [304/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [305/2541], D Loss: 0.6502, G Loss: 2.0907\n",
      "Epoch [7/20], Step [306/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [7/20], Step [307/2541], D Loss: 0.6502, G Loss: 2.1035\n",
      "Epoch [7/20], Step [308/2541], D Loss: 0.6502, G Loss: 2.0582\n",
      "Epoch [7/20], Step [309/2541], D Loss: 0.6502, G Loss: 2.0623\n",
      "Epoch [7/20], Step [310/2541], D Loss: 0.6502, G Loss: 2.0964\n",
      "Epoch [7/20], Step [311/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [7/20], Step [312/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [7/20], Step [313/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [7/20], Step [314/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [7/20], Step [315/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [7/20], Step [316/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [7/20], Step [317/2541], D Loss: 0.6502, G Loss: 2.1003\n",
      "Epoch [7/20], Step [318/2541], D Loss: 0.6502, G Loss: 2.0698\n",
      "Epoch [7/20], Step [319/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [7/20], Step [320/2541], D Loss: 0.6502, G Loss: 2.0649\n",
      "Epoch [7/20], Step [321/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [7/20], Step [322/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [7/20], Step [323/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [324/2541], D Loss: 0.6502, G Loss: 2.0904\n",
      "Epoch [7/20], Step [325/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [7/20], Step [326/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [7/20], Step [327/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [7/20], Step [328/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [7/20], Step [329/2541], D Loss: 0.6502, G Loss: 2.1022\n",
      "Epoch [7/20], Step [330/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [7/20], Step [331/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [7/20], Step [332/2541], D Loss: 0.6502, G Loss: 2.1027\n",
      "Epoch [7/20], Step [333/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [7/20], Step [334/2541], D Loss: 0.6502, G Loss: 2.0720\n",
      "Epoch [7/20], Step [335/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [336/2541], D Loss: 0.6502, G Loss: 2.0974\n",
      "Epoch [7/20], Step [337/2541], D Loss: 0.6502, G Loss: 2.0716\n",
      "Epoch [7/20], Step [338/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [7/20], Step [339/2541], D Loss: 0.6502, G Loss: 2.1018\n",
      "Epoch [7/20], Step [340/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [7/20], Step [341/2541], D Loss: 0.6502, G Loss: 2.1050\n",
      "Epoch [7/20], Step [342/2541], D Loss: 0.6503, G Loss: 2.0709\n",
      "Epoch [7/20], Step [343/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [7/20], Step [344/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [7/20], Step [345/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [7/20], Step [346/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [7/20], Step [347/2541], D Loss: 0.6503, G Loss: 2.0889\n",
      "Epoch [7/20], Step [348/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [7/20], Step [349/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [7/20], Step [350/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [7/20], Step [351/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [7/20], Step [352/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [7/20], Step [353/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [7/20], Step [354/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [7/20], Step [355/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [7/20], Step [356/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [7/20], Step [357/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [7/20], Step [358/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [7/20], Step [359/2541], D Loss: 0.6502, G Loss: 2.0721\n",
      "Epoch [7/20], Step [360/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [7/20], Step [361/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [7/20], Step [362/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [7/20], Step [363/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [7/20], Step [364/2541], D Loss: 0.6502, G Loss: 2.0733\n",
      "Epoch [7/20], Step [365/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [7/20], Step [366/2541], D Loss: 0.6502, G Loss: 2.0816\n",
      "Epoch [7/20], Step [367/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [7/20], Step [368/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [7/20], Step [369/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [7/20], Step [370/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [7/20], Step [371/2541], D Loss: 0.6502, G Loss: 2.0981\n",
      "Epoch [7/20], Step [372/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [7/20], Step [373/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [7/20], Step [374/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [375/2541], D Loss: 0.6502, G Loss: 2.0875\n",
      "Epoch [7/20], Step [376/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [7/20], Step [377/2541], D Loss: 0.6502, G Loss: 2.0717\n",
      "Epoch [7/20], Step [378/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [7/20], Step [379/2541], D Loss: 0.6502, G Loss: 2.1004\n",
      "Epoch [7/20], Step [380/2541], D Loss: 0.6502, G Loss: 2.0697\n",
      "Epoch [7/20], Step [381/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [7/20], Step [382/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [7/20], Step [383/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [7/20], Step [384/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [7/20], Step [385/2541], D Loss: 0.6502, G Loss: 2.0750\n",
      "Epoch [7/20], Step [386/2541], D Loss: 0.6503, G Loss: 2.1033\n",
      "Epoch [7/20], Step [387/2541], D Loss: 0.6503, G Loss: 2.0687\n",
      "Epoch [7/20], Step [388/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [7/20], Step [389/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [7/20], Step [390/2541], D Loss: 0.6503, G Loss: 2.1024\n",
      "Epoch [7/20], Step [391/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [7/20], Step [392/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [393/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [7/20], Step [394/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [7/20], Step [395/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [7/20], Step [396/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [7/20], Step [397/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [7/20], Step [398/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [7/20], Step [399/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [7/20], Step [400/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [7/20], Step [401/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [7/20], Step [402/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [403/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [7/20], Step [404/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [7/20], Step [405/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [7/20], Step [406/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [7/20], Step [407/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [7/20], Step [408/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [409/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [7/20], Step [410/2541], D Loss: 0.6502, G Loss: 2.1020\n",
      "Epoch [7/20], Step [411/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [7/20], Step [412/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [413/2541], D Loss: 0.6502, G Loss: 2.1022\n",
      "Epoch [7/20], Step [414/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [415/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [7/20], Step [416/2541], D Loss: 0.6502, G Loss: 2.0536\n",
      "Epoch [7/20], Step [417/2541], D Loss: 0.6502, G Loss: 2.0623\n",
      "Epoch [7/20], Step [418/2541], D Loss: 0.6502, G Loss: 2.1043\n",
      "Epoch [7/20], Step [419/2541], D Loss: 0.6503, G Loss: 2.0854\n",
      "Epoch [7/20], Step [420/2541], D Loss: 0.6502, G Loss: 2.0711\n",
      "Epoch [7/20], Step [421/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [7/20], Step [422/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [7/20], Step [423/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [7/20], Step [424/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [425/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [426/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [427/2541], D Loss: 0.6502, G Loss: 2.0945\n",
      "Epoch [7/20], Step [428/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [7/20], Step [429/2541], D Loss: 0.6502, G Loss: 2.0740\n",
      "Epoch [7/20], Step [430/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [7/20], Step [431/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [7/20], Step [432/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [7/20], Step [433/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [434/2541], D Loss: 0.6502, G Loss: 2.0985\n",
      "Epoch [7/20], Step [435/2541], D Loss: 0.6503, G Loss: 2.0802\n",
      "Epoch [7/20], Step [436/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [7/20], Step [437/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [7/20], Step [438/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [439/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [440/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [7/20], Step [441/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [7/20], Step [442/2541], D Loss: 0.6504, G Loss: 2.0663\n",
      "Epoch [7/20], Step [443/2541], D Loss: 0.6503, G Loss: 2.1102\n",
      "Epoch [7/20], Step [444/2541], D Loss: 0.6503, G Loss: 2.0920\n",
      "Epoch [7/20], Step [445/2541], D Loss: 0.6502, G Loss: 2.0690\n",
      "Epoch [7/20], Step [446/2541], D Loss: 0.6502, G Loss: 2.0984\n",
      "Epoch [7/20], Step [447/2541], D Loss: 0.6503, G Loss: 2.0942\n",
      "Epoch [7/20], Step [448/2541], D Loss: 0.6503, G Loss: 2.0602\n",
      "Epoch [7/20], Step [449/2541], D Loss: 0.6503, G Loss: 2.0766\n",
      "Epoch [7/20], Step [450/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [7/20], Step [451/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [7/20], Step [452/2541], D Loss: 0.6502, G Loss: 2.0684\n",
      "Epoch [7/20], Step [453/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [7/20], Step [454/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [7/20], Step [455/2541], D Loss: 0.6502, G Loss: 2.1039\n",
      "Epoch [7/20], Step [456/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [7/20], Step [457/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [7/20], Step [458/2541], D Loss: 0.6502, G Loss: 2.1495\n",
      "Epoch [7/20], Step [459/2541], D Loss: 0.6504, G Loss: 2.0521\n",
      "Epoch [7/20], Step [460/2541], D Loss: 0.6504, G Loss: 2.0967\n",
      "Epoch [7/20], Step [461/2541], D Loss: 0.6503, G Loss: 2.0952\n",
      "Epoch [7/20], Step [462/2541], D Loss: 0.6504, G Loss: 2.0736\n",
      "Epoch [7/20], Step [463/2541], D Loss: 0.6505, G Loss: 2.0860\n",
      "Epoch [7/20], Step [464/2541], D Loss: 0.6504, G Loss: 2.0916\n",
      "Epoch [7/20], Step [465/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [7/20], Step [466/2541], D Loss: 0.6502, G Loss: 2.0516\n",
      "Epoch [7/20], Step [467/2541], D Loss: 0.6504, G Loss: 2.1085\n",
      "Epoch [7/20], Step [468/2541], D Loss: 0.6503, G Loss: 2.1038\n",
      "Epoch [7/20], Step [469/2541], D Loss: 0.6503, G Loss: 2.0498\n",
      "Epoch [7/20], Step [470/2541], D Loss: 0.6503, G Loss: 2.0799\n",
      "Epoch [7/20], Step [471/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [472/2541], D Loss: 0.6503, G Loss: 2.0800\n",
      "Epoch [7/20], Step [473/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [474/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [7/20], Step [475/2541], D Loss: 0.6503, G Loss: 2.0624\n",
      "Epoch [7/20], Step [476/2541], D Loss: 0.6504, G Loss: 2.0686\n",
      "Epoch [7/20], Step [477/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [7/20], Step [478/2541], D Loss: 0.6502, G Loss: 2.1035\n",
      "Epoch [7/20], Step [479/2541], D Loss: 0.6504, G Loss: 2.0629\n",
      "Epoch [7/20], Step [480/2541], D Loss: 0.6504, G Loss: 2.0896\n",
      "Epoch [7/20], Step [481/2541], D Loss: 0.6503, G Loss: 2.0875\n",
      "Epoch [7/20], Step [482/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [7/20], Step [483/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [7/20], Step [484/2541], D Loss: 0.6503, G Loss: 2.1025\n",
      "Epoch [7/20], Step [485/2541], D Loss: 0.6503, G Loss: 2.0559\n",
      "Epoch [7/20], Step [486/2541], D Loss: 0.6504, G Loss: 2.0909\n",
      "Epoch [7/20], Step [487/2541], D Loss: 0.6503, G Loss: 2.0750\n",
      "Epoch [7/20], Step [488/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [7/20], Step [489/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [7/20], Step [490/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [491/2541], D Loss: 0.6502, G Loss: 2.1006\n",
      "Epoch [7/20], Step [492/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [493/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [7/20], Step [494/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [7/20], Step [495/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [7/20], Step [496/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [497/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [7/20], Step [498/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [7/20], Step [499/2541], D Loss: 0.6502, G Loss: 2.0693\n",
      "Epoch [7/20], Step [500/2541], D Loss: 0.6502, G Loss: 2.0755\n",
      "Epoch [7/20], Step [501/2541], D Loss: 0.6502, G Loss: 2.0937\n",
      "Epoch [7/20], Step [502/2541], D Loss: 0.6502, G Loss: 2.0816\n",
      "Epoch [7/20], Step [503/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [7/20], Step [504/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [7/20], Step [505/2541], D Loss: 0.6502, G Loss: 2.0927\n",
      "Epoch [7/20], Step [506/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [7/20], Step [507/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [508/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [7/20], Step [509/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [7/20], Step [510/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [7/20], Step [511/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [512/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [7/20], Step [513/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [7/20], Step [514/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [7/20], Step [515/2541], D Loss: 0.6502, G Loss: 2.0853\n",
      "Epoch [7/20], Step [516/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [7/20], Step [517/2541], D Loss: 0.6502, G Loss: 2.0721\n",
      "Epoch [7/20], Step [518/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [7/20], Step [519/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [7/20], Step [520/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [7/20], Step [521/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [7/20], Step [522/2541], D Loss: 0.6502, G Loss: 2.0792\n",
      "Epoch [7/20], Step [523/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [7/20], Step [524/2541], D Loss: 0.6503, G Loss: 2.1123\n",
      "Epoch [7/20], Step [525/2541], D Loss: 0.6503, G Loss: 2.0759\n",
      "Epoch [7/20], Step [526/2541], D Loss: 0.6503, G Loss: 2.0689\n",
      "Epoch [7/20], Step [527/2541], D Loss: 0.6502, G Loss: 2.0945\n",
      "Epoch [7/20], Step [528/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [7/20], Step [529/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [7/20], Step [530/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [7/20], Step [531/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [7/20], Step [532/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [7/20], Step [533/2541], D Loss: 0.6504, G Loss: 2.1047\n",
      "Epoch [7/20], Step [534/2541], D Loss: 0.6503, G Loss: 2.0639\n",
      "Epoch [7/20], Step [535/2541], D Loss: 0.6503, G Loss: 2.0861\n",
      "Epoch [7/20], Step [536/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [7/20], Step [537/2541], D Loss: 0.6502, G Loss: 2.0617\n",
      "Epoch [7/20], Step [538/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [7/20], Step [539/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [540/2541], D Loss: 0.6502, G Loss: 2.0991\n",
      "Epoch [7/20], Step [541/2541], D Loss: 0.6502, G Loss: 2.0624\n",
      "Epoch [7/20], Step [542/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [7/20], Step [543/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [7/20], Step [544/2541], D Loss: 0.6502, G Loss: 2.0991\n",
      "Epoch [7/20], Step [545/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [7/20], Step [546/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [7/20], Step [547/2541], D Loss: 0.6502, G Loss: 2.0958\n",
      "Epoch [7/20], Step [548/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [7/20], Step [549/2541], D Loss: 0.6502, G Loss: 2.0636\n",
      "Epoch [7/20], Step [550/2541], D Loss: 0.6505, G Loss: 2.1037\n",
      "Epoch [7/20], Step [551/2541], D Loss: 0.6504, G Loss: 2.0755\n",
      "Epoch [7/20], Step [552/2541], D Loss: 0.6502, G Loss: 2.0730\n",
      "Epoch [7/20], Step [553/2541], D Loss: 0.6502, G Loss: 2.0876\n",
      "Epoch [7/20], Step [554/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [7/20], Step [555/2541], D Loss: 0.6503, G Loss: 2.0821\n",
      "Epoch [7/20], Step [556/2541], D Loss: 0.6504, G Loss: 2.0840\n",
      "Epoch [7/20], Step [557/2541], D Loss: 0.6503, G Loss: 2.0790\n",
      "Epoch [7/20], Step [558/2541], D Loss: 0.6503, G Loss: 2.0935\n",
      "Epoch [7/20], Step [559/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [7/20], Step [560/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [7/20], Step [561/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [7/20], Step [562/2541], D Loss: 0.6503, G Loss: 2.0703\n",
      "Epoch [7/20], Step [563/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [7/20], Step [564/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [7/20], Step [565/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [7/20], Step [566/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [7/20], Step [567/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [7/20], Step [568/2541], D Loss: 0.6502, G Loss: 2.0706\n",
      "Epoch [7/20], Step [569/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [7/20], Step [570/2541], D Loss: 0.6502, G Loss: 2.0949\n",
      "Epoch [7/20], Step [571/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [7/20], Step [572/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [573/2541], D Loss: 0.6502, G Loss: 2.0792\n",
      "Epoch [7/20], Step [574/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [7/20], Step [575/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [7/20], Step [576/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [7/20], Step [577/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [7/20], Step [578/2541], D Loss: 0.6502, G Loss: 2.0950\n",
      "Epoch [7/20], Step [579/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [7/20], Step [580/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [7/20], Step [581/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [7/20], Step [582/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [7/20], Step [583/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [584/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [585/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [7/20], Step [586/2541], D Loss: 0.6503, G Loss: 2.1126\n",
      "Epoch [7/20], Step [587/2541], D Loss: 0.6504, G Loss: 2.0811\n",
      "Epoch [7/20], Step [588/2541], D Loss: 0.6503, G Loss: 2.0698\n",
      "Epoch [7/20], Step [589/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [7/20], Step [590/2541], D Loss: 0.6503, G Loss: 2.1061\n",
      "Epoch [7/20], Step [591/2541], D Loss: 0.6503, G Loss: 2.0619\n",
      "Epoch [7/20], Step [592/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [7/20], Step [593/2541], D Loss: 0.6502, G Loss: 2.0739\n",
      "Epoch [7/20], Step [594/2541], D Loss: 0.6502, G Loss: 2.0667\n",
      "Epoch [7/20], Step [595/2541], D Loss: 0.6502, G Loss: 2.1142\n",
      "Epoch [7/20], Step [596/2541], D Loss: 0.6503, G Loss: 2.0661\n",
      "Epoch [7/20], Step [597/2541], D Loss: 0.6503, G Loss: 2.0831\n",
      "Epoch [7/20], Step [598/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [7/20], Step [599/2541], D Loss: 0.6502, G Loss: 2.0750\n",
      "Epoch [7/20], Step [600/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [7/20], Step [601/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [7/20], Step [602/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [7/20], Step [603/2541], D Loss: 0.6502, G Loss: 2.0875\n",
      "Epoch [7/20], Step [604/2541], D Loss: 0.6503, G Loss: 2.0791\n",
      "Epoch [7/20], Step [605/2541], D Loss: 0.6505, G Loss: 2.0925\n",
      "Epoch [7/20], Step [606/2541], D Loss: 0.6503, G Loss: 2.0716\n",
      "Epoch [7/20], Step [607/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [608/2541], D Loss: 0.6503, G Loss: 2.0967\n",
      "Epoch [7/20], Step [609/2541], D Loss: 0.6503, G Loss: 2.0775\n",
      "Epoch [7/20], Step [610/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [611/2541], D Loss: 0.6502, G Loss: 2.0950\n",
      "Epoch [7/20], Step [612/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [7/20], Step [613/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [7/20], Step [614/2541], D Loss: 0.6502, G Loss: 2.0811\n",
      "Epoch [7/20], Step [615/2541], D Loss: 0.6503, G Loss: 2.0754\n",
      "Epoch [7/20], Step [616/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [617/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [7/20], Step [618/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [7/20], Step [619/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [7/20], Step [620/2541], D Loss: 0.6503, G Loss: 2.0835\n",
      "Epoch [7/20], Step [621/2541], D Loss: 0.6504, G Loss: 2.0686\n",
      "Epoch [7/20], Step [622/2541], D Loss: 0.6504, G Loss: 2.0871\n",
      "Epoch [7/20], Step [623/2541], D Loss: 0.6503, G Loss: 2.0830\n",
      "Epoch [7/20], Step [624/2541], D Loss: 0.6502, G Loss: 2.0757\n",
      "Epoch [7/20], Step [625/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [7/20], Step [626/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [7/20], Step [627/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [628/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [7/20], Step [629/2541], D Loss: 0.6502, G Loss: 2.0757\n",
      "Epoch [7/20], Step [630/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [7/20], Step [631/2541], D Loss: 0.6502, G Loss: 2.0875\n",
      "Epoch [7/20], Step [632/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [7/20], Step [633/2541], D Loss: 0.6503, G Loss: 2.0873\n",
      "Epoch [7/20], Step [634/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [7/20], Step [635/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [7/20], Step [636/2541], D Loss: 0.6502, G Loss: 2.1046\n",
      "Epoch [7/20], Step [637/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [7/20], Step [638/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [7/20], Step [639/2541], D Loss: 0.6502, G Loss: 2.0898\n",
      "Epoch [7/20], Step [640/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [7/20], Step [641/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [7/20], Step [642/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [7/20], Step [643/2541], D Loss: 0.6502, G Loss: 2.0946\n",
      "Epoch [7/20], Step [644/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [645/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [7/20], Step [646/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [7/20], Step [647/2541], D Loss: 0.6502, G Loss: 2.0986\n",
      "Epoch [7/20], Step [648/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [7/20], Step [649/2541], D Loss: 0.6503, G Loss: 2.0973\n",
      "Epoch [7/20], Step [650/2541], D Loss: 0.6502, G Loss: 2.0953\n",
      "Epoch [7/20], Step [651/2541], D Loss: 0.6505, G Loss: 2.0728\n",
      "Epoch [7/20], Step [652/2541], D Loss: 0.6504, G Loss: 2.1008\n",
      "Epoch [7/20], Step [653/2541], D Loss: 0.6502, G Loss: 2.0673\n",
      "Epoch [7/20], Step [654/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [7/20], Step [655/2541], D Loss: 0.6502, G Loss: 2.1030\n",
      "Epoch [7/20], Step [656/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [7/20], Step [657/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [658/2541], D Loss: 0.6502, G Loss: 2.1081\n",
      "Epoch [7/20], Step [659/2541], D Loss: 0.6503, G Loss: 2.0709\n",
      "Epoch [7/20], Step [660/2541], D Loss: 0.6502, G Loss: 2.0576\n",
      "Epoch [7/20], Step [661/2541], D Loss: 0.6502, G Loss: 2.1053\n",
      "Epoch [7/20], Step [662/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [7/20], Step [663/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [7/20], Step [664/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [7/20], Step [665/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [7/20], Step [666/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [7/20], Step [667/2541], D Loss: 0.6503, G Loss: 2.0763\n",
      "Epoch [7/20], Step [668/2541], D Loss: 0.6505, G Loss: 2.0997\n",
      "Epoch [7/20], Step [669/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [670/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [7/20], Step [671/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [7/20], Step [672/2541], D Loss: 0.6502, G Loss: 2.0923\n",
      "Epoch [7/20], Step [673/2541], D Loss: 0.6505, G Loss: 2.0678\n",
      "Epoch [7/20], Step [674/2541], D Loss: 0.6503, G Loss: 2.0883\n",
      "Epoch [7/20], Step [675/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [7/20], Step [676/2541], D Loss: 0.6504, G Loss: 2.0783\n",
      "Epoch [7/20], Step [677/2541], D Loss: 0.6503, G Loss: 2.1040\n",
      "Epoch [7/20], Step [678/2541], D Loss: 0.6502, G Loss: 2.0750\n",
      "Epoch [7/20], Step [679/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [7/20], Step [680/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [7/20], Step [681/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [682/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [7/20], Step [683/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [684/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [7/20], Step [685/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [7/20], Step [686/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [687/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [7/20], Step [688/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [7/20], Step [689/2541], D Loss: 0.6502, G Loss: 2.0719\n",
      "Epoch [7/20], Step [690/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [7/20], Step [691/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [692/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [7/20], Step [693/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [7/20], Step [694/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [695/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [7/20], Step [696/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [697/2541], D Loss: 0.6502, G Loss: 2.0875\n",
      "Epoch [7/20], Step [698/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [7/20], Step [699/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [7/20], Step [700/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [701/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [7/20], Step [702/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [7/20], Step [703/2541], D Loss: 0.6502, G Loss: 2.0729\n",
      "Epoch [7/20], Step [704/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [7/20], Step [705/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [7/20], Step [706/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [7/20], Step [707/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [7/20], Step [708/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [7/20], Step [709/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [7/20], Step [710/2541], D Loss: 0.6502, G Loss: 2.0969\n",
      "Epoch [7/20], Step [711/2541], D Loss: 0.6502, G Loss: 2.1182\n",
      "Epoch [7/20], Step [712/2541], D Loss: 0.6503, G Loss: 2.0518\n",
      "Epoch [7/20], Step [713/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [7/20], Step [714/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [7/20], Step [715/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [7/20], Step [716/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [717/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [7/20], Step [718/2541], D Loss: 0.6502, G Loss: 2.1118\n",
      "Epoch [7/20], Step [719/2541], D Loss: 0.6502, G Loss: 2.0617\n",
      "Epoch [7/20], Step [720/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [721/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [7/20], Step [722/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [7/20], Step [723/2541], D Loss: 0.6502, G Loss: 2.0661\n",
      "Epoch [7/20], Step [724/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [7/20], Step [725/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [726/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [7/20], Step [727/2541], D Loss: 0.6502, G Loss: 2.0841\n",
      "Epoch [7/20], Step [728/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [7/20], Step [729/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [730/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [7/20], Step [731/2541], D Loss: 0.6503, G Loss: 2.0837\n",
      "Epoch [7/20], Step [732/2541], D Loss: 0.6505, G Loss: 2.1104\n",
      "Epoch [7/20], Step [733/2541], D Loss: 0.6505, G Loss: 2.0585\n",
      "Epoch [7/20], Step [734/2541], D Loss: 0.6502, G Loss: 2.0734\n",
      "Epoch [7/20], Step [735/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [7/20], Step [736/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [737/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [7/20], Step [738/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [7/20], Step [739/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [7/20], Step [740/2541], D Loss: 0.6502, G Loss: 2.0878\n",
      "Epoch [7/20], Step [741/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [7/20], Step [742/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [7/20], Step [743/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [7/20], Step [744/2541], D Loss: 0.6502, G Loss: 2.0692\n",
      "Epoch [7/20], Step [745/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [746/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [7/20], Step [747/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [7/20], Step [748/2541], D Loss: 0.6503, G Loss: 2.0693\n",
      "Epoch [7/20], Step [749/2541], D Loss: 0.6503, G Loss: 2.0947\n",
      "Epoch [7/20], Step [750/2541], D Loss: 0.6504, G Loss: 2.1068\n",
      "Epoch [7/20], Step [751/2541], D Loss: 0.6507, G Loss: 2.0539\n",
      "Epoch [7/20], Step [752/2541], D Loss: 0.6503, G Loss: 2.0810\n",
      "Epoch [7/20], Step [753/2541], D Loss: 0.6502, G Loss: 2.0991\n",
      "Epoch [7/20], Step [754/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [7/20], Step [755/2541], D Loss: 0.6502, G Loss: 2.0640\n",
      "Epoch [7/20], Step [756/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [7/20], Step [757/2541], D Loss: 0.6502, G Loss: 2.1013\n",
      "Epoch [7/20], Step [758/2541], D Loss: 0.6503, G Loss: 2.0883\n",
      "Epoch [7/20], Step [759/2541], D Loss: 0.6504, G Loss: 2.0771\n",
      "Epoch [7/20], Step [760/2541], D Loss: 0.6503, G Loss: 2.1047\n",
      "Epoch [7/20], Step [761/2541], D Loss: 0.6502, G Loss: 2.0504\n",
      "Epoch [7/20], Step [762/2541], D Loss: 0.6503, G Loss: 2.0983\n",
      "Epoch [7/20], Step [763/2541], D Loss: 0.6502, G Loss: 2.0925\n",
      "Epoch [7/20], Step [764/2541], D Loss: 0.6502, G Loss: 2.0687\n",
      "Epoch [7/20], Step [765/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [7/20], Step [766/2541], D Loss: 0.6503, G Loss: 2.0853\n",
      "Epoch [7/20], Step [767/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [7/20], Step [768/2541], D Loss: 0.6502, G Loss: 2.0636\n",
      "Epoch [7/20], Step [769/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [7/20], Step [770/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [7/20], Step [771/2541], D Loss: 0.6502, G Loss: 2.0666\n",
      "Epoch [7/20], Step [772/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [773/2541], D Loss: 0.6502, G Loss: 2.1050\n",
      "Epoch [7/20], Step [774/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [7/20], Step [775/2541], D Loss: 0.6503, G Loss: 2.0738\n",
      "Epoch [7/20], Step [776/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [7/20], Step [777/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [7/20], Step [778/2541], D Loss: 0.6503, G Loss: 2.0835\n",
      "Epoch [7/20], Step [779/2541], D Loss: 0.6505, G Loss: 2.0774\n",
      "Epoch [7/20], Step [780/2541], D Loss: 0.6503, G Loss: 2.1128\n",
      "Epoch [7/20], Step [781/2541], D Loss: 0.6504, G Loss: 2.0610\n",
      "Epoch [7/20], Step [782/2541], D Loss: 0.6510, G Loss: 2.0813\n",
      "Epoch [7/20], Step [783/2541], D Loss: 0.6517, G Loss: 2.1309\n",
      "Epoch [7/20], Step [784/2541], D Loss: 0.6506, G Loss: 2.0521\n",
      "Epoch [7/20], Step [785/2541], D Loss: 0.6509, G Loss: 2.0554\n",
      "Epoch [7/20], Step [786/2541], D Loss: 0.6508, G Loss: 2.1192\n",
      "Epoch [7/20], Step [787/2541], D Loss: 0.6505, G Loss: 2.0843\n",
      "Epoch [7/20], Step [788/2541], D Loss: 0.6503, G Loss: 2.0503\n",
      "Epoch [7/20], Step [789/2541], D Loss: 0.6505, G Loss: 2.0952\n",
      "Epoch [7/20], Step [790/2541], D Loss: 0.6505, G Loss: 2.1093\n",
      "Epoch [7/20], Step [791/2541], D Loss: 0.6503, G Loss: 2.0648\n",
      "Epoch [7/20], Step [792/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [7/20], Step [793/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [7/20], Step [794/2541], D Loss: 0.6503, G Loss: 2.0761\n",
      "Epoch [7/20], Step [795/2541], D Loss: 0.6506, G Loss: 2.0816\n",
      "Epoch [7/20], Step [796/2541], D Loss: 0.6507, G Loss: 2.1137\n",
      "Epoch [7/20], Step [797/2541], D Loss: 0.6507, G Loss: 2.0413\n",
      "Epoch [7/20], Step [798/2541], D Loss: 0.6507, G Loss: 2.0974\n",
      "Epoch [7/20], Step [799/2541], D Loss: 0.6504, G Loss: 2.1182\n",
      "Epoch [7/20], Step [800/2541], D Loss: 0.6506, G Loss: 2.0634\n",
      "Epoch [7/20], Step [801/2541], D Loss: 0.6508, G Loss: 2.1193\n",
      "Epoch [7/20], Step [802/2541], D Loss: 0.6511, G Loss: 2.0384\n",
      "Epoch [7/20], Step [803/2541], D Loss: 0.6515, G Loss: 2.1064\n",
      "Epoch [7/20], Step [804/2541], D Loss: 0.6504, G Loss: 2.0971\n",
      "Epoch [7/20], Step [805/2541], D Loss: 0.6506, G Loss: 2.0559\n",
      "Epoch [7/20], Step [806/2541], D Loss: 0.6506, G Loss: 2.1023\n",
      "Epoch [7/20], Step [807/2541], D Loss: 0.6504, G Loss: 2.0861\n",
      "Epoch [7/20], Step [808/2541], D Loss: 0.6502, G Loss: 2.0637\n",
      "Epoch [7/20], Step [809/2541], D Loss: 0.6507, G Loss: 2.1551\n",
      "Epoch [7/20], Step [810/2541], D Loss: 0.6515, G Loss: 2.0719\n",
      "Epoch [7/20], Step [811/2541], D Loss: 0.6506, G Loss: 2.0984\n",
      "Epoch [7/20], Step [812/2541], D Loss: 0.6504, G Loss: 2.0893\n",
      "Epoch [7/20], Step [813/2541], D Loss: 0.6504, G Loss: 2.0568\n",
      "Epoch [7/20], Step [814/2541], D Loss: 0.6503, G Loss: 2.0905\n",
      "Epoch [7/20], Step [815/2541], D Loss: 0.6507, G Loss: 2.1143\n",
      "Epoch [7/20], Step [816/2541], D Loss: 0.6504, G Loss: 2.0681\n",
      "Epoch [7/20], Step [817/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [7/20], Step [818/2541], D Loss: 0.6502, G Loss: 2.1016\n",
      "Epoch [7/20], Step [819/2541], D Loss: 0.6502, G Loss: 2.0627\n",
      "Epoch [7/20], Step [820/2541], D Loss: 0.6503, G Loss: 2.0815\n",
      "Epoch [7/20], Step [821/2541], D Loss: 0.6504, G Loss: 2.0878\n",
      "Epoch [7/20], Step [822/2541], D Loss: 0.6503, G Loss: 2.1254\n",
      "Epoch [7/20], Step [823/2541], D Loss: 0.6503, G Loss: 2.0884\n",
      "Epoch [7/20], Step [824/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [7/20], Step [825/2541], D Loss: 0.6503, G Loss: 2.0534\n",
      "Epoch [7/20], Step [826/2541], D Loss: 0.6503, G Loss: 2.1046\n",
      "Epoch [7/20], Step [827/2541], D Loss: 0.6503, G Loss: 2.1055\n",
      "Epoch [7/20], Step [828/2541], D Loss: 0.6502, G Loss: 2.0500\n",
      "Epoch [7/20], Step [829/2541], D Loss: 0.6503, G Loss: 2.0893\n",
      "Epoch [7/20], Step [830/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [7/20], Step [831/2541], D Loss: 0.6502, G Loss: 2.1129\n",
      "Epoch [7/20], Step [832/2541], D Loss: 0.6504, G Loss: 2.0903\n",
      "Epoch [7/20], Step [833/2541], D Loss: 0.6502, G Loss: 2.0612\n",
      "Epoch [7/20], Step [834/2541], D Loss: 0.6503, G Loss: 2.0853\n",
      "Epoch [7/20], Step [835/2541], D Loss: 0.6503, G Loss: 2.0956\n",
      "Epoch [7/20], Step [836/2541], D Loss: 0.6502, G Loss: 2.0624\n",
      "Epoch [7/20], Step [837/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [7/20], Step [838/2541], D Loss: 0.6502, G Loss: 2.0909\n",
      "Epoch [7/20], Step [839/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [7/20], Step [840/2541], D Loss: 0.6502, G Loss: 2.0674\n",
      "Epoch [7/20], Step [841/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [7/20], Step [842/2541], D Loss: 0.6502, G Loss: 2.0729\n",
      "Epoch [7/20], Step [843/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [7/20], Step [844/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [845/2541], D Loss: 0.6502, G Loss: 2.0763\n",
      "Epoch [7/20], Step [846/2541], D Loss: 0.6502, G Loss: 2.0999\n",
      "Epoch [7/20], Step [847/2541], D Loss: 0.6502, G Loss: 2.0975\n",
      "Epoch [7/20], Step [848/2541], D Loss: 0.6502, G Loss: 2.0729\n",
      "Epoch [7/20], Step [849/2541], D Loss: 0.6504, G Loss: 2.0968\n",
      "Epoch [7/20], Step [850/2541], D Loss: 0.6503, G Loss: 2.0940\n",
      "Epoch [7/20], Step [851/2541], D Loss: 0.6502, G Loss: 2.0667\n",
      "Epoch [7/20], Step [852/2541], D Loss: 0.6502, G Loss: 2.0733\n",
      "Epoch [7/20], Step [853/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [7/20], Step [854/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [7/20], Step [855/2541], D Loss: 0.6502, G Loss: 2.0731\n",
      "Epoch [7/20], Step [856/2541], D Loss: 0.6502, G Loss: 2.1143\n",
      "Epoch [7/20], Step [857/2541], D Loss: 0.6502, G Loss: 2.1045\n",
      "Epoch [7/20], Step [858/2541], D Loss: 0.6503, G Loss: 2.0691\n",
      "Epoch [7/20], Step [859/2541], D Loss: 0.6503, G Loss: 2.0603\n",
      "Epoch [7/20], Step [860/2541], D Loss: 0.6502, G Loss: 2.0979\n",
      "Epoch [7/20], Step [861/2541], D Loss: 0.6503, G Loss: 2.0793\n",
      "Epoch [7/20], Step [862/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [7/20], Step [863/2541], D Loss: 0.6502, G Loss: 2.0936\n",
      "Epoch [7/20], Step [864/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [7/20], Step [865/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [7/20], Step [866/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [7/20], Step [867/2541], D Loss: 0.6502, G Loss: 2.1032\n",
      "Epoch [7/20], Step [868/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [7/20], Step [869/2541], D Loss: 0.6502, G Loss: 2.0591\n",
      "Epoch [7/20], Step [870/2541], D Loss: 0.6502, G Loss: 2.0989\n",
      "Epoch [7/20], Step [871/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [7/20], Step [872/2541], D Loss: 0.6503, G Loss: 2.0731\n",
      "Epoch [7/20], Step [873/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [874/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [7/20], Step [875/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [7/20], Step [876/2541], D Loss: 0.6505, G Loss: 2.0857\n",
      "Epoch [7/20], Step [877/2541], D Loss: 0.6503, G Loss: 2.0909\n",
      "Epoch [7/20], Step [878/2541], D Loss: 0.6503, G Loss: 2.0741\n",
      "Epoch [7/20], Step [879/2541], D Loss: 0.6503, G Loss: 2.0841\n",
      "Epoch [7/20], Step [880/2541], D Loss: 0.6503, G Loss: 2.0828\n",
      "Epoch [7/20], Step [881/2541], D Loss: 0.6502, G Loss: 2.0651\n",
      "Epoch [7/20], Step [882/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [7/20], Step [883/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [884/2541], D Loss: 0.6505, G Loss: 2.1014\n",
      "Epoch [7/20], Step [885/2541], D Loss: 0.6503, G Loss: 2.0975\n",
      "Epoch [7/20], Step [886/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [7/20], Step [887/2541], D Loss: 0.6505, G Loss: 2.0796\n",
      "Epoch [7/20], Step [888/2541], D Loss: 0.6503, G Loss: 2.0761\n",
      "Epoch [7/20], Step [889/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [7/20], Step [890/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [7/20], Step [891/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [7/20], Step [892/2541], D Loss: 0.6502, G Loss: 2.0901\n",
      "Epoch [7/20], Step [893/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [7/20], Step [894/2541], D Loss: 0.6503, G Loss: 2.0798\n",
      "Epoch [7/20], Step [895/2541], D Loss: 0.6503, G Loss: 2.0807\n",
      "Epoch [7/20], Step [896/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [7/20], Step [897/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [7/20], Step [898/2541], D Loss: 0.6502, G Loss: 2.0666\n",
      "Epoch [7/20], Step [899/2541], D Loss: 0.6502, G Loss: 2.0875\n",
      "Epoch [7/20], Step [900/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [7/20], Step [901/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [7/20], Step [902/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [903/2541], D Loss: 0.6502, G Loss: 2.0877\n",
      "Epoch [7/20], Step [904/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [905/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [7/20], Step [906/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [7/20], Step [907/2541], D Loss: 0.6502, G Loss: 2.0936\n",
      "Epoch [7/20], Step [908/2541], D Loss: 0.6503, G Loss: 2.0896\n",
      "Epoch [7/20], Step [909/2541], D Loss: 0.6503, G Loss: 2.0759\n",
      "Epoch [7/20], Step [910/2541], D Loss: 0.6504, G Loss: 2.0861\n",
      "Epoch [7/20], Step [911/2541], D Loss: 0.6502, G Loss: 2.0923\n",
      "Epoch [7/20], Step [912/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [7/20], Step [913/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [7/20], Step [914/2541], D Loss: 0.6503, G Loss: 2.1069\n",
      "Epoch [7/20], Step [915/2541], D Loss: 0.6503, G Loss: 2.0742\n",
      "Epoch [7/20], Step [916/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [7/20], Step [917/2541], D Loss: 0.6503, G Loss: 2.0730\n",
      "Epoch [7/20], Step [918/2541], D Loss: 0.6504, G Loss: 2.1102\n",
      "Epoch [7/20], Step [919/2541], D Loss: 0.6503, G Loss: 2.0742\n",
      "Epoch [7/20], Step [920/2541], D Loss: 0.6504, G Loss: 2.0649\n",
      "Epoch [7/20], Step [921/2541], D Loss: 0.6503, G Loss: 2.1040\n",
      "Epoch [7/20], Step [922/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [923/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [7/20], Step [924/2541], D Loss: 0.6502, G Loss: 2.0921\n",
      "Epoch [7/20], Step [925/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [7/20], Step [926/2541], D Loss: 0.6502, G Loss: 2.0733\n",
      "Epoch [7/20], Step [927/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [7/20], Step [928/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [7/20], Step [929/2541], D Loss: 0.6502, G Loss: 2.0740\n",
      "Epoch [7/20], Step [930/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [7/20], Step [931/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [7/20], Step [932/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [7/20], Step [933/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [7/20], Step [934/2541], D Loss: 0.6503, G Loss: 2.0764\n",
      "Epoch [7/20], Step [935/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [7/20], Step [936/2541], D Loss: 0.6502, G Loss: 2.1015\n",
      "Epoch [7/20], Step [937/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [7/20], Step [938/2541], D Loss: 0.6502, G Loss: 2.0626\n",
      "Epoch [7/20], Step [939/2541], D Loss: 0.6502, G Loss: 2.0973\n",
      "Epoch [7/20], Step [940/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [7/20], Step [941/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [7/20], Step [942/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [7/20], Step [943/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [7/20], Step [944/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [945/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [7/20], Step [946/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [7/20], Step [947/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [7/20], Step [948/2541], D Loss: 0.6502, G Loss: 2.0956\n",
      "Epoch [7/20], Step [949/2541], D Loss: 0.6502, G Loss: 2.0701\n",
      "Epoch [7/20], Step [950/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [7/20], Step [951/2541], D Loss: 0.6503, G Loss: 2.0809\n",
      "Epoch [7/20], Step [952/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [7/20], Step [953/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [7/20], Step [954/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [955/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [7/20], Step [956/2541], D Loss: 0.6503, G Loss: 2.0876\n",
      "Epoch [7/20], Step [957/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [7/20], Step [958/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [959/2541], D Loss: 0.6502, G Loss: 2.0943\n",
      "Epoch [7/20], Step [960/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [961/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [7/20], Step [962/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [963/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [7/20], Step [964/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [7/20], Step [965/2541], D Loss: 0.6502, G Loss: 2.0626\n",
      "Epoch [7/20], Step [966/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [7/20], Step [967/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [7/20], Step [968/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [969/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [7/20], Step [970/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [7/20], Step [971/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [7/20], Step [972/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [973/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [7/20], Step [974/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [975/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [7/20], Step [976/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [7/20], Step [977/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [7/20], Step [978/2541], D Loss: 0.6502, G Loss: 2.0983\n",
      "Epoch [7/20], Step [979/2541], D Loss: 0.6503, G Loss: 2.0825\n",
      "Epoch [7/20], Step [980/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [7/20], Step [981/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [7/20], Step [982/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [7/20], Step [983/2541], D Loss: 0.6502, G Loss: 2.0771\n",
      "Epoch [7/20], Step [984/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [7/20], Step [985/2541], D Loss: 0.6502, G Loss: 2.0830\n",
      "Epoch [7/20], Step [986/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [7/20], Step [987/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [7/20], Step [988/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [7/20], Step [989/2541], D Loss: 0.6502, G Loss: 2.0923\n",
      "Epoch [7/20], Step [990/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [7/20], Step [991/2541], D Loss: 0.6502, G Loss: 2.0670\n",
      "Epoch [7/20], Step [992/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [7/20], Step [993/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [7/20], Step [994/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [995/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [7/20], Step [996/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [997/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [7/20], Step [998/2541], D Loss: 0.6502, G Loss: 2.0501\n",
      "Epoch [7/20], Step [999/2541], D Loss: 0.6502, G Loss: 2.0910\n",
      "Epoch [7/20], Step [1000/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [7/20], Step [1001/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [1002/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [7/20], Step [1003/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [1004/2541], D Loss: 0.6503, G Loss: 2.0830\n",
      "Epoch [7/20], Step [1005/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [7/20], Step [1006/2541], D Loss: 0.6502, G Loss: 2.0982\n",
      "Epoch [7/20], Step [1007/2541], D Loss: 0.6503, G Loss: 2.0648\n",
      "Epoch [7/20], Step [1008/2541], D Loss: 0.6503, G Loss: 2.0899\n",
      "Epoch [7/20], Step [1009/2541], D Loss: 0.6502, G Loss: 2.0724\n",
      "Epoch [7/20], Step [1010/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [1011/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [1012/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [7/20], Step [1013/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [7/20], Step [1014/2541], D Loss: 0.6503, G Loss: 2.0825\n",
      "Epoch [7/20], Step [1015/2541], D Loss: 0.6504, G Loss: 2.0912\n",
      "Epoch [7/20], Step [1016/2541], D Loss: 0.6503, G Loss: 2.0699\n",
      "Epoch [7/20], Step [1017/2541], D Loss: 0.6503, G Loss: 2.0938\n",
      "Epoch [7/20], Step [1018/2541], D Loss: 0.6503, G Loss: 2.0962\n",
      "Epoch [7/20], Step [1019/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [7/20], Step [1020/2541], D Loss: 0.6504, G Loss: 2.0916\n",
      "Epoch [7/20], Step [1021/2541], D Loss: 0.6503, G Loss: 2.0969\n",
      "Epoch [7/20], Step [1022/2541], D Loss: 0.6503, G Loss: 2.0676\n",
      "Epoch [7/20], Step [1023/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [7/20], Step [1024/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [7/20], Step [1025/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [7/20], Step [1026/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [7/20], Step [1027/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [7/20], Step [1028/2541], D Loss: 0.6502, G Loss: 2.0893\n",
      "Epoch [7/20], Step [1029/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [7/20], Step [1030/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [1031/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [7/20], Step [1032/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [1033/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [7/20], Step [1034/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [1035/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [7/20], Step [1036/2541], D Loss: 0.6502, G Loss: 2.0993\n",
      "Epoch [7/20], Step [1037/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [7/20], Step [1038/2541], D Loss: 0.6502, G Loss: 2.0714\n",
      "Epoch [7/20], Step [1039/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [7/20], Step [1040/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [1041/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [7/20], Step [1042/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [7/20], Step [1043/2541], D Loss: 0.6502, G Loss: 2.0938\n",
      "Epoch [7/20], Step [1044/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [1045/2541], D Loss: 0.6502, G Loss: 2.0635\n",
      "Epoch [7/20], Step [1046/2541], D Loss: 0.6502, G Loss: 2.0903\n",
      "Epoch [7/20], Step [1047/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [7/20], Step [1048/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [1049/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [7/20], Step [1050/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [7/20], Step [1051/2541], D Loss: 0.6503, G Loss: 2.1085\n",
      "Epoch [7/20], Step [1052/2541], D Loss: 0.6505, G Loss: 2.0709\n",
      "Epoch [7/20], Step [1053/2541], D Loss: 0.6503, G Loss: 2.0681\n",
      "Epoch [7/20], Step [1054/2541], D Loss: 0.6503, G Loss: 2.0944\n",
      "Epoch [7/20], Step [1055/2541], D Loss: 0.6503, G Loss: 2.0787\n",
      "Epoch [7/20], Step [1056/2541], D Loss: 0.6503, G Loss: 2.0846\n",
      "Epoch [7/20], Step [1057/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [7/20], Step [1058/2541], D Loss: 0.6502, G Loss: 2.0988\n",
      "Epoch [7/20], Step [1059/2541], D Loss: 0.6502, G Loss: 2.0693\n",
      "Epoch [7/20], Step [1060/2541], D Loss: 0.6502, G Loss: 2.0912\n",
      "Epoch [7/20], Step [1061/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [7/20], Step [1062/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [1063/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [1064/2541], D Loss: 0.6502, G Loss: 2.0950\n",
      "Epoch [7/20], Step [1065/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [7/20], Step [1066/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [7/20], Step [1067/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [1068/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [7/20], Step [1069/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [7/20], Step [1070/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [7/20], Step [1071/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [1072/2541], D Loss: 0.6502, G Loss: 2.0899\n",
      "Epoch [7/20], Step [1073/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [1074/2541], D Loss: 0.6503, G Loss: 2.0957\n",
      "Epoch [7/20], Step [1075/2541], D Loss: 0.6503, G Loss: 2.0703\n",
      "Epoch [7/20], Step [1076/2541], D Loss: 0.6503, G Loss: 2.0910\n",
      "Epoch [7/20], Step [1077/2541], D Loss: 0.6504, G Loss: 2.1070\n",
      "Epoch [7/20], Step [1078/2541], D Loss: 0.6505, G Loss: 2.0678\n",
      "Epoch [7/20], Step [1079/2541], D Loss: 0.6502, G Loss: 2.0757\n",
      "Epoch [7/20], Step [1080/2541], D Loss: 0.6502, G Loss: 2.0945\n",
      "Epoch [7/20], Step [1081/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [7/20], Step [1082/2541], D Loss: 0.6502, G Loss: 2.0680\n",
      "Epoch [7/20], Step [1083/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [1084/2541], D Loss: 0.6502, G Loss: 2.0887\n",
      "Epoch [7/20], Step [1085/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [7/20], Step [1086/2541], D Loss: 0.6503, G Loss: 2.0878\n",
      "Epoch [7/20], Step [1087/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [1088/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [7/20], Step [1089/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [1090/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [1091/2541], D Loss: 0.6502, G Loss: 2.1028\n",
      "Epoch [7/20], Step [1092/2541], D Loss: 0.6503, G Loss: 2.0622\n",
      "Epoch [7/20], Step [1093/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [7/20], Step [1094/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [7/20], Step [1095/2541], D Loss: 0.6503, G Loss: 2.0724\n",
      "Epoch [7/20], Step [1096/2541], D Loss: 0.6503, G Loss: 2.0973\n",
      "Epoch [7/20], Step [1097/2541], D Loss: 0.6502, G Loss: 2.0990\n",
      "Epoch [7/20], Step [1098/2541], D Loss: 0.6502, G Loss: 2.0833\n",
      "Epoch [7/20], Step [1099/2541], D Loss: 0.6502, G Loss: 2.0523\n",
      "Epoch [7/20], Step [1100/2541], D Loss: 0.6503, G Loss: 2.0910\n",
      "Epoch [7/20], Step [1101/2541], D Loss: 0.6503, G Loss: 2.0758\n",
      "Epoch [7/20], Step [1102/2541], D Loss: 0.6503, G Loss: 2.0808\n",
      "Epoch [7/20], Step [1103/2541], D Loss: 0.6503, G Loss: 2.0881\n",
      "Epoch [7/20], Step [1104/2541], D Loss: 0.6503, G Loss: 2.1151\n",
      "Epoch [7/20], Step [1105/2541], D Loss: 0.6503, G Loss: 2.0746\n",
      "Epoch [7/20], Step [1106/2541], D Loss: 0.6503, G Loss: 2.0835\n",
      "Epoch [7/20], Step [1107/2541], D Loss: 0.6503, G Loss: 2.0834\n",
      "Epoch [7/20], Step [1108/2541], D Loss: 0.6502, G Loss: 2.0699\n",
      "Epoch [7/20], Step [1109/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [7/20], Step [1110/2541], D Loss: 0.6503, G Loss: 2.0830\n",
      "Epoch [7/20], Step [1111/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [7/20], Step [1112/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1113/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1114/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [7/20], Step [1115/2541], D Loss: 0.6502, G Loss: 2.0811\n",
      "Epoch [7/20], Step [1116/2541], D Loss: 0.6502, G Loss: 2.0687\n",
      "Epoch [7/20], Step [1117/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [7/20], Step [1118/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [7/20], Step [1119/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [7/20], Step [1120/2541], D Loss: 0.6502, G Loss: 2.0893\n",
      "Epoch [7/20], Step [1121/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [1122/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [7/20], Step [1123/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [7/20], Step [1124/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [7/20], Step [1125/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [7/20], Step [1126/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [7/20], Step [1127/2541], D Loss: 0.6502, G Loss: 2.0519\n",
      "Epoch [7/20], Step [1128/2541], D Loss: 0.6503, G Loss: 2.0870\n",
      "Epoch [7/20], Step [1129/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [7/20], Step [1130/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [7/20], Step [1131/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [7/20], Step [1132/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [7/20], Step [1133/2541], D Loss: 0.6502, G Loss: 2.0703\n",
      "Epoch [7/20], Step [1134/2541], D Loss: 0.6502, G Loss: 2.0887\n",
      "Epoch [7/20], Step [1135/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [7/20], Step [1136/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [7/20], Step [1137/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [7/20], Step [1138/2541], D Loss: 0.6502, G Loss: 2.0792\n",
      "Epoch [7/20], Step [1139/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1140/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [7/20], Step [1141/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [7/20], Step [1142/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [7/20], Step [1143/2541], D Loss: 0.6502, G Loss: 2.0770\n",
      "Epoch [7/20], Step [1144/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [7/20], Step [1145/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [7/20], Step [1146/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [7/20], Step [1147/2541], D Loss: 0.6503, G Loss: 2.0829\n",
      "Epoch [7/20], Step [1148/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [7/20], Step [1149/2541], D Loss: 0.6502, G Loss: 2.0673\n",
      "Epoch [7/20], Step [1150/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [7/20], Step [1151/2541], D Loss: 0.6502, G Loss: 2.0965\n",
      "Epoch [7/20], Step [1152/2541], D Loss: 0.6502, G Loss: 2.0683\n",
      "Epoch [7/20], Step [1153/2541], D Loss: 0.6502, G Loss: 2.0887\n",
      "Epoch [7/20], Step [1154/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [7/20], Step [1155/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [7/20], Step [1156/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [7/20], Step [1157/2541], D Loss: 0.6502, G Loss: 2.0937\n",
      "Epoch [7/20], Step [1158/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [1159/2541], D Loss: 0.6502, G Loss: 2.0913\n",
      "Epoch [7/20], Step [1160/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [7/20], Step [1161/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [7/20], Step [1162/2541], D Loss: 0.6502, G Loss: 2.1003\n",
      "Epoch [7/20], Step [1163/2541], D Loss: 0.6503, G Loss: 2.0841\n",
      "Epoch [7/20], Step [1164/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [7/20], Step [1165/2541], D Loss: 0.6503, G Loss: 2.0685\n",
      "Epoch [7/20], Step [1166/2541], D Loss: 0.6503, G Loss: 2.0933\n",
      "Epoch [7/20], Step [1167/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [7/20], Step [1168/2541], D Loss: 0.6502, G Loss: 2.0758\n",
      "Epoch [7/20], Step [1169/2541], D Loss: 0.6502, G Loss: 2.0987\n",
      "Epoch [7/20], Step [1170/2541], D Loss: 0.6502, G Loss: 2.0623\n",
      "Epoch [7/20], Step [1171/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [7/20], Step [1172/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [7/20], Step [1173/2541], D Loss: 0.6502, G Loss: 2.0692\n",
      "Epoch [7/20], Step [1174/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [7/20], Step [1175/2541], D Loss: 0.6503, G Loss: 2.0697\n",
      "Epoch [7/20], Step [1176/2541], D Loss: 0.6502, G Loss: 2.0898\n",
      "Epoch [7/20], Step [1177/2541], D Loss: 0.6502, G Loss: 2.0724\n",
      "Epoch [7/20], Step [1178/2541], D Loss: 0.6502, G Loss: 2.0922\n",
      "Epoch [7/20], Step [1179/2541], D Loss: 0.6502, G Loss: 2.0975\n",
      "Epoch [7/20], Step [1180/2541], D Loss: 0.6502, G Loss: 2.0649\n",
      "Epoch [7/20], Step [1181/2541], D Loss: 0.6502, G Loss: 2.0862\n",
      "Epoch [7/20], Step [1182/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [7/20], Step [1183/2541], D Loss: 0.6504, G Loss: 2.0650\n",
      "Epoch [7/20], Step [1184/2541], D Loss: 0.6504, G Loss: 2.1097\n",
      "Epoch [7/20], Step [1185/2541], D Loss: 0.6502, G Loss: 2.1369\n",
      "Epoch [7/20], Step [1186/2541], D Loss: 0.6504, G Loss: 2.0512\n",
      "Epoch [7/20], Step [1187/2541], D Loss: 0.6503, G Loss: 2.0802\n",
      "Epoch [7/20], Step [1188/2541], D Loss: 0.6503, G Loss: 2.0882\n",
      "Epoch [7/20], Step [1189/2541], D Loss: 0.6503, G Loss: 2.0744\n",
      "Epoch [7/20], Step [1190/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [1191/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [1192/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [7/20], Step [1193/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [7/20], Step [1194/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [7/20], Step [1195/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [7/20], Step [1196/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [7/20], Step [1197/2541], D Loss: 0.6502, G Loss: 2.0667\n",
      "Epoch [7/20], Step [1198/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [7/20], Step [1199/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [7/20], Step [1200/2541], D Loss: 0.6502, G Loss: 2.1061\n",
      "Epoch [7/20], Step [1201/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [7/20], Step [1202/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [7/20], Step [1203/2541], D Loss: 0.6502, G Loss: 2.0692\n",
      "Epoch [7/20], Step [1204/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [7/20], Step [1205/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [7/20], Step [1206/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [7/20], Step [1207/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [7/20], Step [1208/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [1209/2541], D Loss: 0.6502, G Loss: 2.0972\n",
      "Epoch [7/20], Step [1210/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1211/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [7/20], Step [1212/2541], D Loss: 0.6502, G Loss: 2.0927\n",
      "Epoch [7/20], Step [1213/2541], D Loss: 0.6502, G Loss: 2.0441\n",
      "Epoch [7/20], Step [1214/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [1215/2541], D Loss: 0.6502, G Loss: 2.0974\n",
      "Epoch [7/20], Step [1216/2541], D Loss: 0.6502, G Loss: 2.0645\n",
      "Epoch [7/20], Step [1217/2541], D Loss: 0.6503, G Loss: 2.0954\n",
      "Epoch [7/20], Step [1218/2541], D Loss: 0.6503, G Loss: 2.0888\n",
      "Epoch [7/20], Step [1219/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [7/20], Step [1220/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [7/20], Step [1221/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [7/20], Step [1222/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [7/20], Step [1223/2541], D Loss: 0.6502, G Loss: 2.0677\n",
      "Epoch [7/20], Step [1224/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [1225/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [1226/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [7/20], Step [1227/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [7/20], Step [1228/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [1229/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [7/20], Step [1230/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [7/20], Step [1231/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [7/20], Step [1232/2541], D Loss: 0.6503, G Loss: 2.0678\n",
      "Epoch [7/20], Step [1233/2541], D Loss: 0.6502, G Loss: 2.0937\n",
      "Epoch [7/20], Step [1234/2541], D Loss: 0.6502, G Loss: 2.1099\n",
      "Epoch [7/20], Step [1235/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [7/20], Step [1236/2541], D Loss: 0.6503, G Loss: 2.0848\n",
      "Epoch [7/20], Step [1237/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [1238/2541], D Loss: 0.6502, G Loss: 2.0702\n",
      "Epoch [7/20], Step [1239/2541], D Loss: 0.6505, G Loss: 2.0832\n",
      "Epoch [7/20], Step [1240/2541], D Loss: 0.6508, G Loss: 2.0930\n",
      "Epoch [7/20], Step [1241/2541], D Loss: 0.6503, G Loss: 2.0674\n",
      "Epoch [7/20], Step [1242/2541], D Loss: 0.6503, G Loss: 2.0701\n",
      "Epoch [7/20], Step [1243/2541], D Loss: 0.6503, G Loss: 2.0867\n",
      "Epoch [7/20], Step [1244/2541], D Loss: 0.6502, G Loss: 2.0984\n",
      "Epoch [7/20], Step [1245/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [7/20], Step [1246/2541], D Loss: 0.6502, G Loss: 2.0940\n",
      "Epoch [7/20], Step [1247/2541], D Loss: 0.6503, G Loss: 2.0903\n",
      "Epoch [7/20], Step [1248/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [1249/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [1250/2541], D Loss: 0.6502, G Loss: 2.0912\n",
      "Epoch [7/20], Step [1251/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [7/20], Step [1252/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [1253/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [7/20], Step [1254/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [7/20], Step [1255/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [7/20], Step [1256/2541], D Loss: 0.6503, G Loss: 2.1267\n",
      "Epoch [7/20], Step [1257/2541], D Loss: 0.6504, G Loss: 2.0813\n",
      "Epoch [7/20], Step [1258/2541], D Loss: 0.6502, G Loss: 2.0671\n",
      "Epoch [7/20], Step [1259/2541], D Loss: 0.6503, G Loss: 2.1151\n",
      "Epoch [7/20], Step [1260/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [7/20], Step [1261/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [7/20], Step [1262/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [1263/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [1264/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [7/20], Step [1265/2541], D Loss: 0.6505, G Loss: 2.0279\n",
      "Epoch [7/20], Step [1266/2541], D Loss: 0.6508, G Loss: 2.0989\n",
      "Epoch [7/20], Step [1267/2541], D Loss: 0.6504, G Loss: 2.0811\n",
      "Epoch [7/20], Step [1268/2541], D Loss: 0.6503, G Loss: 2.0776\n",
      "Epoch [7/20], Step [1269/2541], D Loss: 0.6503, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1270/2541], D Loss: 0.6504, G Loss: 2.0832\n",
      "Epoch [7/20], Step [1271/2541], D Loss: 0.6503, G Loss: 2.0933\n",
      "Epoch [7/20], Step [1272/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [7/20], Step [1273/2541], D Loss: 0.6504, G Loss: 2.1104\n",
      "Epoch [7/20], Step [1274/2541], D Loss: 0.6504, G Loss: 2.0600\n",
      "Epoch [7/20], Step [1275/2541], D Loss: 0.6503, G Loss: 2.0996\n",
      "Epoch [7/20], Step [1276/2541], D Loss: 0.6503, G Loss: 2.0978\n",
      "Epoch [7/20], Step [1277/2541], D Loss: 0.6504, G Loss: 2.0768\n",
      "Epoch [7/20], Step [1278/2541], D Loss: 0.6502, G Loss: 2.0556\n",
      "Epoch [7/20], Step [1279/2541], D Loss: 0.6503, G Loss: 2.1171\n",
      "Epoch [7/20], Step [1280/2541], D Loss: 0.6504, G Loss: 2.0578\n",
      "Epoch [7/20], Step [1281/2541], D Loss: 0.6502, G Loss: 2.0664\n",
      "Epoch [7/20], Step [1282/2541], D Loss: 0.6502, G Loss: 2.1190\n",
      "Epoch [7/20], Step [1283/2541], D Loss: 0.6503, G Loss: 2.0747\n",
      "Epoch [7/20], Step [1284/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [1285/2541], D Loss: 0.6503, G Loss: 2.1246\n",
      "Epoch [7/20], Step [1286/2541], D Loss: 0.6505, G Loss: 2.0724\n",
      "Epoch [7/20], Step [1287/2541], D Loss: 0.6503, G Loss: 2.0651\n",
      "Epoch [7/20], Step [1288/2541], D Loss: 0.6503, G Loss: 2.1231\n",
      "Epoch [7/20], Step [1289/2541], D Loss: 0.6504, G Loss: 2.0467\n",
      "Epoch [7/20], Step [1290/2541], D Loss: 0.6503, G Loss: 2.0776\n",
      "Epoch [7/20], Step [1291/2541], D Loss: 0.6503, G Loss: 2.1183\n",
      "Epoch [7/20], Step [1292/2541], D Loss: 0.6506, G Loss: 2.0524\n",
      "Epoch [7/20], Step [1293/2541], D Loss: 0.6509, G Loss: 2.0905\n",
      "Epoch [7/20], Step [1294/2541], D Loss: 0.6504, G Loss: 2.0964\n",
      "Epoch [7/20], Step [1295/2541], D Loss: 0.6505, G Loss: 2.1316\n",
      "Epoch [7/20], Step [1296/2541], D Loss: 0.6505, G Loss: 2.0223\n",
      "Epoch [7/20], Step [1297/2541], D Loss: 0.6503, G Loss: 2.0455\n",
      "Epoch [7/20], Step [1298/2541], D Loss: 0.6508, G Loss: 2.1777\n",
      "Epoch [7/20], Step [1299/2541], D Loss: 0.6511, G Loss: 2.0306\n",
      "Epoch [7/20], Step [1300/2541], D Loss: 0.6504, G Loss: 2.0119\n",
      "Epoch [7/20], Step [1301/2541], D Loss: 0.6505, G Loss: 2.1401\n",
      "Epoch [7/20], Step [1302/2541], D Loss: 0.6504, G Loss: 2.0975\n",
      "Epoch [7/20], Step [1303/2541], D Loss: 0.6502, G Loss: 2.0503\n",
      "Epoch [7/20], Step [1304/2541], D Loss: 0.6503, G Loss: 2.0341\n",
      "Epoch [7/20], Step [1305/2541], D Loss: 0.6502, G Loss: 2.1048\n",
      "Epoch [7/20], Step [1306/2541], D Loss: 0.6503, G Loss: 2.0515\n",
      "Epoch [7/20], Step [1307/2541], D Loss: 0.6504, G Loss: 2.1000\n",
      "Epoch [7/20], Step [1308/2541], D Loss: 0.6502, G Loss: 2.1136\n",
      "Epoch [7/20], Step [1309/2541], D Loss: 0.6503, G Loss: 2.0640\n",
      "Epoch [7/20], Step [1310/2541], D Loss: 0.6503, G Loss: 2.0866\n",
      "Epoch [7/20], Step [1311/2541], D Loss: 0.6502, G Loss: 2.1062\n",
      "Epoch [7/20], Step [1312/2541], D Loss: 0.6502, G Loss: 2.0932\n",
      "Epoch [7/20], Step [1313/2541], D Loss: 0.6504, G Loss: 2.0770\n",
      "Epoch [7/20], Step [1314/2541], D Loss: 0.6503, G Loss: 2.0888\n",
      "Epoch [7/20], Step [1315/2541], D Loss: 0.6503, G Loss: 2.1357\n",
      "Epoch [7/20], Step [1316/2541], D Loss: 0.6504, G Loss: 2.0842\n",
      "Epoch [7/20], Step [1317/2541], D Loss: 0.6504, G Loss: 2.1212\n",
      "Epoch [7/20], Step [1318/2541], D Loss: 0.6503, G Loss: 2.0797\n",
      "Epoch [7/20], Step [1319/2541], D Loss: 0.6502, G Loss: 2.0662\n",
      "Epoch [7/20], Step [1320/2541], D Loss: 0.6502, G Loss: 2.1034\n",
      "Epoch [7/20], Step [1321/2541], D Loss: 0.6502, G Loss: 2.0720\n",
      "Epoch [7/20], Step [1322/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [7/20], Step [1323/2541], D Loss: 0.6502, G Loss: 2.0972\n",
      "Epoch [7/20], Step [1324/2541], D Loss: 0.6503, G Loss: 2.0903\n",
      "Epoch [7/20], Step [1325/2541], D Loss: 0.6503, G Loss: 2.0817\n",
      "Epoch [7/20], Step [1326/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [7/20], Step [1327/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [1328/2541], D Loss: 0.6503, G Loss: 2.1158\n",
      "Epoch [7/20], Step [1329/2541], D Loss: 0.6502, G Loss: 2.1073\n",
      "Epoch [7/20], Step [1330/2541], D Loss: 0.6503, G Loss: 2.0555\n",
      "Epoch [7/20], Step [1331/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [7/20], Step [1332/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [7/20], Step [1333/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [1334/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [7/20], Step [1335/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [7/20], Step [1336/2541], D Loss: 0.6503, G Loss: 2.0606\n",
      "Epoch [7/20], Step [1337/2541], D Loss: 0.6504, G Loss: 2.0823\n",
      "Epoch [7/20], Step [1338/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [7/20], Step [1339/2541], D Loss: 0.6502, G Loss: 2.0866\n",
      "Epoch [7/20], Step [1340/2541], D Loss: 0.6503, G Loss: 2.0887\n",
      "Epoch [7/20], Step [1341/2541], D Loss: 0.6502, G Loss: 2.0676\n",
      "Epoch [7/20], Step [1342/2541], D Loss: 0.6502, G Loss: 2.0923\n",
      "Epoch [7/20], Step [1343/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [7/20], Step [1344/2541], D Loss: 0.6502, G Loss: 2.0961\n",
      "Epoch [7/20], Step [1345/2541], D Loss: 0.6502, G Loss: 2.0629\n",
      "Epoch [7/20], Step [1346/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [7/20], Step [1347/2541], D Loss: 0.6503, G Loss: 2.0978\n",
      "Epoch [7/20], Step [1348/2541], D Loss: 0.6503, G Loss: 2.0707\n",
      "Epoch [7/20], Step [1349/2541], D Loss: 0.6503, G Loss: 2.0877\n",
      "Epoch [7/20], Step [1350/2541], D Loss: 0.6503, G Loss: 2.0876\n",
      "Epoch [7/20], Step [1351/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [7/20], Step [1352/2541], D Loss: 0.6502, G Loss: 2.0716\n",
      "Epoch [7/20], Step [1353/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [7/20], Step [1354/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [7/20], Step [1355/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [7/20], Step [1356/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [7/20], Step [1357/2541], D Loss: 0.6502, G Loss: 2.0677\n",
      "Epoch [7/20], Step [1358/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [7/20], Step [1359/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [7/20], Step [1360/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [7/20], Step [1361/2541], D Loss: 0.6502, G Loss: 2.0907\n",
      "Epoch [7/20], Step [1362/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [7/20], Step [1363/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [1364/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [7/20], Step [1365/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [1366/2541], D Loss: 0.6502, G Loss: 2.0886\n",
      "Epoch [7/20], Step [1367/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [7/20], Step [1368/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [7/20], Step [1369/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [7/20], Step [1370/2541], D Loss: 0.6502, G Loss: 2.1015\n",
      "Epoch [7/20], Step [1371/2541], D Loss: 0.6502, G Loss: 2.0949\n",
      "Epoch [7/20], Step [1372/2541], D Loss: 0.6502, G Loss: 2.0702\n",
      "Epoch [7/20], Step [1373/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [1374/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [7/20], Step [1375/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [7/20], Step [1376/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [7/20], Step [1377/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [7/20], Step [1378/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [7/20], Step [1379/2541], D Loss: 0.6502, G Loss: 2.0818\n",
      "Epoch [7/20], Step [1380/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [7/20], Step [1381/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [7/20], Step [1382/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [1383/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [7/20], Step [1384/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [7/20], Step [1385/2541], D Loss: 0.6502, G Loss: 2.0734\n",
      "Epoch [7/20], Step [1386/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [7/20], Step [1387/2541], D Loss: 0.6502, G Loss: 2.0614\n",
      "Epoch [7/20], Step [1388/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [7/20], Step [1389/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [7/20], Step [1390/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [7/20], Step [1391/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [1392/2541], D Loss: 0.6502, G Loss: 2.0963\n",
      "Epoch [7/20], Step [1393/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [7/20], Step [1394/2541], D Loss: 0.6503, G Loss: 2.0718\n",
      "Epoch [7/20], Step [1395/2541], D Loss: 0.6502, G Loss: 2.0968\n",
      "Epoch [7/20], Step [1396/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [7/20], Step [1397/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [7/20], Step [1398/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [7/20], Step [1399/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [7/20], Step [1400/2541], D Loss: 0.6503, G Loss: 2.0771\n",
      "Epoch [7/20], Step [1401/2541], D Loss: 0.6502, G Loss: 2.1038\n",
      "Epoch [7/20], Step [1402/2541], D Loss: 0.6502, G Loss: 2.0688\n",
      "Epoch [7/20], Step [1403/2541], D Loss: 0.6502, G Loss: 2.0766\n",
      "Epoch [7/20], Step [1404/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [7/20], Step [1405/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [1406/2541], D Loss: 0.6502, G Loss: 2.0676\n",
      "Epoch [7/20], Step [1407/2541], D Loss: 0.6502, G Loss: 2.0732\n",
      "Epoch [7/20], Step [1408/2541], D Loss: 0.6502, G Loss: 2.0943\n",
      "Epoch [7/20], Step [1409/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1410/2541], D Loss: 0.6502, G Loss: 2.1174\n",
      "Epoch [7/20], Step [1411/2541], D Loss: 0.6503, G Loss: 2.0648\n",
      "Epoch [7/20], Step [1412/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [7/20], Step [1413/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [7/20], Step [1414/2541], D Loss: 0.6503, G Loss: 2.0826\n",
      "Epoch [7/20], Step [1415/2541], D Loss: 0.6503, G Loss: 2.0870\n",
      "Epoch [7/20], Step [1416/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [7/20], Step [1417/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [7/20], Step [1418/2541], D Loss: 0.6503, G Loss: 2.0584\n",
      "Epoch [7/20], Step [1419/2541], D Loss: 0.6503, G Loss: 2.0879\n",
      "Epoch [7/20], Step [1420/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [7/20], Step [1421/2541], D Loss: 0.6502, G Loss: 2.1047\n",
      "Epoch [7/20], Step [1422/2541], D Loss: 0.6502, G Loss: 2.0387\n",
      "Epoch [7/20], Step [1423/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [1424/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [1425/2541], D Loss: 0.6505, G Loss: 2.0750\n",
      "Epoch [7/20], Step [1426/2541], D Loss: 0.6515, G Loss: 2.1238\n",
      "Epoch [7/20], Step [1427/2541], D Loss: 0.6507, G Loss: 2.0623\n",
      "Epoch [7/20], Step [1428/2541], D Loss: 0.6549, G Loss: 2.0385\n",
      "Epoch [7/20], Step [1429/2541], D Loss: 0.6523, G Loss: 2.1295\n",
      "Epoch [7/20], Step [1430/2541], D Loss: 0.6506, G Loss: 2.0840\n",
      "Epoch [7/20], Step [1431/2541], D Loss: 0.6507, G Loss: 2.0326\n",
      "Epoch [7/20], Step [1432/2541], D Loss: 0.6503, G Loss: 2.0956\n",
      "Epoch [7/20], Step [1433/2541], D Loss: 0.6503, G Loss: 2.0959\n",
      "Epoch [7/20], Step [1434/2541], D Loss: 0.6503, G Loss: 2.0665\n",
      "Epoch [7/20], Step [1435/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [7/20], Step [1436/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [7/20], Step [1437/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [7/20], Step [1438/2541], D Loss: 0.6505, G Loss: 2.0985\n",
      "Epoch [7/20], Step [1439/2541], D Loss: 0.6504, G Loss: 2.0610\n",
      "Epoch [7/20], Step [1440/2541], D Loss: 0.6503, G Loss: 2.0766\n",
      "Epoch [7/20], Step [1441/2541], D Loss: 0.6502, G Loss: 2.0959\n",
      "Epoch [7/20], Step [1442/2541], D Loss: 0.6504, G Loss: 2.1123\n",
      "Epoch [7/20], Step [1443/2541], D Loss: 0.6503, G Loss: 2.0673\n",
      "Epoch [7/20], Step [1444/2541], D Loss: 0.6503, G Loss: 2.0859\n",
      "Epoch [7/20], Step [1445/2541], D Loss: 0.6504, G Loss: 2.0940\n",
      "Epoch [7/20], Step [1446/2541], D Loss: 0.6507, G Loss: 2.0766\n",
      "Epoch [7/20], Step [1447/2541], D Loss: 0.6509, G Loss: 2.0934\n",
      "Epoch [7/20], Step [1448/2541], D Loss: 0.6518, G Loss: 2.0958\n",
      "Epoch [7/20], Step [1449/2541], D Loss: 0.6503, G Loss: 2.0676\n",
      "Epoch [7/20], Step [1450/2541], D Loss: 0.6504, G Loss: 2.0899\n",
      "Epoch [7/20], Step [1451/2541], D Loss: 0.6507, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1452/2541], D Loss: 0.6504, G Loss: 2.0757\n",
      "Epoch [7/20], Step [1453/2541], D Loss: 0.6504, G Loss: 2.0947\n",
      "Epoch [7/20], Step [1454/2541], D Loss: 0.6506, G Loss: 2.0908\n",
      "Epoch [7/20], Step [1455/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [7/20], Step [1456/2541], D Loss: 0.6503, G Loss: 2.0872\n",
      "Epoch [7/20], Step [1457/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [7/20], Step [1458/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [7/20], Step [1459/2541], D Loss: 0.6504, G Loss: 2.0926\n",
      "Epoch [7/20], Step [1460/2541], D Loss: 0.6503, G Loss: 2.0753\n",
      "Epoch [7/20], Step [1461/2541], D Loss: 0.6503, G Loss: 2.0839\n",
      "Epoch [7/20], Step [1462/2541], D Loss: 0.6503, G Loss: 2.0814\n",
      "Epoch [7/20], Step [1463/2541], D Loss: 0.6503, G Loss: 2.0887\n",
      "Epoch [7/20], Step [1464/2541], D Loss: 0.6503, G Loss: 2.0713\n",
      "Epoch [7/20], Step [1465/2541], D Loss: 0.6502, G Loss: 2.0907\n",
      "Epoch [7/20], Step [1466/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [1467/2541], D Loss: 0.6502, G Loss: 2.0532\n",
      "Epoch [7/20], Step [1468/2541], D Loss: 0.6502, G Loss: 2.1008\n",
      "Epoch [7/20], Step [1469/2541], D Loss: 0.6502, G Loss: 2.0717\n",
      "Epoch [7/20], Step [1470/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [7/20], Step [1471/2541], D Loss: 0.6502, G Loss: 2.1057\n",
      "Epoch [7/20], Step [1472/2541], D Loss: 0.6505, G Loss: 2.1009\n",
      "Epoch [7/20], Step [1473/2541], D Loss: 0.6505, G Loss: 2.0707\n",
      "Epoch [7/20], Step [1474/2541], D Loss: 0.6503, G Loss: 2.0749\n",
      "Epoch [7/20], Step [1475/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [7/20], Step [1476/2541], D Loss: 0.6503, G Loss: 2.0804\n",
      "Epoch [7/20], Step [1477/2541], D Loss: 0.6503, G Loss: 2.0925\n",
      "Epoch [7/20], Step [1478/2541], D Loss: 0.6504, G Loss: 2.0818\n",
      "Epoch [7/20], Step [1479/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [1480/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [7/20], Step [1481/2541], D Loss: 0.6505, G Loss: 2.0864\n",
      "Epoch [7/20], Step [1482/2541], D Loss: 0.6503, G Loss: 2.0738\n",
      "Epoch [7/20], Step [1483/2541], D Loss: 0.6502, G Loss: 2.0942\n",
      "Epoch [7/20], Step [1484/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [1485/2541], D Loss: 0.6502, G Loss: 2.0663\n",
      "Epoch [7/20], Step [1486/2541], D Loss: 0.6503, G Loss: 2.0985\n",
      "Epoch [7/20], Step [1487/2541], D Loss: 0.6504, G Loss: 2.0904\n",
      "Epoch [7/20], Step [1488/2541], D Loss: 0.6504, G Loss: 2.0876\n",
      "Epoch [7/20], Step [1489/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [7/20], Step [1490/2541], D Loss: 0.6502, G Loss: 2.0721\n",
      "Epoch [7/20], Step [1491/2541], D Loss: 0.6502, G Loss: 2.0992\n",
      "Epoch [7/20], Step [1492/2541], D Loss: 0.6503, G Loss: 2.0801\n",
      "Epoch [7/20], Step [1493/2541], D Loss: 0.6503, G Loss: 2.0834\n",
      "Epoch [7/20], Step [1494/2541], D Loss: 0.6502, G Loss: 2.1005\n",
      "Epoch [7/20], Step [1495/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [7/20], Step [1496/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [7/20], Step [1497/2541], D Loss: 0.6502, G Loss: 2.0944\n",
      "Epoch [7/20], Step [1498/2541], D Loss: 0.6502, G Loss: 2.0746\n",
      "Epoch [7/20], Step [1499/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [7/20], Step [1500/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [7/20], Step [1501/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [7/20], Step [1502/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [7/20], Step [1503/2541], D Loss: 0.6503, G Loss: 2.0858\n",
      "Epoch [7/20], Step [1504/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [7/20], Step [1505/2541], D Loss: 0.6515, G Loss: 2.2280\n",
      "Epoch [7/20], Step [1506/2541], D Loss: 0.6530, G Loss: 2.0014\n",
      "Epoch [7/20], Step [1507/2541], D Loss: 0.6514, G Loss: 2.0706\n",
      "Epoch [7/20], Step [1508/2541], D Loss: 0.6513, G Loss: 2.1794\n",
      "Epoch [7/20], Step [1509/2541], D Loss: 0.6520, G Loss: 1.9860\n",
      "Epoch [7/20], Step [1510/2541], D Loss: 0.6529, G Loss: 2.1371\n",
      "Epoch [7/20], Step [1511/2541], D Loss: 0.6506, G Loss: 2.0952\n",
      "Epoch [7/20], Step [1512/2541], D Loss: 0.6507, G Loss: 2.0795\n",
      "Epoch [7/20], Step [1513/2541], D Loss: 0.6505, G Loss: 2.0451\n",
      "Epoch [7/20], Step [1514/2541], D Loss: 0.6505, G Loss: 2.0252\n",
      "Epoch [7/20], Step [1515/2541], D Loss: 0.6505, G Loss: 2.0923\n",
      "Epoch [7/20], Step [1516/2541], D Loss: 0.6508, G Loss: 2.0716\n",
      "Epoch [7/20], Step [1517/2541], D Loss: 0.6505, G Loss: 2.0974\n",
      "Epoch [7/20], Step [1518/2541], D Loss: 0.6503, G Loss: 2.1152\n",
      "Epoch [7/20], Step [1519/2541], D Loss: 0.6504, G Loss: 2.0503\n",
      "Epoch [7/20], Step [1520/2541], D Loss: 0.6503, G Loss: 2.0922\n",
      "Epoch [7/20], Step [1521/2541], D Loss: 0.6503, G Loss: 2.0800\n",
      "Epoch [7/20], Step [1522/2541], D Loss: 0.6504, G Loss: 2.0899\n",
      "Epoch [7/20], Step [1523/2541], D Loss: 0.6503, G Loss: 2.1052\n",
      "Epoch [7/20], Step [1524/2541], D Loss: 0.6503, G Loss: 2.0657\n",
      "Epoch [7/20], Step [1525/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [7/20], Step [1526/2541], D Loss: 0.6503, G Loss: 2.0730\n",
      "Epoch [7/20], Step [1527/2541], D Loss: 0.6504, G Loss: 2.0781\n",
      "Epoch [7/20], Step [1528/2541], D Loss: 0.6502, G Loss: 2.1192\n",
      "Epoch [7/20], Step [1529/2541], D Loss: 0.6503, G Loss: 2.0958\n",
      "Epoch [7/20], Step [1530/2541], D Loss: 0.6507, G Loss: 2.0579\n",
      "Epoch [7/20], Step [1531/2541], D Loss: 0.6506, G Loss: 2.1186\n",
      "Epoch [7/20], Step [1532/2541], D Loss: 0.6505, G Loss: 2.0310\n",
      "Epoch [7/20], Step [1533/2541], D Loss: 0.6506, G Loss: 2.0691\n",
      "Epoch [7/20], Step [1534/2541], D Loss: 0.6506, G Loss: 2.1044\n",
      "Epoch [7/20], Step [1535/2541], D Loss: 0.6506, G Loss: 2.0776\n",
      "Epoch [7/20], Step [1536/2541], D Loss: 0.6504, G Loss: 2.1189\n",
      "Epoch [7/20], Step [1537/2541], D Loss: 0.6505, G Loss: 2.1153\n",
      "Epoch [7/20], Step [1538/2541], D Loss: 0.6506, G Loss: 2.0565\n",
      "Epoch [7/20], Step [1539/2541], D Loss: 0.6505, G Loss: 2.1246\n",
      "Epoch [7/20], Step [1540/2541], D Loss: 0.6504, G Loss: 2.0653\n",
      "Epoch [7/20], Step [1541/2541], D Loss: 0.6503, G Loss: 2.0885\n",
      "Epoch [7/20], Step [1542/2541], D Loss: 0.6503, G Loss: 2.1038\n",
      "Epoch [7/20], Step [1543/2541], D Loss: 0.6505, G Loss: 2.0515\n",
      "Epoch [7/20], Step [1544/2541], D Loss: 0.6505, G Loss: 2.0621\n",
      "Epoch [7/20], Step [1545/2541], D Loss: 0.6504, G Loss: 2.0854\n",
      "Epoch [7/20], Step [1546/2541], D Loss: 0.6504, G Loss: 2.1675\n",
      "Epoch [7/20], Step [1547/2541], D Loss: 0.6507, G Loss: 2.0473\n",
      "Epoch [7/20], Step [1548/2541], D Loss: 0.6506, G Loss: 2.0890\n",
      "Epoch [7/20], Step [1549/2541], D Loss: 0.6504, G Loss: 2.1146\n",
      "Epoch [7/20], Step [1550/2541], D Loss: 0.6507, G Loss: 2.1069\n",
      "Epoch [7/20], Step [1551/2541], D Loss: 0.6504, G Loss: 2.0879\n",
      "Epoch [7/20], Step [1552/2541], D Loss: 0.6503, G Loss: 2.0652\n",
      "Epoch [7/20], Step [1553/2541], D Loss: 0.6503, G Loss: 2.0657\n",
      "Epoch [7/20], Step [1554/2541], D Loss: 0.6502, G Loss: 2.1323\n",
      "Epoch [7/20], Step [1555/2541], D Loss: 0.6505, G Loss: 2.0831\n",
      "Epoch [7/20], Step [1556/2541], D Loss: 0.6503, G Loss: 2.0551\n",
      "Epoch [7/20], Step [1557/2541], D Loss: 0.6503, G Loss: 2.1174\n",
      "Epoch [7/20], Step [1558/2541], D Loss: 0.6505, G Loss: 2.0766\n",
      "Epoch [7/20], Step [1559/2541], D Loss: 0.6507, G Loss: 2.0752\n",
      "Epoch [7/20], Step [1560/2541], D Loss: 0.6502, G Loss: 2.0816\n",
      "Epoch [7/20], Step [1561/2541], D Loss: 0.6504, G Loss: 2.0747\n",
      "Epoch [7/20], Step [1562/2541], D Loss: 0.6502, G Loss: 2.0671\n",
      "Epoch [7/20], Step [1563/2541], D Loss: 0.6503, G Loss: 2.0882\n",
      "Epoch [7/20], Step [1564/2541], D Loss: 0.6503, G Loss: 2.0932\n",
      "Epoch [7/20], Step [1565/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [7/20], Step [1566/2541], D Loss: 0.6503, G Loss: 2.0913\n",
      "Epoch [7/20], Step [1567/2541], D Loss: 0.6504, G Loss: 2.1825\n",
      "Epoch [7/20], Step [1568/2541], D Loss: 0.6508, G Loss: 2.0421\n",
      "Epoch [7/20], Step [1569/2541], D Loss: 0.6504, G Loss: 2.0567\n",
      "Epoch [7/20], Step [1570/2541], D Loss: 0.6503, G Loss: 2.1197\n",
      "Epoch [7/20], Step [1571/2541], D Loss: 0.6504, G Loss: 2.0568\n",
      "Epoch [7/20], Step [1572/2541], D Loss: 0.6503, G Loss: 2.0851\n",
      "Epoch [7/20], Step [1573/2541], D Loss: 0.6502, G Loss: 2.1021\n",
      "Epoch [7/20], Step [1574/2541], D Loss: 0.6503, G Loss: 2.0869\n",
      "Epoch [7/20], Step [1575/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [7/20], Step [1576/2541], D Loss: 0.6502, G Loss: 2.0957\n",
      "Epoch [7/20], Step [1577/2541], D Loss: 0.6502, G Loss: 2.0507\n",
      "Epoch [7/20], Step [1578/2541], D Loss: 0.6503, G Loss: 2.1390\n",
      "Epoch [7/20], Step [1579/2541], D Loss: 0.6504, G Loss: 2.0704\n",
      "Epoch [7/20], Step [1580/2541], D Loss: 0.6502, G Loss: 2.0597\n",
      "Epoch [7/20], Step [1581/2541], D Loss: 0.6504, G Loss: 2.0720\n",
      "Epoch [7/20], Step [1582/2541], D Loss: 0.6507, G Loss: 2.1136\n",
      "Epoch [7/20], Step [1583/2541], D Loss: 0.6504, G Loss: 2.0680\n",
      "Epoch [7/20], Step [1584/2541], D Loss: 0.6504, G Loss: 2.0596\n",
      "Epoch [7/20], Step [1585/2541], D Loss: 0.6505, G Loss: 2.1065\n",
      "Epoch [7/20], Step [1586/2541], D Loss: 0.6503, G Loss: 2.1001\n",
      "Epoch [7/20], Step [1587/2541], D Loss: 0.6503, G Loss: 2.0549\n",
      "Epoch [7/20], Step [1588/2541], D Loss: 0.6503, G Loss: 2.0836\n",
      "Epoch [7/20], Step [1589/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [7/20], Step [1590/2541], D Loss: 0.6503, G Loss: 2.0794\n",
      "Epoch [7/20], Step [1591/2541], D Loss: 0.6504, G Loss: 2.1028\n",
      "Epoch [7/20], Step [1592/2541], D Loss: 0.6503, G Loss: 2.0864\n",
      "Epoch [7/20], Step [1593/2541], D Loss: 0.6504, G Loss: 2.0637\n",
      "Epoch [7/20], Step [1594/2541], D Loss: 0.6503, G Loss: 2.0713\n",
      "Epoch [7/20], Step [1595/2541], D Loss: 0.6502, G Loss: 2.0923\n",
      "Epoch [7/20], Step [1596/2541], D Loss: 0.6503, G Loss: 2.1001\n",
      "Epoch [7/20], Step [1597/2541], D Loss: 0.6504, G Loss: 2.0539\n",
      "Epoch [7/20], Step [1598/2541], D Loss: 0.6503, G Loss: 2.0984\n",
      "Epoch [7/20], Step [1599/2541], D Loss: 0.6558, G Loss: 2.1988\n",
      "Epoch [7/20], Step [1600/2541], D Loss: 0.6561, G Loss: 1.9719\n",
      "Epoch [7/20], Step [1601/2541], D Loss: 0.6669, G Loss: 2.3997\n",
      "Epoch [7/20], Step [1602/2541], D Loss: 0.6874, G Loss: 2.3573\n",
      "Epoch [7/20], Step [1603/2541], D Loss: 0.6800, G Loss: 2.2646\n",
      "Epoch [7/20], Step [1604/2541], D Loss: 0.6944, G Loss: 2.7862\n",
      "Epoch [7/20], Step [1605/2541], D Loss: 0.6987, G Loss: 2.1939\n",
      "Epoch [7/20], Step [1606/2541], D Loss: 0.6878, G Loss: 2.2475\n",
      "Epoch [7/20], Step [1607/2541], D Loss: 0.6763, G Loss: 2.1951\n",
      "Epoch [7/20], Step [1608/2541], D Loss: 0.6594, G Loss: 2.0348\n",
      "Epoch [7/20], Step [1609/2541], D Loss: 0.6529, G Loss: 2.0771\n",
      "Epoch [7/20], Step [1610/2541], D Loss: 0.6554, G Loss: 2.0901\n",
      "Epoch [7/20], Step [1611/2541], D Loss: 0.6515, G Loss: 2.1208\n",
      "Epoch [7/20], Step [1612/2541], D Loss: 0.6515, G Loss: 2.1920\n",
      "Epoch [7/20], Step [1613/2541], D Loss: 0.6509, G Loss: 2.1154\n",
      "Epoch [7/20], Step [1614/2541], D Loss: 0.6512, G Loss: 2.0220\n",
      "Epoch [7/20], Step [1615/2541], D Loss: 0.6521, G Loss: 2.0478\n",
      "Epoch [7/20], Step [1616/2541], D Loss: 0.6510, G Loss: 2.0956\n",
      "Epoch [7/20], Step [1617/2541], D Loss: 0.6506, G Loss: 2.0928\n",
      "Epoch [7/20], Step [1618/2541], D Loss: 0.6513, G Loss: 2.1447\n",
      "Epoch [7/20], Step [1619/2541], D Loss: 0.6504, G Loss: 2.1266\n",
      "Epoch [7/20], Step [1620/2541], D Loss: 0.6506, G Loss: 2.0873\n",
      "Epoch [7/20], Step [1621/2541], D Loss: 0.6503, G Loss: 2.0885\n",
      "Epoch [7/20], Step [1622/2541], D Loss: 0.6503, G Loss: 2.0692\n",
      "Epoch [7/20], Step [1623/2541], D Loss: 0.6502, G Loss: 2.1053\n",
      "Epoch [7/20], Step [1624/2541], D Loss: 0.6502, G Loss: 2.1002\n",
      "Epoch [7/20], Step [1625/2541], D Loss: 0.6502, G Loss: 2.0679\n",
      "Epoch [7/20], Step [1626/2541], D Loss: 0.6506, G Loss: 2.0626\n",
      "Epoch [7/20], Step [1627/2541], D Loss: 0.6507, G Loss: 2.0605\n",
      "Epoch [7/20], Step [1628/2541], D Loss: 0.6506, G Loss: 2.0803\n",
      "Epoch [7/20], Step [1629/2541], D Loss: 0.6505, G Loss: 2.0981\n",
      "Epoch [7/20], Step [1630/2541], D Loss: 0.6503, G Loss: 2.1024\n",
      "Epoch [7/20], Step [1631/2541], D Loss: 0.6505, G Loss: 2.1193\n",
      "Epoch [7/20], Step [1632/2541], D Loss: 0.6503, G Loss: 2.0872\n",
      "Epoch [7/20], Step [1633/2541], D Loss: 0.6503, G Loss: 2.0903\n",
      "Epoch [7/20], Step [1634/2541], D Loss: 0.6503, G Loss: 2.0879\n",
      "Epoch [7/20], Step [1635/2541], D Loss: 0.6506, G Loss: 2.0469\n",
      "Epoch [7/20], Step [1636/2541], D Loss: 0.6505, G Loss: 2.0945\n",
      "Epoch [7/20], Step [1637/2541], D Loss: 0.6503, G Loss: 2.1169\n",
      "Epoch [7/20], Step [1638/2541], D Loss: 0.6504, G Loss: 2.0559\n",
      "Epoch [7/20], Step [1639/2541], D Loss: 0.6502, G Loss: 2.0731\n",
      "Epoch [7/20], Step [1640/2541], D Loss: 0.6507, G Loss: 2.1266\n",
      "Epoch [7/20], Step [1641/2541], D Loss: 0.6504, G Loss: 2.1015\n",
      "Epoch [7/20], Step [1642/2541], D Loss: 0.6502, G Loss: 2.0678\n",
      "Epoch [7/20], Step [1643/2541], D Loss: 0.6509, G Loss: 2.0656\n",
      "Epoch [7/20], Step [1644/2541], D Loss: 0.6502, G Loss: 2.0929\n",
      "Epoch [7/20], Step [1645/2541], D Loss: 0.6503, G Loss: 2.1042\n",
      "Epoch [7/20], Step [1646/2541], D Loss: 0.6504, G Loss: 2.0979\n",
      "Epoch [7/20], Step [1647/2541], D Loss: 0.6507, G Loss: 2.0828\n",
      "Epoch [7/20], Step [1648/2541], D Loss: 0.6503, G Loss: 2.0613\n",
      "Epoch [7/20], Step [1649/2541], D Loss: 0.6503, G Loss: 2.0572\n",
      "Epoch [7/20], Step [1650/2541], D Loss: 0.6503, G Loss: 2.0535\n",
      "Epoch [7/20], Step [1651/2541], D Loss: 0.6505, G Loss: 2.0353\n",
      "Epoch [7/20], Step [1652/2541], D Loss: 0.6507, G Loss: 2.1011\n",
      "Epoch [7/20], Step [1653/2541], D Loss: 0.6503, G Loss: 2.0918\n",
      "Epoch [7/20], Step [1654/2541], D Loss: 0.6508, G Loss: 2.1056\n",
      "Epoch [7/20], Step [1655/2541], D Loss: 0.6505, G Loss: 2.0698\n",
      "Epoch [7/20], Step [1656/2541], D Loss: 0.6503, G Loss: 2.1038\n",
      "Epoch [7/20], Step [1657/2541], D Loss: 0.6503, G Loss: 2.0832\n",
      "Epoch [7/20], Step [1658/2541], D Loss: 0.6503, G Loss: 2.0415\n",
      "Epoch [7/20], Step [1659/2541], D Loss: 0.6504, G Loss: 2.1008\n",
      "Epoch [7/20], Step [1660/2541], D Loss: 0.6504, G Loss: 2.0825\n",
      "Epoch [7/20], Step [1661/2541], D Loss: 0.6503, G Loss: 2.1425\n",
      "Epoch [7/20], Step [1662/2541], D Loss: 0.6508, G Loss: 2.0468\n",
      "Epoch [7/20], Step [1663/2541], D Loss: 0.6504, G Loss: 1.9896\n",
      "Epoch [7/20], Step [1664/2541], D Loss: 0.6509, G Loss: 2.0669\n",
      "Epoch [7/20], Step [1665/2541], D Loss: 0.6506, G Loss: 2.1282\n",
      "Epoch [7/20], Step [1666/2541], D Loss: 0.6505, G Loss: 2.1065\n",
      "Epoch [7/20], Step [1667/2541], D Loss: 0.6504, G Loss: 2.0696\n",
      "Epoch [7/20], Step [1668/2541], D Loss: 0.6503, G Loss: 2.0457\n",
      "Epoch [7/20], Step [1669/2541], D Loss: 0.6509, G Loss: 2.1184\n",
      "Epoch [7/20], Step [1670/2541], D Loss: 0.6507, G Loss: 2.0802\n",
      "Epoch [7/20], Step [1671/2541], D Loss: 0.6503, G Loss: 2.0536\n",
      "Epoch [7/20], Step [1672/2541], D Loss: 0.6503, G Loss: 2.0533\n",
      "Epoch [7/20], Step [1673/2541], D Loss: 0.6504, G Loss: 2.0764\n",
      "Epoch [7/20], Step [1674/2541], D Loss: 0.6502, G Loss: 2.0651\n",
      "Epoch [7/20], Step [1675/2541], D Loss: 0.6504, G Loss: 2.0888\n",
      "Epoch [7/20], Step [1676/2541], D Loss: 0.6505, G Loss: 2.0804\n",
      "Epoch [7/20], Step [1677/2541], D Loss: 0.6507, G Loss: 2.0768\n",
      "Epoch [7/20], Step [1678/2541], D Loss: 0.6503, G Loss: 2.0655\n",
      "Epoch [7/20], Step [1679/2541], D Loss: 0.6506, G Loss: 2.0860\n",
      "Epoch [7/20], Step [1680/2541], D Loss: 0.6504, G Loss: 2.0722\n",
      "Epoch [7/20], Step [1681/2541], D Loss: 0.6503, G Loss: 2.0673\n",
      "Epoch [7/20], Step [1682/2541], D Loss: 0.6503, G Loss: 2.1096\n",
      "Epoch [7/20], Step [1683/2541], D Loss: 0.6503, G Loss: 2.0805\n",
      "Epoch [7/20], Step [1684/2541], D Loss: 0.6504, G Loss: 2.1104\n",
      "Epoch [7/20], Step [1685/2541], D Loss: 0.6503, G Loss: 2.0678\n",
      "Epoch [7/20], Step [1686/2541], D Loss: 0.6505, G Loss: 2.0700\n",
      "Epoch [7/20], Step [1687/2541], D Loss: 0.6504, G Loss: 2.0806\n",
      "Epoch [7/20], Step [1688/2541], D Loss: 0.6504, G Loss: 2.1008\n",
      "Epoch [7/20], Step [1689/2541], D Loss: 0.6503, G Loss: 2.0691\n",
      "Epoch [7/20], Step [1690/2541], D Loss: 0.6507, G Loss: 2.0816\n",
      "Epoch [7/20], Step [1691/2541], D Loss: 0.6507, G Loss: 2.0857\n",
      "Epoch [7/20], Step [1692/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [1693/2541], D Loss: 0.6504, G Loss: 2.0935\n",
      "Epoch [7/20], Step [1694/2541], D Loss: 0.6508, G Loss: 2.1080\n",
      "Epoch [7/20], Step [1695/2541], D Loss: 0.6506, G Loss: 2.0945\n",
      "Epoch [7/20], Step [1696/2541], D Loss: 0.6503, G Loss: 2.0860\n",
      "Epoch [7/20], Step [1697/2541], D Loss: 0.6503, G Loss: 2.0975\n",
      "Epoch [7/20], Step [1698/2541], D Loss: 0.6503, G Loss: 2.0795\n",
      "Epoch [7/20], Step [1699/2541], D Loss: 0.6506, G Loss: 2.0679\n",
      "Epoch [7/20], Step [1700/2541], D Loss: 0.6503, G Loss: 2.0859\n",
      "Epoch [7/20], Step [1701/2541], D Loss: 0.6506, G Loss: 2.0864\n",
      "Epoch [7/20], Step [1702/2541], D Loss: 0.6505, G Loss: 2.0635\n",
      "Epoch [7/20], Step [1703/2541], D Loss: 0.6504, G Loss: 2.0683\n",
      "Epoch [7/20], Step [1704/2541], D Loss: 0.6504, G Loss: 2.0330\n",
      "Epoch [7/20], Step [1705/2541], D Loss: 0.6503, G Loss: 2.0744\n",
      "Epoch [7/20], Step [1706/2541], D Loss: 0.6503, G Loss: 2.0695\n",
      "Epoch [7/20], Step [1707/2541], D Loss: 0.6506, G Loss: 2.0907\n",
      "Epoch [7/20], Step [1708/2541], D Loss: 0.6506, G Loss: 2.0432\n",
      "Epoch [7/20], Step [1709/2541], D Loss: 0.6503, G Loss: 2.0686\n",
      "Epoch [7/20], Step [1710/2541], D Loss: 0.6503, G Loss: 2.1088\n",
      "Epoch [7/20], Step [1711/2541], D Loss: 0.6502, G Loss: 2.1192\n",
      "Epoch [7/20], Step [1712/2541], D Loss: 0.6503, G Loss: 2.1079\n",
      "Epoch [7/20], Step [1713/2541], D Loss: 0.6503, G Loss: 2.1001\n",
      "Epoch [7/20], Step [1714/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [7/20], Step [1715/2541], D Loss: 0.6504, G Loss: 2.0908\n",
      "Epoch [7/20], Step [1716/2541], D Loss: 0.6504, G Loss: 2.0820\n",
      "Epoch [7/20], Step [1717/2541], D Loss: 0.6503, G Loss: 2.0999\n",
      "Epoch [7/20], Step [1718/2541], D Loss: 0.6502, G Loss: 2.0692\n",
      "Epoch [7/20], Step [1719/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [7/20], Step [1720/2541], D Loss: 0.6503, G Loss: 2.0685\n",
      "Epoch [7/20], Step [1721/2541], D Loss: 0.6504, G Loss: 2.0863\n",
      "Epoch [7/20], Step [1722/2541], D Loss: 0.6508, G Loss: 2.0751\n",
      "Epoch [7/20], Step [1723/2541], D Loss: 0.6504, G Loss: 2.0646\n",
      "Epoch [7/20], Step [1724/2541], D Loss: 0.6503, G Loss: 2.0719\n",
      "Epoch [7/20], Step [1725/2541], D Loss: 0.6502, G Loss: 2.0695\n",
      "Epoch [7/20], Step [1726/2541], D Loss: 0.6502, G Loss: 2.0626\n",
      "Epoch [7/20], Step [1727/2541], D Loss: 0.6504, G Loss: 2.1133\n",
      "Epoch [7/20], Step [1728/2541], D Loss: 0.6503, G Loss: 2.1081\n",
      "Epoch [7/20], Step [1729/2541], D Loss: 0.6502, G Loss: 2.0703\n",
      "Epoch [7/20], Step [1730/2541], D Loss: 0.6504, G Loss: 2.0720\n",
      "Epoch [7/20], Step [1731/2541], D Loss: 0.6511, G Loss: 2.0344\n",
      "Epoch [7/20], Step [1732/2541], D Loss: 0.6503, G Loss: 2.0621\n",
      "Epoch [7/20], Step [1733/2541], D Loss: 0.6505, G Loss: 2.0690\n",
      "Epoch [7/20], Step [1734/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [7/20], Step [1735/2541], D Loss: 0.6504, G Loss: 2.0880\n",
      "Epoch [7/20], Step [1736/2541], D Loss: 0.6507, G Loss: 2.1012\n",
      "Epoch [7/20], Step [1737/2541], D Loss: 0.6505, G Loss: 2.1009\n",
      "Epoch [7/20], Step [1738/2541], D Loss: 0.6504, G Loss: 2.1102\n",
      "Epoch [7/20], Step [1739/2541], D Loss: 0.6503, G Loss: 2.0879\n",
      "Epoch [7/20], Step [1740/2541], D Loss: 0.6503, G Loss: 2.0343\n",
      "Epoch [7/20], Step [1741/2541], D Loss: 0.6502, G Loss: 2.0501\n",
      "Epoch [7/20], Step [1742/2541], D Loss: 0.6507, G Loss: 2.0939\n",
      "Epoch [7/20], Step [1743/2541], D Loss: 0.6504, G Loss: 2.0852\n",
      "Epoch [7/20], Step [1744/2541], D Loss: 0.6503, G Loss: 2.0829\n",
      "Epoch [7/20], Step [1745/2541], D Loss: 0.6503, G Loss: 2.0984\n",
      "Epoch [7/20], Step [1746/2541], D Loss: 0.6504, G Loss: 2.0876\n",
      "Epoch [7/20], Step [1747/2541], D Loss: 0.6503, G Loss: 2.0748\n",
      "Epoch [7/20], Step [1748/2541], D Loss: 0.6504, G Loss: 2.1149\n",
      "Epoch [7/20], Step [1749/2541], D Loss: 0.6503, G Loss: 2.1433\n",
      "Epoch [7/20], Step [1750/2541], D Loss: 0.6504, G Loss: 2.1296\n",
      "Epoch [7/20], Step [1751/2541], D Loss: 0.6503, G Loss: 2.0598\n",
      "Epoch [7/20], Step [1752/2541], D Loss: 0.6504, G Loss: 2.0648\n",
      "Epoch [7/20], Step [1753/2541], D Loss: 0.6503, G Loss: 2.0848\n",
      "Epoch [7/20], Step [1754/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [7/20], Step [1755/2541], D Loss: 0.6502, G Loss: 2.0730\n",
      "Epoch [7/20], Step [1756/2541], D Loss: 0.6502, G Loss: 2.0587\n",
      "Epoch [7/20], Step [1757/2541], D Loss: 0.6505, G Loss: 2.1237\n",
      "Epoch [7/20], Step [1758/2541], D Loss: 0.6502, G Loss: 2.1196\n",
      "Epoch [7/20], Step [1759/2541], D Loss: 0.6504, G Loss: 2.0969\n",
      "Epoch [7/20], Step [1760/2541], D Loss: 0.6503, G Loss: 2.0749\n",
      "Epoch [7/20], Step [1761/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [7/20], Step [1762/2541], D Loss: 0.6503, G Loss: 2.0725\n",
      "Epoch [7/20], Step [1763/2541], D Loss: 0.6503, G Loss: 2.0684\n",
      "Epoch [7/20], Step [1764/2541], D Loss: 0.6502, G Loss: 2.1000\n",
      "Epoch [7/20], Step [1765/2541], D Loss: 0.6503, G Loss: 2.0606\n",
      "Epoch [7/20], Step [1766/2541], D Loss: 0.6502, G Loss: 2.0659\n",
      "Epoch [7/20], Step [1767/2541], D Loss: 0.6503, G Loss: 2.0745\n",
      "Epoch [7/20], Step [1768/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [7/20], Step [1769/2541], D Loss: 0.6503, G Loss: 2.0800\n",
      "Epoch [7/20], Step [1770/2541], D Loss: 0.6502, G Loss: 2.0652\n",
      "Epoch [7/20], Step [1771/2541], D Loss: 0.6503, G Loss: 2.0906\n",
      "Epoch [7/20], Step [1772/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [7/20], Step [1773/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [7/20], Step [1774/2541], D Loss: 0.6502, G Loss: 2.0623\n",
      "Epoch [7/20], Step [1775/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [1776/2541], D Loss: 0.6502, G Loss: 2.0725\n",
      "Epoch [7/20], Step [1777/2541], D Loss: 0.6502, G Loss: 2.0398\n",
      "Epoch [7/20], Step [1778/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [7/20], Step [1779/2541], D Loss: 0.6502, G Loss: 2.1042\n",
      "Epoch [7/20], Step [1780/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [7/20], Step [1781/2541], D Loss: 0.6505, G Loss: 2.0964\n",
      "Epoch [7/20], Step [1782/2541], D Loss: 0.6511, G Loss: 2.0754\n",
      "Epoch [7/20], Step [1783/2541], D Loss: 0.6503, G Loss: 2.0792\n",
      "Epoch [7/20], Step [1784/2541], D Loss: 0.6503, G Loss: 2.0697\n",
      "Epoch [7/20], Step [1785/2541], D Loss: 0.6503, G Loss: 2.0890\n",
      "Epoch [7/20], Step [1786/2541], D Loss: 0.6503, G Loss: 2.1144\n",
      "Epoch [7/20], Step [1787/2541], D Loss: 0.6505, G Loss: 2.0917\n",
      "Epoch [7/20], Step [1788/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [7/20], Step [1789/2541], D Loss: 0.6504, G Loss: 2.0582\n",
      "Epoch [7/20], Step [1790/2541], D Loss: 0.6503, G Loss: 2.0489\n",
      "Epoch [7/20], Step [1791/2541], D Loss: 0.6503, G Loss: 2.0261\n",
      "Epoch [7/20], Step [1792/2541], D Loss: 0.6504, G Loss: 2.0919\n",
      "Epoch [7/20], Step [1793/2541], D Loss: 0.6504, G Loss: 2.0835\n",
      "Epoch [7/20], Step [1794/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [7/20], Step [1795/2541], D Loss: 0.6503, G Loss: 2.1143\n",
      "Epoch [7/20], Step [1796/2541], D Loss: 0.6502, G Loss: 2.1010\n",
      "Epoch [7/20], Step [1797/2541], D Loss: 0.6503, G Loss: 2.0935\n",
      "Epoch [7/20], Step [1798/2541], D Loss: 0.6502, G Loss: 2.0684\n",
      "Epoch [7/20], Step [1799/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [7/20], Step [1800/2541], D Loss: 0.6503, G Loss: 2.1152\n",
      "Epoch [7/20], Step [1801/2541], D Loss: 0.6503, G Loss: 2.1005\n",
      "Epoch [7/20], Step [1802/2541], D Loss: 0.6505, G Loss: 2.0661\n",
      "Epoch [7/20], Step [1803/2541], D Loss: 0.6502, G Loss: 2.0414\n",
      "Epoch [7/20], Step [1804/2541], D Loss: 0.6503, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1805/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [7/20], Step [1806/2541], D Loss: 0.6502, G Loss: 2.0966\n",
      "Epoch [7/20], Step [1807/2541], D Loss: 0.6503, G Loss: 2.0261\n",
      "Epoch [7/20], Step [1808/2541], D Loss: 0.6502, G Loss: 2.0666\n",
      "Epoch [7/20], Step [1809/2541], D Loss: 0.6502, G Loss: 2.0790\n",
      "Epoch [7/20], Step [1810/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [7/20], Step [1811/2541], D Loss: 0.6502, G Loss: 2.0332\n",
      "Epoch [7/20], Step [1812/2541], D Loss: 0.6504, G Loss: 2.0792\n",
      "Epoch [7/20], Step [1813/2541], D Loss: 0.6504, G Loss: 2.1164\n",
      "Epoch [7/20], Step [1814/2541], D Loss: 0.6504, G Loss: 2.0606\n",
      "Epoch [7/20], Step [1815/2541], D Loss: 0.6502, G Loss: 2.0595\n",
      "Epoch [7/20], Step [1816/2541], D Loss: 0.6503, G Loss: 2.0774\n",
      "Epoch [7/20], Step [1817/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [7/20], Step [1818/2541], D Loss: 0.6503, G Loss: 2.0853\n",
      "Epoch [7/20], Step [1819/2541], D Loss: 0.6502, G Loss: 2.1073\n",
      "Epoch [7/20], Step [1820/2541], D Loss: 0.6503, G Loss: 2.0598\n",
      "Epoch [7/20], Step [1821/2541], D Loss: 0.6502, G Loss: 2.0712\n",
      "Epoch [7/20], Step [1822/2541], D Loss: 0.6502, G Loss: 2.0600\n",
      "Epoch [7/20], Step [1823/2541], D Loss: 0.6502, G Loss: 2.1090\n",
      "Epoch [7/20], Step [1824/2541], D Loss: 0.6502, G Loss: 2.0996\n",
      "Epoch [7/20], Step [1825/2541], D Loss: 0.6502, G Loss: 2.0975\n",
      "Epoch [7/20], Step [1826/2541], D Loss: 0.6503, G Loss: 2.0402\n",
      "Epoch [7/20], Step [1827/2541], D Loss: 0.6503, G Loss: 2.0430\n",
      "Epoch [7/20], Step [1828/2541], D Loss: 0.6502, G Loss: 2.1045\n",
      "Epoch [7/20], Step [1829/2541], D Loss: 0.6503, G Loss: 2.1132\n",
      "Epoch [7/20], Step [1830/2541], D Loss: 0.6503, G Loss: 2.0641\n",
      "Epoch [7/20], Step [1831/2541], D Loss: 0.6502, G Loss: 2.0278\n",
      "Epoch [7/20], Step [1832/2541], D Loss: 0.6503, G Loss: 2.0786\n",
      "Epoch [7/20], Step [1833/2541], D Loss: 0.6502, G Loss: 2.0898\n",
      "Epoch [7/20], Step [1834/2541], D Loss: 0.6502, G Loss: 2.0694\n",
      "Epoch [7/20], Step [1835/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [7/20], Step [1836/2541], D Loss: 0.6504, G Loss: 2.1209\n",
      "Epoch [7/20], Step [1837/2541], D Loss: 0.6503, G Loss: 2.0975\n",
      "Epoch [7/20], Step [1838/2541], D Loss: 0.6502, G Loss: 2.0846\n",
      "Epoch [7/20], Step [1839/2541], D Loss: 0.6502, G Loss: 2.0614\n",
      "Epoch [7/20], Step [1840/2541], D Loss: 0.6503, G Loss: 2.0650\n",
      "Epoch [7/20], Step [1841/2541], D Loss: 0.6502, G Loss: 2.0965\n",
      "Epoch [7/20], Step [1842/2541], D Loss: 0.6503, G Loss: 2.0627\n",
      "Epoch [7/20], Step [1843/2541], D Loss: 0.6502, G Loss: 2.0707\n",
      "Epoch [7/20], Step [1844/2541], D Loss: 0.6503, G Loss: 2.0892\n",
      "Epoch [7/20], Step [1845/2541], D Loss: 0.6502, G Loss: 2.0667\n",
      "Epoch [7/20], Step [1846/2541], D Loss: 0.6502, G Loss: 2.0819\n",
      "Epoch [7/20], Step [1847/2541], D Loss: 0.6502, G Loss: 2.0569\n",
      "Epoch [7/20], Step [1848/2541], D Loss: 0.6502, G Loss: 2.0847\n",
      "Epoch [7/20], Step [1849/2541], D Loss: 0.6504, G Loss: 2.0842\n",
      "Epoch [7/20], Step [1850/2541], D Loss: 0.6502, G Loss: 2.0595\n",
      "Epoch [7/20], Step [1851/2541], D Loss: 0.6502, G Loss: 2.0549\n",
      "Epoch [7/20], Step [1852/2541], D Loss: 0.6502, G Loss: 2.0738\n",
      "Epoch [7/20], Step [1853/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [7/20], Step [1854/2541], D Loss: 0.6503, G Loss: 2.1090\n",
      "Epoch [7/20], Step [1855/2541], D Loss: 0.6503, G Loss: 2.0585\n",
      "Epoch [7/20], Step [1856/2541], D Loss: 0.6502, G Loss: 2.0628\n",
      "Epoch [7/20], Step [1857/2541], D Loss: 0.6503, G Loss: 2.0637\n",
      "Epoch [7/20], Step [1858/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [7/20], Step [1859/2541], D Loss: 0.6503, G Loss: 2.1166\n",
      "Epoch [7/20], Step [1860/2541], D Loss: 0.6502, G Loss: 2.1034\n",
      "Epoch [7/20], Step [1861/2541], D Loss: 0.6504, G Loss: 2.1132\n",
      "Epoch [7/20], Step [1862/2541], D Loss: 0.6503, G Loss: 2.1037\n",
      "Epoch [7/20], Step [1863/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [7/20], Step [1864/2541], D Loss: 0.6502, G Loss: 2.0257\n",
      "Epoch [7/20], Step [1865/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [7/20], Step [1866/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [7/20], Step [1867/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [1868/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [7/20], Step [1869/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [7/20], Step [1870/2541], D Loss: 0.6503, G Loss: 2.0863\n",
      "Epoch [7/20], Step [1871/2541], D Loss: 0.6502, G Loss: 2.1121\n",
      "Epoch [7/20], Step [1872/2541], D Loss: 0.6503, G Loss: 2.0611\n",
      "Epoch [7/20], Step [1873/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [7/20], Step [1874/2541], D Loss: 0.6502, G Loss: 2.0664\n",
      "Epoch [7/20], Step [1875/2541], D Loss: 0.6502, G Loss: 2.0697\n",
      "Epoch [7/20], Step [1876/2541], D Loss: 0.6502, G Loss: 2.0638\n",
      "Epoch [7/20], Step [1877/2541], D Loss: 0.6502, G Loss: 2.0105\n",
      "Epoch [7/20], Step [1878/2541], D Loss: 0.6502, G Loss: 2.0580\n",
      "Epoch [7/20], Step [1879/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [7/20], Step [1880/2541], D Loss: 0.6502, G Loss: 2.0974\n",
      "Epoch [7/20], Step [1881/2541], D Loss: 0.6502, G Loss: 2.0636\n",
      "Epoch [7/20], Step [1882/2541], D Loss: 0.6502, G Loss: 2.0291\n",
      "Epoch [7/20], Step [1883/2541], D Loss: 0.6504, G Loss: 2.0908\n",
      "Epoch [7/20], Step [1884/2541], D Loss: 0.6504, G Loss: 2.0720\n",
      "Epoch [7/20], Step [1885/2541], D Loss: 0.6503, G Loss: 2.1022\n",
      "Epoch [7/20], Step [1886/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [1887/2541], D Loss: 0.6503, G Loss: 2.0608\n",
      "Epoch [7/20], Step [1888/2541], D Loss: 0.6502, G Loss: 2.0580\n",
      "Epoch [7/20], Step [1889/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [7/20], Step [1890/2541], D Loss: 0.6502, G Loss: 2.0946\n",
      "Epoch [7/20], Step [1891/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [1892/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [7/20], Step [1893/2541], D Loss: 0.6503, G Loss: 2.0961\n",
      "Epoch [7/20], Step [1894/2541], D Loss: 0.6503, G Loss: 2.0884\n",
      "Epoch [7/20], Step [1895/2541], D Loss: 0.6502, G Loss: 2.0721\n",
      "Epoch [7/20], Step [1896/2541], D Loss: 0.6503, G Loss: 2.0582\n",
      "Epoch [7/20], Step [1897/2541], D Loss: 0.6504, G Loss: 2.0860\n",
      "Epoch [7/20], Step [1898/2541], D Loss: 0.6502, G Loss: 2.0529\n",
      "Epoch [7/20], Step [1899/2541], D Loss: 0.6502, G Loss: 2.0716\n",
      "Epoch [7/20], Step [1900/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [7/20], Step [1901/2541], D Loss: 0.6502, G Loss: 2.0860\n",
      "Epoch [7/20], Step [1902/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [7/20], Step [1903/2541], D Loss: 0.6502, G Loss: 2.0678\n",
      "Epoch [7/20], Step [1904/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [7/20], Step [1905/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [1906/2541], D Loss: 0.6503, G Loss: 2.1002\n",
      "Epoch [7/20], Step [1907/2541], D Loss: 0.6502, G Loss: 2.0927\n",
      "Epoch [7/20], Step [1908/2541], D Loss: 0.6503, G Loss: 2.1007\n",
      "Epoch [7/20], Step [1909/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [7/20], Step [1910/2541], D Loss: 0.6502, G Loss: 2.0660\n",
      "Epoch [7/20], Step [1911/2541], D Loss: 0.6503, G Loss: 2.0788\n",
      "Epoch [7/20], Step [1912/2541], D Loss: 0.6504, G Loss: 2.1019\n",
      "Epoch [7/20], Step [1913/2541], D Loss: 0.6502, G Loss: 2.1080\n",
      "Epoch [7/20], Step [1914/2541], D Loss: 0.6503, G Loss: 2.0752\n",
      "Epoch [7/20], Step [1915/2541], D Loss: 0.6502, G Loss: 2.0774\n",
      "Epoch [7/20], Step [1916/2541], D Loss: 0.6503, G Loss: 2.0847\n",
      "Epoch [7/20], Step [1917/2541], D Loss: 0.6502, G Loss: 2.0518\n",
      "Epoch [7/20], Step [1918/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [7/20], Step [1919/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [1920/2541], D Loss: 0.6502, G Loss: 2.0953\n",
      "Epoch [7/20], Step [1921/2541], D Loss: 0.6504, G Loss: 2.0556\n",
      "Epoch [7/20], Step [1922/2541], D Loss: 0.6503, G Loss: 2.0623\n",
      "Epoch [7/20], Step [1923/2541], D Loss: 0.6503, G Loss: 2.0576\n",
      "Epoch [7/20], Step [1924/2541], D Loss: 0.6503, G Loss: 2.0778\n",
      "Epoch [7/20], Step [1925/2541], D Loss: 0.6502, G Loss: 2.0991\n",
      "Epoch [7/20], Step [1926/2541], D Loss: 0.6502, G Loss: 2.1043\n",
      "Epoch [7/20], Step [1927/2541], D Loss: 0.6503, G Loss: 2.0669\n",
      "Epoch [7/20], Step [1928/2541], D Loss: 0.6502, G Loss: 2.0583\n",
      "Epoch [7/20], Step [1929/2541], D Loss: 0.6502, G Loss: 2.0671\n",
      "Epoch [7/20], Step [1930/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [7/20], Step [1931/2541], D Loss: 0.6503, G Loss: 2.0783\n",
      "Epoch [7/20], Step [1932/2541], D Loss: 0.6502, G Loss: 2.1012\n",
      "Epoch [7/20], Step [1933/2541], D Loss: 0.6502, G Loss: 2.0652\n",
      "Epoch [7/20], Step [1934/2541], D Loss: 0.6502, G Loss: 2.0993\n",
      "Epoch [7/20], Step [1935/2541], D Loss: 0.6502, G Loss: 2.1019\n",
      "Epoch [7/20], Step [1936/2541], D Loss: 0.6502, G Loss: 2.1191\n",
      "Epoch [7/20], Step [1937/2541], D Loss: 0.6503, G Loss: 2.0751\n",
      "Epoch [7/20], Step [1938/2541], D Loss: 0.6502, G Loss: 2.0630\n",
      "Epoch [7/20], Step [1939/2541], D Loss: 0.6503, G Loss: 2.0699\n",
      "Epoch [7/20], Step [1940/2541], D Loss: 0.6502, G Loss: 2.1174\n",
      "Epoch [7/20], Step [1941/2541], D Loss: 0.6502, G Loss: 2.1113\n",
      "Epoch [7/20], Step [1942/2541], D Loss: 0.6503, G Loss: 2.0898\n",
      "Epoch [7/20], Step [1943/2541], D Loss: 0.6502, G Loss: 2.0696\n",
      "Epoch [7/20], Step [1944/2541], D Loss: 0.6502, G Loss: 2.0625\n",
      "Epoch [7/20], Step [1945/2541], D Loss: 0.6502, G Loss: 2.0854\n",
      "Epoch [7/20], Step [1946/2541], D Loss: 0.6502, G Loss: 2.0631\n",
      "Epoch [7/20], Step [1947/2541], D Loss: 0.6502, G Loss: 2.1033\n",
      "Epoch [7/20], Step [1948/2541], D Loss: 0.6502, G Loss: 2.0888\n",
      "Epoch [7/20], Step [1949/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [7/20], Step [1950/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [7/20], Step [1951/2541], D Loss: 0.6503, G Loss: 2.1047\n",
      "Epoch [7/20], Step [1952/2541], D Loss: 0.6503, G Loss: 2.0888\n",
      "Epoch [7/20], Step [1953/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [7/20], Step [1954/2541], D Loss: 0.6503, G Loss: 2.0958\n",
      "Epoch [7/20], Step [1955/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [1956/2541], D Loss: 0.6503, G Loss: 2.0907\n",
      "Epoch [7/20], Step [1957/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [7/20], Step [1958/2541], D Loss: 0.6502, G Loss: 2.1013\n",
      "Epoch [7/20], Step [1959/2541], D Loss: 0.6502, G Loss: 2.0964\n",
      "Epoch [7/20], Step [1960/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [7/20], Step [1961/2541], D Loss: 0.6502, G Loss: 2.0687\n",
      "Epoch [7/20], Step [1962/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [1963/2541], D Loss: 0.6502, G Loss: 2.0969\n",
      "Epoch [7/20], Step [1964/2541], D Loss: 0.6502, G Loss: 2.1028\n",
      "Epoch [7/20], Step [1965/2541], D Loss: 0.6502, G Loss: 2.0750\n",
      "Epoch [7/20], Step [1966/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [7/20], Step [1967/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [1968/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [7/20], Step [1969/2541], D Loss: 0.6502, G Loss: 2.0731\n",
      "Epoch [7/20], Step [1970/2541], D Loss: 0.6502, G Loss: 2.0892\n",
      "Epoch [7/20], Step [1971/2541], D Loss: 0.6502, G Loss: 2.0698\n",
      "Epoch [7/20], Step [1972/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [7/20], Step [1973/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [1974/2541], D Loss: 0.6502, G Loss: 2.0811\n",
      "Epoch [7/20], Step [1975/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [7/20], Step [1976/2541], D Loss: 0.6502, G Loss: 2.0716\n",
      "Epoch [7/20], Step [1977/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [1978/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [7/20], Step [1979/2541], D Loss: 0.6502, G Loss: 2.0983\n",
      "Epoch [7/20], Step [1980/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [7/20], Step [1981/2541], D Loss: 0.6504, G Loss: 2.1020\n",
      "Epoch [7/20], Step [1982/2541], D Loss: 0.6504, G Loss: 2.0878\n",
      "Epoch [7/20], Step [1983/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [7/20], Step [1984/2541], D Loss: 0.6503, G Loss: 2.0935\n",
      "Epoch [7/20], Step [1985/2541], D Loss: 0.6502, G Loss: 2.0228\n",
      "Epoch [7/20], Step [1986/2541], D Loss: 0.6502, G Loss: 2.0547\n",
      "Epoch [7/20], Step [1987/2541], D Loss: 0.6503, G Loss: 2.0547\n",
      "Epoch [7/20], Step [1988/2541], D Loss: 0.6503, G Loss: 2.0923\n",
      "Epoch [7/20], Step [1989/2541], D Loss: 0.6503, G Loss: 2.1056\n",
      "Epoch [7/20], Step [1990/2541], D Loss: 0.6503, G Loss: 2.0833\n",
      "Epoch [7/20], Step [1991/2541], D Loss: 0.6502, G Loss: 2.0547\n",
      "Epoch [7/20], Step [1992/2541], D Loss: 0.6502, G Loss: 2.0645\n",
      "Epoch [7/20], Step [1993/2541], D Loss: 0.6502, G Loss: 2.0952\n",
      "Epoch [7/20], Step [1994/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [7/20], Step [1995/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [7/20], Step [1996/2541], D Loss: 0.6504, G Loss: 2.1158\n",
      "Epoch [7/20], Step [1997/2541], D Loss: 0.6507, G Loss: 2.1000\n",
      "Epoch [7/20], Step [1998/2541], D Loss: 0.6506, G Loss: 2.0502\n",
      "Epoch [7/20], Step [1999/2541], D Loss: 0.6504, G Loss: 2.0747\n",
      "Epoch [7/20], Step [2000/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [7/20], Step [2001/2541], D Loss: 0.6503, G Loss: 2.1101\n",
      "Epoch [7/20], Step [2002/2541], D Loss: 0.6504, G Loss: 2.0728\n",
      "Epoch [7/20], Step [2003/2541], D Loss: 0.6502, G Loss: 2.0656\n",
      "Epoch [7/20], Step [2004/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [2005/2541], D Loss: 0.6503, G Loss: 2.0694\n",
      "Epoch [7/20], Step [2006/2541], D Loss: 0.6502, G Loss: 2.0880\n",
      "Epoch [7/20], Step [2007/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [7/20], Step [2008/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [2009/2541], D Loss: 0.6504, G Loss: 2.0790\n",
      "Epoch [7/20], Step [2010/2541], D Loss: 0.6503, G Loss: 2.0852\n",
      "Epoch [7/20], Step [2011/2541], D Loss: 0.6502, G Loss: 2.0773\n",
      "Epoch [7/20], Step [2012/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [7/20], Step [2013/2541], D Loss: 0.6503, G Loss: 2.0914\n",
      "Epoch [7/20], Step [2014/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [2015/2541], D Loss: 0.6503, G Loss: 2.0797\n",
      "Epoch [7/20], Step [2016/2541], D Loss: 0.6502, G Loss: 2.0666\n",
      "Epoch [7/20], Step [2017/2541], D Loss: 0.6502, G Loss: 2.0496\n",
      "Epoch [7/20], Step [2018/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [7/20], Step [2019/2541], D Loss: 0.6502, G Loss: 2.0976\n",
      "Epoch [7/20], Step [2020/2541], D Loss: 0.6502, G Loss: 2.1012\n",
      "Epoch [7/20], Step [2021/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [2022/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [7/20], Step [2023/2541], D Loss: 0.6502, G Loss: 2.0742\n",
      "Epoch [7/20], Step [2024/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [7/20], Step [2025/2541], D Loss: 0.6502, G Loss: 2.1160\n",
      "Epoch [7/20], Step [2026/2541], D Loss: 0.6502, G Loss: 2.1052\n",
      "Epoch [7/20], Step [2027/2541], D Loss: 0.6502, G Loss: 2.0752\n",
      "Epoch [7/20], Step [2028/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [7/20], Step [2029/2541], D Loss: 0.6502, G Loss: 2.0688\n",
      "Epoch [7/20], Step [2030/2541], D Loss: 0.6502, G Loss: 2.0998\n",
      "Epoch [7/20], Step [2031/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [7/20], Step [2032/2541], D Loss: 0.6503, G Loss: 2.0543\n",
      "Epoch [7/20], Step [2033/2541], D Loss: 0.6503, G Loss: 2.0644\n",
      "Epoch [7/20], Step [2034/2541], D Loss: 0.6502, G Loss: 2.0923\n",
      "Epoch [7/20], Step [2035/2541], D Loss: 0.6502, G Loss: 2.1055\n",
      "Epoch [7/20], Step [2036/2541], D Loss: 0.6502, G Loss: 2.0708\n",
      "Epoch [7/20], Step [2037/2541], D Loss: 0.6503, G Loss: 2.0728\n",
      "Epoch [7/20], Step [2038/2541], D Loss: 0.6502, G Loss: 2.0704\n",
      "Epoch [7/20], Step [2039/2541], D Loss: 0.6503, G Loss: 2.0852\n",
      "Epoch [7/20], Step [2040/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [2041/2541], D Loss: 0.6502, G Loss: 2.0754\n",
      "Epoch [7/20], Step [2042/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [7/20], Step [2043/2541], D Loss: 0.6502, G Loss: 2.0981\n",
      "Epoch [7/20], Step [2044/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [7/20], Step [2045/2541], D Loss: 0.6505, G Loss: 2.1171\n",
      "Epoch [7/20], Step [2046/2541], D Loss: 0.6502, G Loss: 2.1121\n",
      "Epoch [7/20], Step [2047/2541], D Loss: 0.6502, G Loss: 2.0882\n",
      "Epoch [7/20], Step [2048/2541], D Loss: 0.6503, G Loss: 2.0798\n",
      "Epoch [7/20], Step [2049/2541], D Loss: 0.6506, G Loss: 2.0625\n",
      "Epoch [7/20], Step [2050/2541], D Loss: 0.6508, G Loss: 2.0999\n",
      "Epoch [7/20], Step [2051/2541], D Loss: 0.6502, G Loss: 2.0939\n",
      "Epoch [7/20], Step [2052/2541], D Loss: 0.6503, G Loss: 2.0895\n",
      "Epoch [7/20], Step [2053/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [2054/2541], D Loss: 0.6520, G Loss: 2.1269\n",
      "Epoch [7/20], Step [2055/2541], D Loss: 0.6547, G Loss: 2.0681\n",
      "Epoch [7/20], Step [2056/2541], D Loss: 0.6508, G Loss: 2.0662\n",
      "Epoch [7/20], Step [2057/2541], D Loss: 0.6510, G Loss: 2.1165\n",
      "Epoch [7/20], Step [2058/2541], D Loss: 0.6504, G Loss: 2.0925\n",
      "Epoch [7/20], Step [2059/2541], D Loss: 0.6504, G Loss: 2.0585\n",
      "Epoch [7/20], Step [2060/2541], D Loss: 0.6503, G Loss: 2.0712\n",
      "Epoch [7/20], Step [2061/2541], D Loss: 0.6502, G Loss: 2.0964\n",
      "Epoch [7/20], Step [2062/2541], D Loss: 0.6502, G Loss: 2.1036\n",
      "Epoch [7/20], Step [2063/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [7/20], Step [2064/2541], D Loss: 0.6502, G Loss: 2.0702\n",
      "Epoch [7/20], Step [2065/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [7/20], Step [2066/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [7/20], Step [2067/2541], D Loss: 0.6502, G Loss: 2.0937\n",
      "Epoch [7/20], Step [2068/2541], D Loss: 0.6502, G Loss: 2.0852\n",
      "Epoch [7/20], Step [2069/2541], D Loss: 0.6503, G Loss: 2.0688\n",
      "Epoch [7/20], Step [2070/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [7/20], Step [2071/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [7/20], Step [2072/2541], D Loss: 0.6502, G Loss: 2.0577\n",
      "Epoch [7/20], Step [2073/2541], D Loss: 0.6502, G Loss: 2.0699\n",
      "Epoch [7/20], Step [2074/2541], D Loss: 0.6502, G Loss: 2.0696\n",
      "Epoch [7/20], Step [2075/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [7/20], Step [2076/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [7/20], Step [2077/2541], D Loss: 0.6502, G Loss: 2.0658\n",
      "Epoch [7/20], Step [2078/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [2079/2541], D Loss: 0.6503, G Loss: 2.0847\n",
      "Epoch [7/20], Step [2080/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [7/20], Step [2081/2541], D Loss: 0.6504, G Loss: 2.0659\n",
      "Epoch [7/20], Step [2082/2541], D Loss: 0.6503, G Loss: 2.0834\n",
      "Epoch [7/20], Step [2083/2541], D Loss: 0.6502, G Loss: 2.0511\n",
      "Epoch [7/20], Step [2084/2541], D Loss: 0.6502, G Loss: 2.0718\n",
      "Epoch [7/20], Step [2085/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [2086/2541], D Loss: 0.6503, G Loss: 2.0747\n",
      "Epoch [7/20], Step [2087/2541], D Loss: 0.6503, G Loss: 2.0728\n",
      "Epoch [7/20], Step [2088/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [2089/2541], D Loss: 0.6502, G Loss: 2.0941\n",
      "Epoch [7/20], Step [2090/2541], D Loss: 0.6502, G Loss: 2.0969\n",
      "Epoch [7/20], Step [2091/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [7/20], Step [2092/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [7/20], Step [2093/2541], D Loss: 0.6502, G Loss: 2.0912\n",
      "Epoch [7/20], Step [2094/2541], D Loss: 0.6502, G Loss: 2.0893\n",
      "Epoch [7/20], Step [2095/2541], D Loss: 0.6502, G Loss: 2.0956\n",
      "Epoch [7/20], Step [2096/2541], D Loss: 0.6503, G Loss: 2.0827\n",
      "Epoch [7/20], Step [2097/2541], D Loss: 0.6502, G Loss: 2.0743\n",
      "Epoch [7/20], Step [2098/2541], D Loss: 0.6502, G Loss: 2.0653\n",
      "Epoch [7/20], Step [2099/2541], D Loss: 0.6502, G Loss: 2.0619\n",
      "Epoch [7/20], Step [2100/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [7/20], Step [2101/2541], D Loss: 0.6502, G Loss: 2.1030\n",
      "Epoch [7/20], Step [2102/2541], D Loss: 0.6502, G Loss: 2.0879\n",
      "Epoch [7/20], Step [2103/2541], D Loss: 0.6503, G Loss: 2.0619\n",
      "Epoch [7/20], Step [2104/2541], D Loss: 0.6502, G Loss: 2.0725\n",
      "Epoch [7/20], Step [2105/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [7/20], Step [2106/2541], D Loss: 0.6502, G Loss: 2.0657\n",
      "Epoch [7/20], Step [2107/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [7/20], Step [2108/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [7/20], Step [2109/2541], D Loss: 0.6504, G Loss: 2.0808\n",
      "Epoch [7/20], Step [2110/2541], D Loss: 0.6505, G Loss: 2.0867\n",
      "Epoch [7/20], Step [2111/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [7/20], Step [2112/2541], D Loss: 0.6503, G Loss: 2.0918\n",
      "Epoch [7/20], Step [2113/2541], D Loss: 0.6502, G Loss: 2.0490\n",
      "Epoch [7/20], Step [2114/2541], D Loss: 0.6503, G Loss: 2.0699\n",
      "Epoch [7/20], Step [2115/2541], D Loss: 0.6502, G Loss: 2.0845\n",
      "Epoch [7/20], Step [2116/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [2117/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [7/20], Step [2118/2541], D Loss: 0.6503, G Loss: 2.0658\n",
      "Epoch [7/20], Step [2119/2541], D Loss: 0.6506, G Loss: 2.0874\n",
      "Epoch [7/20], Step [2120/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [7/20], Step [2121/2541], D Loss: 0.6506, G Loss: 2.0759\n",
      "Epoch [7/20], Step [2122/2541], D Loss: 0.6504, G Loss: 2.0860\n",
      "Epoch [7/20], Step [2123/2541], D Loss: 0.6503, G Loss: 2.0912\n",
      "Epoch [7/20], Step [2124/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [7/20], Step [2125/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [7/20], Step [2126/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [2127/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [2128/2541], D Loss: 0.6502, G Loss: 2.1055\n",
      "Epoch [7/20], Step [2129/2541], D Loss: 0.6503, G Loss: 2.0854\n",
      "Epoch [7/20], Step [2130/2541], D Loss: 0.6502, G Loss: 2.0577\n",
      "Epoch [7/20], Step [2131/2541], D Loss: 0.6503, G Loss: 2.0915\n",
      "Epoch [7/20], Step [2132/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [7/20], Step [2133/2541], D Loss: 0.6502, G Loss: 2.0567\n",
      "Epoch [7/20], Step [2134/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [7/20], Step [2135/2541], D Loss: 0.6502, G Loss: 2.0940\n",
      "Epoch [7/20], Step [2136/2541], D Loss: 0.6502, G Loss: 2.0681\n",
      "Epoch [7/20], Step [2137/2541], D Loss: 0.6502, G Loss: 2.0781\n",
      "Epoch [7/20], Step [2138/2541], D Loss: 0.6503, G Loss: 2.0881\n",
      "Epoch [7/20], Step [2139/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [2140/2541], D Loss: 0.6502, G Loss: 2.0693\n",
      "Epoch [7/20], Step [2141/2541], D Loss: 0.6503, G Loss: 2.0641\n",
      "Epoch [7/20], Step [2142/2541], D Loss: 0.6503, G Loss: 2.0755\n",
      "Epoch [7/20], Step [2143/2541], D Loss: 0.6503, G Loss: 2.0620\n",
      "Epoch [7/20], Step [2144/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [7/20], Step [2145/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [7/20], Step [2146/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [2147/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [2148/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [7/20], Step [2149/2541], D Loss: 0.6502, G Loss: 2.1050\n",
      "Epoch [7/20], Step [2150/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [7/20], Step [2151/2541], D Loss: 0.6502, G Loss: 2.0524\n",
      "Epoch [7/20], Step [2152/2541], D Loss: 0.6503, G Loss: 2.0859\n",
      "Epoch [7/20], Step [2153/2541], D Loss: 0.6502, G Loss: 2.1099\n",
      "Epoch [7/20], Step [2154/2541], D Loss: 0.6503, G Loss: 2.0665\n",
      "Epoch [7/20], Step [2155/2541], D Loss: 0.6503, G Loss: 2.0630\n",
      "Epoch [7/20], Step [2156/2541], D Loss: 0.6503, G Loss: 2.0771\n",
      "Epoch [7/20], Step [2157/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [7/20], Step [2158/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [2159/2541], D Loss: 0.6502, G Loss: 2.0715\n",
      "Epoch [7/20], Step [2160/2541], D Loss: 0.6502, G Loss: 2.0953\n",
      "Epoch [7/20], Step [2161/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [7/20], Step [2162/2541], D Loss: 0.6502, G Loss: 2.0727\n",
      "Epoch [7/20], Step [2163/2541], D Loss: 0.6503, G Loss: 2.0922\n",
      "Epoch [7/20], Step [2164/2541], D Loss: 0.6503, G Loss: 2.0782\n",
      "Epoch [7/20], Step [2165/2541], D Loss: 0.6502, G Loss: 2.0823\n",
      "Epoch [7/20], Step [2166/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [7/20], Step [2167/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [7/20], Step [2168/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [7/20], Step [2169/2541], D Loss: 0.6502, G Loss: 2.0615\n",
      "Epoch [7/20], Step [2170/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [7/20], Step [2171/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [2172/2541], D Loss: 0.6502, G Loss: 2.0814\n",
      "Epoch [7/20], Step [2173/2541], D Loss: 0.6502, G Loss: 2.0631\n",
      "Epoch [7/20], Step [2174/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [7/20], Step [2175/2541], D Loss: 0.6502, G Loss: 2.0578\n",
      "Epoch [7/20], Step [2176/2541], D Loss: 0.6502, G Loss: 2.0849\n",
      "Epoch [7/20], Step [2177/2541], D Loss: 0.6502, G Loss: 2.1102\n",
      "Epoch [7/20], Step [2178/2541], D Loss: 0.6502, G Loss: 2.0987\n",
      "Epoch [7/20], Step [2179/2541], D Loss: 0.6502, G Loss: 2.0670\n",
      "Epoch [7/20], Step [2180/2541], D Loss: 0.6502, G Loss: 2.0722\n",
      "Epoch [7/20], Step [2181/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [7/20], Step [2182/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [7/20], Step [2183/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [7/20], Step [2184/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [7/20], Step [2185/2541], D Loss: 0.6502, G Loss: 2.0987\n",
      "Epoch [7/20], Step [2186/2541], D Loss: 0.6502, G Loss: 2.0966\n",
      "Epoch [7/20], Step [2187/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [7/20], Step [2188/2541], D Loss: 0.6502, G Loss: 2.0588\n",
      "Epoch [7/20], Step [2189/2541], D Loss: 0.6502, G Loss: 2.0951\n",
      "Epoch [7/20], Step [2190/2541], D Loss: 0.6502, G Loss: 2.1047\n",
      "Epoch [7/20], Step [2191/2541], D Loss: 0.6502, G Loss: 2.0987\n",
      "Epoch [7/20], Step [2192/2541], D Loss: 0.6502, G Loss: 2.1160\n",
      "Epoch [7/20], Step [2193/2541], D Loss: 0.6502, G Loss: 2.0768\n",
      "Epoch [7/20], Step [2194/2541], D Loss: 0.6502, G Loss: 2.0668\n",
      "Epoch [7/20], Step [2195/2541], D Loss: 0.6503, G Loss: 2.0872\n",
      "Epoch [7/20], Step [2196/2541], D Loss: 0.6502, G Loss: 2.0922\n",
      "Epoch [7/20], Step [2197/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [7/20], Step [2198/2541], D Loss: 0.6502, G Loss: 2.0710\n",
      "Epoch [7/20], Step [2199/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [2200/2541], D Loss: 0.6502, G Loss: 2.0994\n",
      "Epoch [7/20], Step [2201/2541], D Loss: 0.6502, G Loss: 2.0924\n",
      "Epoch [7/20], Step [2202/2541], D Loss: 0.6502, G Loss: 2.0533\n",
      "Epoch [7/20], Step [2203/2541], D Loss: 0.6502, G Loss: 2.0786\n",
      "Epoch [7/20], Step [2204/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [2205/2541], D Loss: 0.6502, G Loss: 2.0725\n",
      "Epoch [7/20], Step [2206/2541], D Loss: 0.6502, G Loss: 2.0872\n",
      "Epoch [7/20], Step [2207/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [7/20], Step [2208/2541], D Loss: 0.6502, G Loss: 2.0974\n",
      "Epoch [7/20], Step [2209/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [7/20], Step [2210/2541], D Loss: 0.6502, G Loss: 2.0788\n",
      "Epoch [7/20], Step [2211/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [2212/2541], D Loss: 0.6502, G Loss: 2.0655\n",
      "Epoch [7/20], Step [2213/2541], D Loss: 0.6502, G Loss: 2.0685\n",
      "Epoch [7/20], Step [2214/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [7/20], Step [2215/2541], D Loss: 0.6502, G Loss: 2.0982\n",
      "Epoch [7/20], Step [2216/2541], D Loss: 0.6502, G Loss: 2.0960\n",
      "Epoch [7/20], Step [2217/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [7/20], Step [2218/2541], D Loss: 0.6502, G Loss: 2.0995\n",
      "Epoch [7/20], Step [2219/2541], D Loss: 0.6502, G Loss: 2.0820\n",
      "Epoch [7/20], Step [2220/2541], D Loss: 0.6502, G Loss: 2.0956\n",
      "Epoch [7/20], Step [2221/2541], D Loss: 0.6502, G Loss: 2.0542\n",
      "Epoch [7/20], Step [2222/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [7/20], Step [2223/2541], D Loss: 0.6502, G Loss: 2.0687\n",
      "Epoch [7/20], Step [2224/2541], D Loss: 0.6502, G Loss: 2.0921\n",
      "Epoch [7/20], Step [2225/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [7/20], Step [2226/2541], D Loss: 0.6509, G Loss: 2.0590\n",
      "Epoch [7/20], Step [2227/2541], D Loss: 0.6519, G Loss: 2.0929\n",
      "Epoch [7/20], Step [2228/2541], D Loss: 0.6507, G Loss: 2.1069\n",
      "Epoch [7/20], Step [2229/2541], D Loss: 0.6513, G Loss: 2.0890\n",
      "Epoch [7/20], Step [2230/2541], D Loss: 0.6505, G Loss: 2.0747\n",
      "Epoch [7/20], Step [2231/2541], D Loss: 0.6505, G Loss: 2.0599\n",
      "Epoch [7/20], Step [2232/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [7/20], Step [2233/2541], D Loss: 0.6503, G Loss: 2.0880\n",
      "Epoch [7/20], Step [2234/2541], D Loss: 0.6509, G Loss: 2.0840\n",
      "Epoch [7/20], Step [2235/2541], D Loss: 0.6502, G Loss: 2.0916\n",
      "Epoch [7/20], Step [2236/2541], D Loss: 0.6503, G Loss: 2.0769\n",
      "Epoch [7/20], Step [2237/2541], D Loss: 0.6505, G Loss: 2.0901\n",
      "Epoch [7/20], Step [2238/2541], D Loss: 0.6503, G Loss: 2.0871\n",
      "Epoch [7/20], Step [2239/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [7/20], Step [2240/2541], D Loss: 0.6502, G Loss: 2.0941\n",
      "Epoch [7/20], Step [2241/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [7/20], Step [2242/2541], D Loss: 0.6502, G Loss: 2.0700\n",
      "Epoch [7/20], Step [2243/2541], D Loss: 0.6502, G Loss: 2.0574\n",
      "Epoch [7/20], Step [2244/2541], D Loss: 0.6503, G Loss: 2.1058\n",
      "Epoch [7/20], Step [2245/2541], D Loss: 0.6502, G Loss: 2.1039\n",
      "Epoch [7/20], Step [2246/2541], D Loss: 0.6502, G Loss: 2.0692\n",
      "Epoch [7/20], Step [2247/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [7/20], Step [2248/2541], D Loss: 0.6503, G Loss: 2.1069\n",
      "Epoch [7/20], Step [2249/2541], D Loss: 0.6503, G Loss: 2.0885\n",
      "Epoch [7/20], Step [2250/2541], D Loss: 0.6504, G Loss: 2.0797\n",
      "Epoch [7/20], Step [2251/2541], D Loss: 0.6504, G Loss: 2.0839\n",
      "Epoch [7/20], Step [2252/2541], D Loss: 0.6503, G Loss: 2.0952\n",
      "Epoch [7/20], Step [2253/2541], D Loss: 0.6503, G Loss: 2.0865\n",
      "Epoch [7/20], Step [2254/2541], D Loss: 0.6503, G Loss: 2.0855\n",
      "Epoch [7/20], Step [2255/2541], D Loss: 0.6502, G Loss: 2.0763\n",
      "Epoch [7/20], Step [2256/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [7/20], Step [2257/2541], D Loss: 0.6503, G Loss: 2.1023\n",
      "Epoch [7/20], Step [2258/2541], D Loss: 0.6504, G Loss: 2.0829\n",
      "Epoch [7/20], Step [2259/2541], D Loss: 0.6502, G Loss: 2.0714\n",
      "Epoch [7/20], Step [2260/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [7/20], Step [2261/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [7/20], Step [2262/2541], D Loss: 0.6503, G Loss: 2.0852\n",
      "Epoch [7/20], Step [2263/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [7/20], Step [2264/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [2265/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [2266/2541], D Loss: 0.6502, G Loss: 2.0967\n",
      "Epoch [7/20], Step [2267/2541], D Loss: 0.6502, G Loss: 2.1018\n",
      "Epoch [7/20], Step [2268/2541], D Loss: 0.6503, G Loss: 2.0706\n",
      "Epoch [7/20], Step [2269/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [7/20], Step [2270/2541], D Loss: 0.6502, G Loss: 2.0705\n",
      "Epoch [7/20], Step [2271/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [7/20], Step [2272/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [7/20], Step [2273/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [7/20], Step [2274/2541], D Loss: 0.6502, G Loss: 2.0905\n",
      "Epoch [7/20], Step [2275/2541], D Loss: 0.6502, G Loss: 2.0650\n",
      "Epoch [7/20], Step [2276/2541], D Loss: 0.6502, G Loss: 2.0726\n",
      "Epoch [7/20], Step [2277/2541], D Loss: 0.6502, G Loss: 2.0885\n",
      "Epoch [7/20], Step [2278/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [7/20], Step [2279/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [7/20], Step [2280/2541], D Loss: 0.6502, G Loss: 2.1045\n",
      "Epoch [7/20], Step [2281/2541], D Loss: 0.6502, G Loss: 2.0920\n",
      "Epoch [7/20], Step [2282/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [2283/2541], D Loss: 0.6502, G Loss: 2.0747\n",
      "Epoch [7/20], Step [2284/2541], D Loss: 0.6502, G Loss: 2.0680\n",
      "Epoch [7/20], Step [2285/2541], D Loss: 0.6502, G Loss: 2.0932\n",
      "Epoch [7/20], Step [2286/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [2287/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [2288/2541], D Loss: 0.6503, G Loss: 2.0869\n",
      "Epoch [7/20], Step [2289/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [7/20], Step [2290/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [7/20], Step [2291/2541], D Loss: 0.6502, G Loss: 2.0573\n",
      "Epoch [7/20], Step [2292/2541], D Loss: 0.6502, G Loss: 2.0703\n",
      "Epoch [7/20], Step [2293/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [7/20], Step [2294/2541], D Loss: 0.6502, G Loss: 2.0930\n",
      "Epoch [7/20], Step [2295/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [7/20], Step [2296/2541], D Loss: 0.6502, G Loss: 2.0840\n",
      "Epoch [7/20], Step [2297/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [7/20], Step [2298/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [2299/2541], D Loss: 0.6502, G Loss: 2.0709\n",
      "Epoch [7/20], Step [2300/2541], D Loss: 0.6502, G Loss: 2.0813\n",
      "Epoch [7/20], Step [2301/2541], D Loss: 0.6502, G Loss: 2.0699\n",
      "Epoch [7/20], Step [2302/2541], D Loss: 0.6503, G Loss: 2.0806\n",
      "Epoch [7/20], Step [2303/2541], D Loss: 0.6504, G Loss: 2.0878\n",
      "Epoch [7/20], Step [2304/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [7/20], Step [2305/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [7/20], Step [2306/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [7/20], Step [2307/2541], D Loss: 0.6504, G Loss: 2.0852\n",
      "Epoch [7/20], Step [2308/2541], D Loss: 0.6502, G Loss: 2.0924\n",
      "Epoch [7/20], Step [2309/2541], D Loss: 0.6502, G Loss: 2.0724\n",
      "Epoch [7/20], Step [2310/2541], D Loss: 0.6502, G Loss: 2.0794\n",
      "Epoch [7/20], Step [2311/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [7/20], Step [2312/2541], D Loss: 0.6502, G Loss: 2.0979\n",
      "Epoch [7/20], Step [2313/2541], D Loss: 0.6502, G Loss: 2.0927\n",
      "Epoch [7/20], Step [2314/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [7/20], Step [2315/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [7/20], Step [2316/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [7/20], Step [2317/2541], D Loss: 0.6502, G Loss: 2.0740\n",
      "Epoch [7/20], Step [2318/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [2319/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [7/20], Step [2320/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [7/20], Step [2321/2541], D Loss: 0.6502, G Loss: 2.0713\n",
      "Epoch [7/20], Step [2322/2541], D Loss: 0.6502, G Loss: 2.0848\n",
      "Epoch [7/20], Step [2323/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [7/20], Step [2324/2541], D Loss: 0.6502, G Loss: 2.0988\n",
      "Epoch [7/20], Step [2325/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [7/20], Step [2326/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [7/20], Step [2327/2541], D Loss: 0.6502, G Loss: 2.0680\n",
      "Epoch [7/20], Step [2328/2541], D Loss: 0.6502, G Loss: 2.0526\n",
      "Epoch [7/20], Step [2329/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [7/20], Step [2330/2541], D Loss: 0.6502, G Loss: 2.0792\n",
      "Epoch [7/20], Step [2331/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [2332/2541], D Loss: 0.6502, G Loss: 2.0876\n",
      "Epoch [7/20], Step [2333/2541], D Loss: 0.6502, G Loss: 2.0581\n",
      "Epoch [7/20], Step [2334/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [2335/2541], D Loss: 0.6502, G Loss: 2.0831\n",
      "Epoch [7/20], Step [2336/2541], D Loss: 0.6502, G Loss: 2.0984\n",
      "Epoch [7/20], Step [2337/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [7/20], Step [2338/2541], D Loss: 0.6502, G Loss: 2.0629\n",
      "Epoch [7/20], Step [2339/2541], D Loss: 0.6503, G Loss: 2.0948\n",
      "Epoch [7/20], Step [2340/2541], D Loss: 0.6502, G Loss: 2.0865\n",
      "Epoch [7/20], Step [2341/2541], D Loss: 0.6502, G Loss: 2.0679\n",
      "Epoch [7/20], Step [2342/2541], D Loss: 0.6502, G Loss: 2.0800\n",
      "Epoch [7/20], Step [2343/2541], D Loss: 0.6502, G Loss: 2.0893\n",
      "Epoch [7/20], Step [2344/2541], D Loss: 0.6502, G Loss: 2.1054\n",
      "Epoch [7/20], Step [2345/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [7/20], Step [2346/2541], D Loss: 0.6502, G Loss: 2.0631\n",
      "Epoch [7/20], Step [2347/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [7/20], Step [2348/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [2349/2541], D Loss: 0.6502, G Loss: 2.0868\n",
      "Epoch [7/20], Step [2350/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [7/20], Step [2351/2541], D Loss: 0.6502, G Loss: 2.0755\n",
      "Epoch [7/20], Step [2352/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [7/20], Step [2353/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [7/20], Step [2354/2541], D Loss: 0.6502, G Loss: 2.0760\n",
      "Epoch [7/20], Step [2355/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [7/20], Step [2356/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [7/20], Step [2357/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [7/20], Step [2358/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [7/20], Step [2359/2541], D Loss: 0.6502, G Loss: 2.0884\n",
      "Epoch [7/20], Step [2360/2541], D Loss: 0.6502, G Loss: 2.0952\n",
      "Epoch [7/20], Step [2361/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [7/20], Step [2362/2541], D Loss: 0.6502, G Loss: 2.0821\n",
      "Epoch [7/20], Step [2363/2541], D Loss: 0.6502, G Loss: 2.0881\n",
      "Epoch [7/20], Step [2364/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [7/20], Step [2365/2541], D Loss: 0.6505, G Loss: 2.0818\n",
      "Epoch [7/20], Step [2366/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [7/20], Step [2367/2541], D Loss: 0.6502, G Loss: 2.0805\n",
      "Epoch [7/20], Step [2368/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [7/20], Step [2369/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [7/20], Step [2370/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [2371/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [7/20], Step [2372/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [7/20], Step [2373/2541], D Loss: 0.6502, G Loss: 2.0810\n",
      "Epoch [7/20], Step [2374/2541], D Loss: 0.6502, G Loss: 2.0859\n",
      "Epoch [7/20], Step [2375/2541], D Loss: 0.6502, G Loss: 2.0861\n",
      "Epoch [7/20], Step [2376/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [7/20], Step [2377/2541], D Loss: 0.6502, G Loss: 2.0834\n",
      "Epoch [7/20], Step [2378/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [7/20], Step [2379/2541], D Loss: 0.6502, G Loss: 2.0915\n",
      "Epoch [7/20], Step [2380/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [7/20], Step [2381/2541], D Loss: 0.6502, G Loss: 2.0843\n",
      "Epoch [7/20], Step [2382/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [2383/2541], D Loss: 0.6502, G Loss: 2.1015\n",
      "Epoch [7/20], Step [2384/2541], D Loss: 0.6502, G Loss: 2.1068\n",
      "Epoch [7/20], Step [2385/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [2386/2541], D Loss: 0.6502, G Loss: 2.0624\n",
      "Epoch [7/20], Step [2387/2541], D Loss: 0.6502, G Loss: 2.0907\n",
      "Epoch [7/20], Step [2388/2541], D Loss: 0.6502, G Loss: 2.0851\n",
      "Epoch [7/20], Step [2389/2541], D Loss: 0.6502, G Loss: 2.1064\n",
      "Epoch [7/20], Step [2390/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [7/20], Step [2391/2541], D Loss: 0.6502, G Loss: 2.0603\n",
      "Epoch [7/20], Step [2392/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [7/20], Step [2393/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [7/20], Step [2394/2541], D Loss: 0.6502, G Loss: 2.0806\n",
      "Epoch [7/20], Step [2395/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [7/20], Step [2396/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [2397/2541], D Loss: 0.6502, G Loss: 2.1095\n",
      "Epoch [7/20], Step [2398/2541], D Loss: 0.6503, G Loss: 2.0674\n",
      "Epoch [7/20], Step [2399/2541], D Loss: 0.6502, G Loss: 2.0665\n",
      "Epoch [7/20], Step [2400/2541], D Loss: 0.6504, G Loss: 2.1159\n",
      "Epoch [7/20], Step [2401/2541], D Loss: 0.6503, G Loss: 2.0951\n",
      "Epoch [7/20], Step [2402/2541], D Loss: 0.6502, G Loss: 2.0611\n",
      "Epoch [7/20], Step [2403/2541], D Loss: 0.6502, G Loss: 2.0772\n",
      "Epoch [7/20], Step [2404/2541], D Loss: 0.6502, G Loss: 2.1108\n",
      "Epoch [7/20], Step [2405/2541], D Loss: 0.6502, G Loss: 2.0946\n",
      "Epoch [7/20], Step [2406/2541], D Loss: 0.6502, G Loss: 2.0765\n",
      "Epoch [7/20], Step [2407/2541], D Loss: 0.6502, G Loss: 2.0827\n",
      "Epoch [7/20], Step [2408/2541], D Loss: 0.6502, G Loss: 2.0863\n",
      "Epoch [7/20], Step [2409/2541], D Loss: 0.6503, G Loss: 2.1045\n",
      "Epoch [7/20], Step [2410/2541], D Loss: 0.6503, G Loss: 2.0802\n",
      "Epoch [7/20], Step [2411/2541], D Loss: 0.6502, G Loss: 2.0612\n",
      "Epoch [7/20], Step [2412/2541], D Loss: 0.6502, G Loss: 2.0733\n",
      "Epoch [7/20], Step [2413/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [7/20], Step [2414/2541], D Loss: 0.6502, G Loss: 2.0824\n",
      "Epoch [7/20], Step [2415/2541], D Loss: 0.6502, G Loss: 2.0775\n",
      "Epoch [7/20], Step [2416/2541], D Loss: 0.6502, G Loss: 2.0962\n",
      "Epoch [7/20], Step [2417/2541], D Loss: 0.6502, G Loss: 2.0817\n",
      "Epoch [7/20], Step [2418/2541], D Loss: 0.6502, G Loss: 2.0665\n",
      "Epoch [7/20], Step [2419/2541], D Loss: 0.6503, G Loss: 2.1000\n",
      "Epoch [7/20], Step [2420/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [7/20], Step [2421/2541], D Loss: 0.6503, G Loss: 2.1196\n",
      "Epoch [7/20], Step [2422/2541], D Loss: 0.6503, G Loss: 2.0904\n",
      "Epoch [7/20], Step [2423/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [2424/2541], D Loss: 0.6502, G Loss: 2.0778\n",
      "Epoch [7/20], Step [2425/2541], D Loss: 0.6502, G Loss: 2.0783\n",
      "Epoch [7/20], Step [2426/2541], D Loss: 0.6502, G Loss: 2.0955\n",
      "Epoch [7/20], Step [2427/2541], D Loss: 0.6502, G Loss: 2.0935\n",
      "Epoch [7/20], Step [2428/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [7/20], Step [2429/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [7/20], Step [2430/2541], D Loss: 0.6502, G Loss: 2.0864\n",
      "Epoch [7/20], Step [2431/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [7/20], Step [2432/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [7/20], Step [2433/2541], D Loss: 0.6502, G Loss: 2.0755\n",
      "Epoch [7/20], Step [2434/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [7/20], Step [2435/2541], D Loss: 0.6502, G Loss: 2.0769\n",
      "Epoch [7/20], Step [2436/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [7/20], Step [2437/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [7/20], Step [2438/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [2439/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [7/20], Step [2440/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [7/20], Step [2441/2541], D Loss: 0.6502, G Loss: 2.0787\n",
      "Epoch [7/20], Step [2442/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [7/20], Step [2443/2541], D Loss: 0.6502, G Loss: 2.0803\n",
      "Epoch [7/20], Step [2444/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [7/20], Step [2445/2541], D Loss: 0.6502, G Loss: 2.0743\n",
      "Epoch [7/20], Step [2446/2541], D Loss: 0.6502, G Loss: 2.0607\n",
      "Epoch [7/20], Step [2447/2541], D Loss: 0.6502, G Loss: 2.0796\n",
      "Epoch [7/20], Step [2448/2541], D Loss: 0.6502, G Loss: 2.0979\n",
      "Epoch [7/20], Step [2449/2541], D Loss: 0.6502, G Loss: 2.0897\n",
      "Epoch [7/20], Step [2450/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [7/20], Step [2451/2541], D Loss: 0.6502, G Loss: 2.0724\n",
      "Epoch [7/20], Step [2452/2541], D Loss: 0.6502, G Loss: 2.0661\n",
      "Epoch [7/20], Step [2453/2541], D Loss: 0.6502, G Loss: 2.0816\n",
      "Epoch [7/20], Step [2454/2541], D Loss: 0.6502, G Loss: 2.0894\n",
      "Epoch [7/20], Step [2455/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [7/20], Step [2456/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [7/20], Step [2457/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [7/20], Step [2458/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [7/20], Step [2459/2541], D Loss: 0.6502, G Loss: 2.0887\n",
      "Epoch [7/20], Step [2460/2541], D Loss: 0.6503, G Loss: 2.0757\n",
      "Epoch [7/20], Step [2461/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [7/20], Step [2462/2541], D Loss: 0.6502, G Loss: 2.0855\n",
      "Epoch [7/20], Step [2463/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [2464/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [7/20], Step [2465/2541], D Loss: 0.6502, G Loss: 2.0908\n",
      "Epoch [7/20], Step [2466/2541], D Loss: 0.6502, G Loss: 2.0687\n",
      "Epoch [7/20], Step [2467/2541], D Loss: 0.6502, G Loss: 2.0948\n",
      "Epoch [7/20], Step [2468/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [7/20], Step [2469/2541], D Loss: 0.6503, G Loss: 2.0852\n",
      "Epoch [7/20], Step [2470/2541], D Loss: 0.6504, G Loss: 2.0790\n",
      "Epoch [7/20], Step [2471/2541], D Loss: 0.6502, G Loss: 2.0782\n",
      "Epoch [7/20], Step [2472/2541], D Loss: 0.6502, G Loss: 2.0682\n",
      "Epoch [7/20], Step [2473/2541], D Loss: 0.6502, G Loss: 2.0890\n",
      "Epoch [7/20], Step [2474/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [7/20], Step [2475/2541], D Loss: 0.6502, G Loss: 2.0653\n",
      "Epoch [7/20], Step [2476/2541], D Loss: 0.6502, G Loss: 2.0762\n",
      "Epoch [7/20], Step [2477/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [7/20], Step [2478/2541], D Loss: 0.6502, G Loss: 2.0825\n",
      "Epoch [7/20], Step [2479/2541], D Loss: 0.6502, G Loss: 2.0721\n",
      "Epoch [7/20], Step [2480/2541], D Loss: 0.6502, G Loss: 2.0808\n",
      "Epoch [7/20], Step [2481/2541], D Loss: 0.6502, G Loss: 2.0902\n",
      "Epoch [7/20], Step [2482/2541], D Loss: 0.6502, G Loss: 2.0767\n",
      "Epoch [7/20], Step [2483/2541], D Loss: 0.6502, G Loss: 2.0675\n",
      "Epoch [7/20], Step [2484/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [2485/2541], D Loss: 0.6502, G Loss: 2.0776\n",
      "Epoch [7/20], Step [2486/2541], D Loss: 0.6502, G Loss: 2.0875\n",
      "Epoch [7/20], Step [2487/2541], D Loss: 0.6502, G Loss: 2.0815\n",
      "Epoch [7/20], Step [2488/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [7/20], Step [2489/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [7/20], Step [2490/2541], D Loss: 0.6502, G Loss: 2.0972\n",
      "Epoch [7/20], Step [2491/2541], D Loss: 0.6502, G Loss: 2.0911\n",
      "Epoch [7/20], Step [2492/2541], D Loss: 0.6502, G Loss: 2.0784\n",
      "Epoch [7/20], Step [2493/2541], D Loss: 0.6502, G Loss: 2.0956\n",
      "Epoch [7/20], Step [2494/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [7/20], Step [2495/2541], D Loss: 0.6502, G Loss: 2.0730\n",
      "Epoch [7/20], Step [2496/2541], D Loss: 0.6502, G Loss: 2.0694\n",
      "Epoch [7/20], Step [2497/2541], D Loss: 0.6502, G Loss: 2.0797\n",
      "Epoch [7/20], Step [2498/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [7/20], Step [2499/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [7/20], Step [2500/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [2501/2541], D Loss: 0.6502, G Loss: 2.0741\n",
      "Epoch [7/20], Step [2502/2541], D Loss: 0.6502, G Loss: 2.0745\n",
      "Epoch [7/20], Step [2503/2541], D Loss: 0.6502, G Loss: 2.0749\n",
      "Epoch [7/20], Step [2504/2541], D Loss: 0.6502, G Loss: 2.0744\n",
      "Epoch [7/20], Step [2505/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [7/20], Step [2506/2541], D Loss: 0.6502, G Loss: 2.0934\n",
      "Epoch [7/20], Step [2507/2541], D Loss: 0.6502, G Loss: 2.0896\n",
      "Epoch [7/20], Step [2508/2541], D Loss: 0.6503, G Loss: 2.1039\n",
      "Epoch [7/20], Step [2509/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [2510/2541], D Loss: 0.6502, G Loss: 2.0680\n",
      "Epoch [7/20], Step [2511/2541], D Loss: 0.6502, G Loss: 2.0764\n",
      "Epoch [7/20], Step [2512/2541], D Loss: 0.6502, G Loss: 2.1020\n",
      "Epoch [7/20], Step [2513/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [7/20], Step [2514/2541], D Loss: 0.6502, G Loss: 2.0665\n",
      "Epoch [7/20], Step [2515/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [7/20], Step [2516/2541], D Loss: 0.6502, G Loss: 2.0869\n",
      "Epoch [7/20], Step [2517/2541], D Loss: 0.6502, G Loss: 2.0777\n",
      "Epoch [7/20], Step [2518/2541], D Loss: 0.6502, G Loss: 2.0759\n",
      "Epoch [7/20], Step [2519/2541], D Loss: 0.6502, G Loss: 2.0871\n",
      "Epoch [7/20], Step [2520/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [7/20], Step [2521/2541], D Loss: 0.6502, G Loss: 2.0699\n",
      "Epoch [7/20], Step [2522/2541], D Loss: 0.6502, G Loss: 2.0793\n",
      "Epoch [7/20], Step [2523/2541], D Loss: 0.6502, G Loss: 2.0828\n",
      "Epoch [7/20], Step [2524/2541], D Loss: 0.6502, G Loss: 2.0785\n",
      "Epoch [7/20], Step [2525/2541], D Loss: 0.6502, G Loss: 2.0985\n",
      "Epoch [7/20], Step [2526/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [7/20], Step [2527/2541], D Loss: 0.6502, G Loss: 2.0780\n",
      "Epoch [7/20], Step [2528/2541], D Loss: 0.6502, G Loss: 2.0842\n",
      "Epoch [7/20], Step [2529/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [7/20], Step [2530/2541], D Loss: 0.6502, G Loss: 2.0811\n",
      "Epoch [7/20], Step [2531/2541], D Loss: 0.6502, G Loss: 2.1010\n",
      "Epoch [7/20], Step [2532/2541], D Loss: 0.6502, G Loss: 2.0958\n",
      "Epoch [7/20], Step [2533/2541], D Loss: 0.6502, G Loss: 2.0736\n",
      "Epoch [7/20], Step [2534/2541], D Loss: 0.6502, G Loss: 2.0600\n",
      "Epoch [7/20], Step [2535/2541], D Loss: 0.6502, G Loss: 2.0807\n",
      "Epoch [7/20], Step [2536/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [7/20], Step [2537/2541], D Loss: 0.6502, G Loss: 2.0895\n",
      "Epoch [7/20], Step [2538/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [7/20], Step [2539/2541], D Loss: 0.6502, G Loss: 2.0728\n",
      "Epoch [7/20], Step [2540/2541], D Loss: 0.6505, G Loss: 2.1773\n",
      "Models saved after epoch 7\n",
      "Epoch [8/20], Step [0/2541], D Loss: 0.6509, G Loss: 2.0725\n",
      "Epoch [8/20], Step [1/2541], D Loss: 0.6502, G Loss: 2.0579\n",
      "Epoch [8/20], Step [2/2541], D Loss: 0.6505, G Loss: 2.1022\n",
      "Epoch [8/20], Step [3/2541], D Loss: 0.6502, G Loss: 2.0891\n",
      "Epoch [8/20], Step [4/2541], D Loss: 0.6502, G Loss: 2.0669\n",
      "Epoch [8/20], Step [5/2541], D Loss: 0.6502, G Loss: 2.0751\n",
      "Epoch [8/20], Step [6/2541], D Loss: 0.6502, G Loss: 2.0928\n",
      "Epoch [8/20], Step [7/2541], D Loss: 0.6503, G Loss: 2.0809\n",
      "Epoch [8/20], Step [8/2541], D Loss: 0.6502, G Loss: 2.0698\n",
      "Epoch [8/20], Step [9/2541], D Loss: 0.6503, G Loss: 2.1054\n",
      "Epoch [8/20], Step [10/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [8/20], Step [11/2541], D Loss: 0.6502, G Loss: 2.0900\n",
      "Epoch [8/20], Step [12/2541], D Loss: 0.6502, G Loss: 2.0723\n",
      "Epoch [8/20], Step [13/2541], D Loss: 0.6502, G Loss: 2.0870\n",
      "Epoch [8/20], Step [14/2541], D Loss: 0.6502, G Loss: 2.0850\n",
      "Epoch [8/20], Step [15/2541], D Loss: 0.6502, G Loss: 2.0716\n",
      "Epoch [8/20], Step [16/2541], D Loss: 0.6502, G Loss: 2.0931\n",
      "Epoch [8/20], Step [17/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [8/20], Step [18/2541], D Loss: 0.6502, G Loss: 2.0924\n",
      "Epoch [8/20], Step [19/2541], D Loss: 0.6503, G Loss: 2.1047\n",
      "Epoch [8/20], Step [20/2541], D Loss: 0.6503, G Loss: 2.0938\n",
      "Epoch [8/20], Step [21/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [8/20], Step [22/2541], D Loss: 0.6502, G Loss: 2.0893\n",
      "Epoch [8/20], Step [23/2541], D Loss: 0.6502, G Loss: 2.0626\n",
      "Epoch [8/20], Step [24/2541], D Loss: 0.6503, G Loss: 2.0248\n",
      "Epoch [8/20], Step [25/2541], D Loss: 0.6505, G Loss: 2.0827\n",
      "Epoch [8/20], Step [26/2541], D Loss: 0.6502, G Loss: 2.1090\n",
      "Epoch [8/20], Step [27/2541], D Loss: 0.6503, G Loss: 2.0952\n",
      "Epoch [8/20], Step [28/2541], D Loss: 0.6502, G Loss: 2.0669\n",
      "Epoch [8/20], Step [29/2541], D Loss: 0.6502, G Loss: 2.0804\n",
      "Epoch [8/20], Step [30/2541], D Loss: 0.6502, G Loss: 2.0938\n",
      "Epoch [8/20], Step [31/2541], D Loss: 0.6502, G Loss: 2.0836\n",
      "Epoch [8/20], Step [32/2541], D Loss: 0.6502, G Loss: 2.0943\n",
      "Epoch [8/20], Step [33/2541], D Loss: 0.6502, G Loss: 2.0691\n",
      "Epoch [8/20], Step [34/2541], D Loss: 0.6502, G Loss: 2.0761\n",
      "Epoch [8/20], Step [35/2541], D Loss: 0.6502, G Loss: 2.0720\n",
      "Epoch [8/20], Step [36/2541], D Loss: 0.6502, G Loss: 2.0914\n",
      "Epoch [8/20], Step [37/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [8/20], Step [38/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [8/20], Step [39/2541], D Loss: 0.6502, G Loss: 2.0737\n",
      "Epoch [8/20], Step [40/2541], D Loss: 0.6502, G Loss: 2.0917\n",
      "Epoch [8/20], Step [41/2541], D Loss: 0.6502, G Loss: 2.0918\n",
      "Epoch [8/20], Step [42/2541], D Loss: 0.6502, G Loss: 2.0789\n",
      "Epoch [8/20], Step [43/2541], D Loss: 0.6502, G Loss: 2.0835\n",
      "Epoch [8/20], Step [44/2541], D Loss: 0.6502, G Loss: 2.0837\n",
      "Epoch [8/20], Step [45/2541], D Loss: 0.6502, G Loss: 2.0809\n",
      "Epoch [8/20], Step [46/2541], D Loss: 0.6502, G Loss: 2.0874\n",
      "Epoch [8/20], Step [47/2541], D Loss: 0.6502, G Loss: 2.0822\n",
      "Epoch [8/20], Step [48/2541], D Loss: 0.6502, G Loss: 2.0826\n",
      "Epoch [8/20], Step [49/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [8/20], Step [50/2541], D Loss: 0.6502, G Loss: 2.0883\n",
      "Epoch [8/20], Step [51/2541], D Loss: 0.6502, G Loss: 2.0856\n",
      "Epoch [8/20], Step [52/2541], D Loss: 0.6502, G Loss: 2.0889\n",
      "Epoch [8/20], Step [53/2541], D Loss: 0.6502, G Loss: 2.0748\n",
      "Epoch [8/20], Step [54/2541], D Loss: 0.6502, G Loss: 2.0795\n",
      "Epoch [8/20], Step [55/2541], D Loss: 0.6502, G Loss: 2.0838\n",
      "Epoch [8/20], Step [56/2541], D Loss: 0.6502, G Loss: 2.0735\n",
      "Epoch [8/20], Step [57/2541], D Loss: 0.6502, G Loss: 2.0753\n",
      "Epoch [8/20], Step [58/2541], D Loss: 0.6502, G Loss: 2.0829\n",
      "Epoch [8/20], Step [59/2541], D Loss: 0.6502, G Loss: 2.0844\n",
      "Epoch [8/20], Step [60/2541], D Loss: 0.6502, G Loss: 2.0939\n",
      "Epoch [8/20], Step [61/2541], D Loss: 0.6502, G Loss: 2.0779\n",
      "Epoch [8/20], Step [62/2541], D Loss: 0.6502, G Loss: 2.0680\n",
      "Epoch [8/20], Step [63/2541], D Loss: 0.6502, G Loss: 2.0873\n",
      "Epoch [8/20], Step [64/2541], D Loss: 0.6502, G Loss: 2.0858\n",
      "Epoch [8/20], Step [65/2541], D Loss: 0.6502, G Loss: 2.0801\n",
      "Epoch [8/20], Step [66/2541], D Loss: 0.6502, G Loss: 2.0812\n",
      "Epoch [8/20], Step [67/2541], D Loss: 0.6502, G Loss: 2.0832\n",
      "Epoch [8/20], Step [68/2541], D Loss: 0.6502, G Loss: 2.0857\n",
      "Epoch [8/20], Step [69/2541], D Loss: 0.6502, G Loss: 2.0933\n",
      "Epoch [8/20], Step [70/2541], D Loss: 0.6502, G Loss: 2.0839\n",
      "Epoch [8/20], Step [71/2541], D Loss: 0.6502, G Loss: 2.0798\n",
      "Epoch [8/20], Step [72/2541], D Loss: 0.6502, G Loss: 2.0791\n",
      "Epoch [8/20], Step [73/2541], D Loss: 0.6502, G Loss: 2.0867\n",
      "Epoch [8/20], Step [74/2541], D Loss: 0.6502, G Loss: 2.0906\n",
      "Epoch [8/20], Step [75/2541], D Loss: 0.6502, G Loss: 2.0799\n",
      "Epoch [8/20], Step [76/2541], D Loss: 0.6502, G Loss: 2.0703\n",
      "Epoch [8/20], Step [77/2541], D Loss: 0.6502, G Loss: 2.0756\n",
      "Epoch [8/20], Step [78/2541], D Loss: 0.6502, G Loss: 2.0919\n",
      "Epoch [8/20], Step [79/2541], D Loss: 0.6502, G Loss: 2.0904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, num_epochs):\n\u001b[1;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\sanu8\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\sanu8\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sanu8\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mDeepFakeFrameDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 18\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure 3 channels\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     20\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\sanu8\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:941\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    891\u001b[0m     mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    895\u001b[0m     colors: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[0;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    945\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanu8\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(8, num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Move data to device\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        real_labels = torch.ones(batch_size, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        # Real images\n",
    "        real_outputs = discriminator(real_images)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        \n",
    "        # Fake images\n",
    "        z = torch.randn(batch_size, 100, 1, 1, device=device)  # Random noise\n",
    "        fake_images = generator(z)\n",
    "        fake_outputs = discriminator(fake_images.detach())\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        z = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "        fake_images = generator(z)\n",
    "        fake_outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "\n",
    "        # Print losses\n",
    "        if not i%5:\n",
    "            print(f\"Epoch: [{epoch}/{num_epochs}], Step: [{i}/{len(dataloader)}], \"\n",
    "                    f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Save models after each epoch\n",
    "    torch.save(generator.state_dict(), f\"generator_epoch_{epoch}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch}.pth\")\n",
    "    print(f\"Models saved after epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(discriminator.state_dict(), \"discriminator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating model on Testing videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testing_videos(file_path, common_directory):\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            label, video_name = line.strip().split(\" \", 1)\n",
    "            video_path = os.path.join(common_directory, video_name)\n",
    "            video_paths.append(video_path)\n",
    "            labels.append(int(label))\n",
    "    return video_paths, labels\n",
    "\n",
    "# Example usage\n",
    "common_directory = \"Celeb-DF-v2\"\n",
    "video_paths, labels = load_testing_videos(\"Celeb-Df-v2/List_of_testing_videos.txt\", common_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_frames(video_path, frame_interval=30):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define preprocessing transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    return torch.stack([transform(frame) for frame in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_video(discriminator, video_path, device):\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = preprocess_frames(frames).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = discriminator(frames).mean(dim=(2, 3))  # Global average pooling\n",
    "        predictions = (outputs > 0.5).float()  # Threshold at 0.5\n",
    "        avg_prediction = predictions.mean().item()  # Average prediction for the video\n",
    "    \n",
    "    return avg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(discriminator, video_paths, labels, device):\n",
    "    correct = 0\n",
    "    total = len(video_paths)\n",
    "    \n",
    "    for video_path, label in zip(video_paths, labels):\n",
    "        avg_prediction = evaluate_video(discriminator, video_path, device)\n",
    "        predicted_label = 1 if avg_prediction > 0.5 else 0\n",
    "        if predicted_label == label:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.load_state_dict(torch.load(\"discriminator.pth\"))  # Load trained model\n",
    "discriminator.eval()  # Set to evaluation mode \n",
    "\n",
    "accuracy = calculate_accuracy(discriminator, video_paths, labels, device)\n",
    "print(f\"Accuracy on test videos: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
