{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Frames from the training videos to train the model\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_folder, target_fps=1 ):\n",
    "    # Open the video file\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get the original frame rate of the video (fps)\n",
    "    original_fps = 30\n",
    "\n",
    "    # Calculate the interval based on the target fps and the original fps\n",
    "    frame_interval = int(original_fps // target_fps)\n",
    "\n",
    "    # Get the video name (without extension) to include it in the frame filename\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        # If we are at the right interval, save the frame\n",
    "        if count % frame_interval == 0:\n",
    "            # Include the video name in the frame filename\n",
    "            frame_path = os.path.join(output_folder, f\"{video_name}_frame_{count:04d}.jpg\")\n",
    "            cv2.imwrite(frame_path, image)\n",
    "        \n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "\n",
    "# # Process all videos in the real and fake folders\n",
    "real_video_folder = \"Celeb-DF-v2/Celeb-real\"\n",
    "fake_video_folder = \"Celeb-DF-v2/Celeb-synthesis\"\n",
    "real_frames_folder = \"Frames/real_frames\"\n",
    "fake_frames_folder = \"Frames/fake_frames\"\n",
    "# # #Replace these paths with respective paths of the directory\n",
    "\n",
    "if not os.path.isdir(real_frames_folder) and not os.path.isdir(fake_frames_folder):\n",
    "    os.makedirs(real_frames_folder, exist_ok=True)\n",
    "    os.makedirs(fake_frames_folder, exist_ok=True)\n",
    "\n",
    "    for video_name in os.listdir(real_video_folder):\n",
    "        video_path = os.path.join(real_video_folder, video_name)\n",
    "        extract_frames(video_path, real_frames_folder)\n",
    "\n",
    "    for video_name in os.listdir(fake_video_folder):\n",
    "        video_path = os.path.join(fake_video_folder, video_name)\n",
    "        extract_frames(video_path, fake_frames_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class DeepFakeFrameDataset(Dataset):\n",
    "    def __init__(self, real_folder, fake_folder, transform=None):\n",
    "        self.real_frames = [os.path.join(real_folder, f) for f in os.listdir(real_folder)]\n",
    "        self.fake_frames = [os.path.join(fake_folder, f) for f in os.listdir(fake_folder)]\n",
    "        self.all_frames = self.real_frames + self.fake_frames\n",
    "        self.labels = [0] * len(self.real_frames) + [1] * len(self.fake_frames)  # 0 = real, 1 = fake\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.all_frames[idx]).convert(\"RGB\")  # Ensure 3 channels\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1] range\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = DeepFakeFrameDataset(real_frames_folder, fake_frames_folder, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Load XceptionNet as the backbone\n",
    "        self.backbone = timm.create_model('legacy_xception', pretrained=True, features_only=True)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layer for binary classification\n",
    "        self.fc = nn.Linear(self.backbone.feature_info.channels()[-1], num_classes)\n",
    "        \n",
    "        # ReLU activation for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features from XceptionNet\n",
    "        features = self.backbone(x)[-1]  # Use the last feature map\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled = self.global_pool(features)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.fc(pooled)\n",
    "        \n",
    "        # Sigmoid activation\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: latent_dim x 1 x 1\n",
    "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 256 x 4 x 4\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 128 x 8 x 8\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 64 x 16 x 16\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output: 3 x 32 x 32 (normalized to [-1, 1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device to GPU or CPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small Learning Rate helps model converge quickly leading to less training time\n",
    "#If the model overfits, learning rate will be decreased\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0005, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(8, num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Move data to device\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        real_labels = torch.ones(batch_size, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        # Real images\n",
    "        real_outputs = discriminator(real_images)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        \n",
    "        # Fake images\n",
    "        z = torch.randn(batch_size, 100, 1, 1, device=device)  # Random noise\n",
    "        fake_images = generator(z)\n",
    "        fake_outputs = discriminator(fake_images.detach())\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        z = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "        fake_images = generator(z)\n",
    "        fake_outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "\n",
    "        # Print losses\n",
    "        if not i%5:\n",
    "            print(f\"Epoch: [{epoch}/{num_epochs}], Step: [{i}/{len(dataloader)}], \"\n",
    "                    f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Save models after each epoch\n",
    "    torch.save(generator.state_dict(), f\"generator_epoch_{epoch}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch}.pth\")\n",
    "    print(f\"Models saved after epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(discriminator.state_dict(), \"discriminator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating model on Testing videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testing_videos(file_path, common_directory):\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            label, video_name = line.strip().split(\" \", 1)\n",
    "            video_path = os.path.join(common_directory, video_name)\n",
    "            video_paths.append(video_path)\n",
    "            labels.append(int(label))\n",
    "    return video_paths, labels\n",
    "\n",
    "# Example usage\n",
    "common_directory = \"Celeb-DF-v2\"\n",
    "video_paths, labels = load_testing_videos(\"Celeb-Df-v2/List_of_testing_videos.txt\", common_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_frames(video_path, frame_interval=30):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define preprocessing transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    return torch.stack([transform(frame) for frame in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_video(discriminator, video_path, device):\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = preprocess_frames(frames).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = discriminator(frames).mean(dim=(2, 3))  # Global average pooling\n",
    "        predictions = (outputs > 0.5).float()  # Threshold at 0.5\n",
    "        avg_prediction = predictions.mean().item()  # Average prediction for the video\n",
    "    \n",
    "    return avg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(discriminator, video_paths, labels, device):\n",
    "    correct = 0\n",
    "    total = len(video_paths)\n",
    "    \n",
    "    for video_path, label in zip(video_paths, labels):\n",
    "        avg_prediction = evaluate_video(discriminator, video_path, device)\n",
    "        predicted_label = 1 if avg_prediction > 0.5 else 0\n",
    "        if predicted_label == label:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.load_state_dict(torch.load(\"discriminator.pth\"))  # Load trained model\n",
    "discriminator.eval()  # Set to evaluation mode \n",
    "\n",
    "accuracy = calculate_accuracy(discriminator, video_paths, labels, device)\n",
    "print(f\"Accuracy on test videos: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
